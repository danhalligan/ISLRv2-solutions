<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>4 Classification | An Introduction to Statistical Learning</title>
  <meta name="description" content="4 Classification | An Introduction to Statistical Learning" />
  <meta name="generator" content="bookdown 0.40.1 and GitBook 2.6.7" />

  <meta property="og:title" content="4 Classification | An Introduction to Statistical Learning" />
  <meta property="og:type" content="book" />
  
  
  <meta name="github-repo" content="danhalligan/ISLRv2-solutions" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="4 Classification | An Introduction to Statistical Learning" />
  
  
  




  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="linear-regression.html"/>
<link rel="next" href="resampling-methods.html"/>
<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<script src="libs/accessible-code-block-0.0.1/empty-anchor.js"></script>
<link href="libs/anchor-sections-1.1.0/anchor-sections.css" rel="stylesheet" />
<link href="libs/anchor-sections-1.1.0/anchor-sections-hash.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.1.0/anchor-sections.js"></script>
<link href="libs/htmltools-fill-0.5.8.1/fill.css" rel="stylesheet" />
<script src="libs/htmlwidgets-1.6.4/htmlwidgets.js"></script>
<script src="libs/plotly-binding-4.10.4/plotly.js"></script>
<script src="libs/typedarray-0.1/typedarray.min.js"></script>
<link href="libs/crosstalk-1.2.1/css/crosstalk.min.css" rel="stylesheet" />
<script src="libs/crosstalk-1.2.1/js/crosstalk.min.js"></script>
<link href="libs/plotly-htmlwidgets-css-2.11.1/plotly-htmlwidgets.css" rel="stylesheet" />
<script src="libs/plotly-main-2.11.1/plotly-latest.min.js"></script>


<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<style type="text/css">
  
  div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
</style>

<link rel="stylesheet" href="islrv2.css" type="text/css" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/aaaakshat/cm-web-fonts@latest/fonts.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">ISLRv2 Solutions</a></li>

<li class="divider"></li>
<li class="chapter" data-level="1" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i><b>1</b> Introduction</a></li>
<li class="chapter" data-level="2" data-path="statistical-learning.html"><a href="statistical-learning.html"><i class="fa fa-check"></i><b>2</b> Statistical Learning</a>
<ul>
<li class="chapter" data-level="2.1" data-path="statistical-learning.html"><a href="statistical-learning.html#conceptual"><i class="fa fa-check"></i><b>2.1</b> Conceptual</a>
<ul>
<li class="chapter" data-level="2.1.1" data-path="statistical-learning.html"><a href="statistical-learning.html#question-1"><i class="fa fa-check"></i><b>2.1.1</b> Question 1</a></li>
<li class="chapter" data-level="2.1.2" data-path="statistical-learning.html"><a href="statistical-learning.html#question-2"><i class="fa fa-check"></i><b>2.1.2</b> Question 2</a></li>
<li class="chapter" data-level="2.1.3" data-path="statistical-learning.html"><a href="statistical-learning.html#question-3"><i class="fa fa-check"></i><b>2.1.3</b> Question 3</a></li>
<li class="chapter" data-level="2.1.4" data-path="statistical-learning.html"><a href="statistical-learning.html#question-4"><i class="fa fa-check"></i><b>2.1.4</b> Question 4</a></li>
<li class="chapter" data-level="2.1.5" data-path="statistical-learning.html"><a href="statistical-learning.html#question-5"><i class="fa fa-check"></i><b>2.1.5</b> Question 5</a></li>
<li class="chapter" data-level="2.1.6" data-path="statistical-learning.html"><a href="statistical-learning.html#question-6"><i class="fa fa-check"></i><b>2.1.6</b> Question 6</a></li>
<li class="chapter" data-level="2.1.7" data-path="statistical-learning.html"><a href="statistical-learning.html#question-7"><i class="fa fa-check"></i><b>2.1.7</b> Question 7</a></li>
</ul></li>
<li class="chapter" data-level="2.2" data-path="statistical-learning.html"><a href="statistical-learning.html#applied"><i class="fa fa-check"></i><b>2.2</b> Applied</a>
<ul>
<li class="chapter" data-level="2.2.1" data-path="statistical-learning.html"><a href="statistical-learning.html#question-8"><i class="fa fa-check"></i><b>2.2.1</b> Question 8</a></li>
<li class="chapter" data-level="2.2.2" data-path="statistical-learning.html"><a href="statistical-learning.html#question-9"><i class="fa fa-check"></i><b>2.2.2</b> Question 9</a></li>
<li class="chapter" data-level="2.2.3" data-path="statistical-learning.html"><a href="statistical-learning.html#question-10"><i class="fa fa-check"></i><b>2.2.3</b> Question 10</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="3" data-path="linear-regression.html"><a href="linear-regression.html"><i class="fa fa-check"></i><b>3</b> Linear Regression</a>
<ul>
<li class="chapter" data-level="3.1" data-path="linear-regression.html"><a href="linear-regression.html#conceptual-1"><i class="fa fa-check"></i><b>3.1</b> Conceptual</a>
<ul>
<li class="chapter" data-level="3.1.1" data-path="linear-regression.html"><a href="linear-regression.html#question-1-1"><i class="fa fa-check"></i><b>3.1.1</b> Question 1</a></li>
<li class="chapter" data-level="3.1.2" data-path="linear-regression.html"><a href="linear-regression.html#question-2-1"><i class="fa fa-check"></i><b>3.1.2</b> Question 2</a></li>
<li class="chapter" data-level="3.1.3" data-path="linear-regression.html"><a href="linear-regression.html#question-3-1"><i class="fa fa-check"></i><b>3.1.3</b> Question 3</a></li>
<li class="chapter" data-level="3.1.4" data-path="linear-regression.html"><a href="linear-regression.html#question-4-1"><i class="fa fa-check"></i><b>3.1.4</b> Question 4</a></li>
<li class="chapter" data-level="3.1.5" data-path="linear-regression.html"><a href="linear-regression.html#question-5-1"><i class="fa fa-check"></i><b>3.1.5</b> Question 5</a></li>
<li class="chapter" data-level="3.1.6" data-path="linear-regression.html"><a href="linear-regression.html#question-6-1"><i class="fa fa-check"></i><b>3.1.6</b> Question 6</a></li>
<li class="chapter" data-level="3.1.7" data-path="linear-regression.html"><a href="linear-regression.html#question-7-1"><i class="fa fa-check"></i><b>3.1.7</b> Question 7</a></li>
</ul></li>
<li class="chapter" data-level="3.2" data-path="linear-regression.html"><a href="linear-regression.html#applied-1"><i class="fa fa-check"></i><b>3.2</b> Applied</a>
<ul>
<li class="chapter" data-level="3.2.1" data-path="linear-regression.html"><a href="linear-regression.html#question-8-1"><i class="fa fa-check"></i><b>3.2.1</b> Question 8</a></li>
<li class="chapter" data-level="3.2.2" data-path="linear-regression.html"><a href="linear-regression.html#question-9-1"><i class="fa fa-check"></i><b>3.2.2</b> Question 9</a></li>
<li class="chapter" data-level="3.2.3" data-path="linear-regression.html"><a href="linear-regression.html#question-10-1"><i class="fa fa-check"></i><b>3.2.3</b> Question 10</a></li>
<li class="chapter" data-level="3.2.4" data-path="linear-regression.html"><a href="linear-regression.html#question-11"><i class="fa fa-check"></i><b>3.2.4</b> Question 11</a></li>
<li class="chapter" data-level="3.2.5" data-path="linear-regression.html"><a href="linear-regression.html#question-12"><i class="fa fa-check"></i><b>3.2.5</b> Question 12</a></li>
<li class="chapter" data-level="3.2.6" data-path="linear-regression.html"><a href="linear-regression.html#question-13"><i class="fa fa-check"></i><b>3.2.6</b> Question 13</a></li>
<li class="chapter" data-level="3.2.7" data-path="linear-regression.html"><a href="linear-regression.html#question-14"><i class="fa fa-check"></i><b>3.2.7</b> Question 14</a></li>
<li class="chapter" data-level="3.2.8" data-path="linear-regression.html"><a href="linear-regression.html#question-15"><i class="fa fa-check"></i><b>3.2.8</b> Question 15</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="4" data-path="classification.html"><a href="classification.html"><i class="fa fa-check"></i><b>4</b> Classification</a>
<ul>
<li class="chapter" data-level="4.1" data-path="classification.html"><a href="classification.html#conceptual-2"><i class="fa fa-check"></i><b>4.1</b> Conceptual</a>
<ul>
<li class="chapter" data-level="4.1.1" data-path="classification.html"><a href="classification.html#question-1-2"><i class="fa fa-check"></i><b>4.1.1</b> Question 1</a></li>
<li class="chapter" data-level="4.1.2" data-path="classification.html"><a href="classification.html#question-2-2"><i class="fa fa-check"></i><b>4.1.2</b> Question 2</a></li>
<li class="chapter" data-level="4.1.3" data-path="classification.html"><a href="classification.html#question-3-2"><i class="fa fa-check"></i><b>4.1.3</b> Question 3</a></li>
<li class="chapter" data-level="4.1.4" data-path="classification.html"><a href="classification.html#question-4-2"><i class="fa fa-check"></i><b>4.1.4</b> Question 4</a></li>
<li class="chapter" data-level="4.1.5" data-path="classification.html"><a href="classification.html#question-5-2"><i class="fa fa-check"></i><b>4.1.5</b> Question 5</a></li>
<li class="chapter" data-level="4.1.6" data-path="classification.html"><a href="classification.html#question-6-2"><i class="fa fa-check"></i><b>4.1.6</b> Question 6</a></li>
<li class="chapter" data-level="4.1.7" data-path="classification.html"><a href="classification.html#question-7-2"><i class="fa fa-check"></i><b>4.1.7</b> Question 7</a></li>
<li class="chapter" data-level="4.1.8" data-path="classification.html"><a href="classification.html#question-8-2"><i class="fa fa-check"></i><b>4.1.8</b> Question 8</a></li>
<li class="chapter" data-level="4.1.9" data-path="classification.html"><a href="classification.html#question-9-2"><i class="fa fa-check"></i><b>4.1.9</b> Question 9</a></li>
<li class="chapter" data-level="4.1.10" data-path="classification.html"><a href="classification.html#question-10-2"><i class="fa fa-check"></i><b>4.1.10</b> Question 10</a></li>
<li class="chapter" data-level="4.1.11" data-path="classification.html"><a href="classification.html#question-11-1"><i class="fa fa-check"></i><b>4.1.11</b> Question 11</a></li>
<li class="chapter" data-level="4.1.12" data-path="classification.html"><a href="classification.html#question-12-1"><i class="fa fa-check"></i><b>4.1.12</b> Question 12</a></li>
</ul></li>
<li class="chapter" data-level="4.2" data-path="classification.html"><a href="classification.html#applied-2"><i class="fa fa-check"></i><b>4.2</b> Applied</a>
<ul>
<li class="chapter" data-level="4.2.1" data-path="classification.html"><a href="classification.html#question-13-1"><i class="fa fa-check"></i><b>4.2.1</b> Question 13</a></li>
<li class="chapter" data-level="4.2.2" data-path="classification.html"><a href="classification.html#question-14-1"><i class="fa fa-check"></i><b>4.2.2</b> Question 14</a></li>
<li class="chapter" data-level="4.2.3" data-path="classification.html"><a href="classification.html#question-15-1"><i class="fa fa-check"></i><b>4.2.3</b> Question 15</a></li>
<li class="chapter" data-level="4.2.4" data-path="classification.html"><a href="classification.html#question-13-2"><i class="fa fa-check"></i><b>4.2.4</b> Question 13</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="5" data-path="resampling-methods.html"><a href="resampling-methods.html"><i class="fa fa-check"></i><b>5</b> Resampling Methods</a>
<ul>
<li class="chapter" data-level="5.1" data-path="resampling-methods.html"><a href="resampling-methods.html#conceptual-3"><i class="fa fa-check"></i><b>5.1</b> Conceptual</a>
<ul>
<li class="chapter" data-level="5.1.1" data-path="resampling-methods.html"><a href="resampling-methods.html#question-1-3"><i class="fa fa-check"></i><b>5.1.1</b> Question 1</a></li>
<li class="chapter" data-level="5.1.2" data-path="resampling-methods.html"><a href="resampling-methods.html#question-2-3"><i class="fa fa-check"></i><b>5.1.2</b> Question 2</a></li>
<li class="chapter" data-level="5.1.3" data-path="resampling-methods.html"><a href="resampling-methods.html#question-3-3"><i class="fa fa-check"></i><b>5.1.3</b> Question 3</a></li>
<li class="chapter" data-level="5.1.4" data-path="resampling-methods.html"><a href="resampling-methods.html#question-4-3"><i class="fa fa-check"></i><b>5.1.4</b> Question 4</a></li>
</ul></li>
<li class="chapter" data-level="5.2" data-path="resampling-methods.html"><a href="resampling-methods.html#applied-3"><i class="fa fa-check"></i><b>5.2</b> Applied</a>
<ul>
<li class="chapter" data-level="5.2.1" data-path="resampling-methods.html"><a href="resampling-methods.html#question-5-3"><i class="fa fa-check"></i><b>5.2.1</b> Question 5</a></li>
<li class="chapter" data-level="5.2.2" data-path="resampling-methods.html"><a href="resampling-methods.html#question-6-3"><i class="fa fa-check"></i><b>5.2.2</b> Question 6</a></li>
<li class="chapter" data-level="5.2.3" data-path="resampling-methods.html"><a href="resampling-methods.html#question-7-3"><i class="fa fa-check"></i><b>5.2.3</b> Question 7</a></li>
<li class="chapter" data-level="5.2.4" data-path="resampling-methods.html"><a href="resampling-methods.html#question-8-3"><i class="fa fa-check"></i><b>5.2.4</b> Question 8</a></li>
<li class="chapter" data-level="5.2.5" data-path="resampling-methods.html"><a href="resampling-methods.html#question-9-3"><i class="fa fa-check"></i><b>5.2.5</b> Question 9</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="6" data-path="linear-model-selection-and-regularization.html"><a href="linear-model-selection-and-regularization.html"><i class="fa fa-check"></i><b>6</b> Linear Model Selection and Regularization</a>
<ul>
<li class="chapter" data-level="6.1" data-path="linear-model-selection-and-regularization.html"><a href="linear-model-selection-and-regularization.html#conceptual-4"><i class="fa fa-check"></i><b>6.1</b> Conceptual</a>
<ul>
<li class="chapter" data-level="6.1.1" data-path="linear-model-selection-and-regularization.html"><a href="linear-model-selection-and-regularization.html#question-1-4"><i class="fa fa-check"></i><b>6.1.1</b> Question 1</a></li>
<li class="chapter" data-level="6.1.2" data-path="linear-model-selection-and-regularization.html"><a href="linear-model-selection-and-regularization.html#question-2-4"><i class="fa fa-check"></i><b>6.1.2</b> Question 2</a></li>
<li class="chapter" data-level="6.1.3" data-path="linear-model-selection-and-regularization.html"><a href="linear-model-selection-and-regularization.html#question-3-4"><i class="fa fa-check"></i><b>6.1.3</b> Question 3</a></li>
<li class="chapter" data-level="6.1.4" data-path="linear-model-selection-and-regularization.html"><a href="linear-model-selection-and-regularization.html#question-4-4"><i class="fa fa-check"></i><b>6.1.4</b> Question 4</a></li>
<li class="chapter" data-level="6.1.5" data-path="linear-model-selection-and-regularization.html"><a href="linear-model-selection-and-regularization.html#question-5-4"><i class="fa fa-check"></i><b>6.1.5</b> Question 5</a></li>
<li class="chapter" data-level="6.1.6" data-path="linear-model-selection-and-regularization.html"><a href="linear-model-selection-and-regularization.html#question-6-4"><i class="fa fa-check"></i><b>6.1.6</b> Question 6</a></li>
<li class="chapter" data-level="6.1.7" data-path="linear-model-selection-and-regularization.html"><a href="linear-model-selection-and-regularization.html#question-7-4"><i class="fa fa-check"></i><b>6.1.7</b> Question 7</a></li>
</ul></li>
<li class="chapter" data-level="6.2" data-path="linear-model-selection-and-regularization.html"><a href="linear-model-selection-and-regularization.html#applied-4"><i class="fa fa-check"></i><b>6.2</b> Applied</a>
<ul>
<li class="chapter" data-level="6.2.1" data-path="linear-model-selection-and-regularization.html"><a href="linear-model-selection-and-regularization.html#question-8-4"><i class="fa fa-check"></i><b>6.2.1</b> Question 8</a></li>
<li class="chapter" data-level="6.2.2" data-path="linear-model-selection-and-regularization.html"><a href="linear-model-selection-and-regularization.html#question-9-4"><i class="fa fa-check"></i><b>6.2.2</b> Question 9</a></li>
<li class="chapter" data-level="6.2.3" data-path="linear-model-selection-and-regularization.html"><a href="linear-model-selection-and-regularization.html#question-10-3"><i class="fa fa-check"></i><b>6.2.3</b> Question 10</a></li>
<li class="chapter" data-level="6.2.4" data-path="linear-model-selection-and-regularization.html"><a href="linear-model-selection-and-regularization.html#question-11-2"><i class="fa fa-check"></i><b>6.2.4</b> Question 11</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="7" data-path="moving-beyond-linearity.html"><a href="moving-beyond-linearity.html"><i class="fa fa-check"></i><b>7</b> Moving Beyond Linearity</a>
<ul>
<li class="chapter" data-level="7.1" data-path="moving-beyond-linearity.html"><a href="moving-beyond-linearity.html#conceptual-5"><i class="fa fa-check"></i><b>7.1</b> Conceptual</a>
<ul>
<li class="chapter" data-level="7.1.1" data-path="moving-beyond-linearity.html"><a href="moving-beyond-linearity.html#question-1-5"><i class="fa fa-check"></i><b>7.1.1</b> Question 1</a></li>
<li class="chapter" data-level="7.1.2" data-path="moving-beyond-linearity.html"><a href="moving-beyond-linearity.html#question-2-5"><i class="fa fa-check"></i><b>7.1.2</b> Question 2</a></li>
<li class="chapter" data-level="7.1.3" data-path="moving-beyond-linearity.html"><a href="moving-beyond-linearity.html#question-3-5"><i class="fa fa-check"></i><b>7.1.3</b> Question 3</a></li>
<li class="chapter" data-level="7.1.4" data-path="moving-beyond-linearity.html"><a href="moving-beyond-linearity.html#question-4-5"><i class="fa fa-check"></i><b>7.1.4</b> Question 4</a></li>
<li class="chapter" data-level="7.1.5" data-path="moving-beyond-linearity.html"><a href="moving-beyond-linearity.html#question-5-5"><i class="fa fa-check"></i><b>7.1.5</b> Question 5</a></li>
</ul></li>
<li class="chapter" data-level="7.2" data-path="moving-beyond-linearity.html"><a href="moving-beyond-linearity.html#applied-5"><i class="fa fa-check"></i><b>7.2</b> Applied</a>
<ul>
<li class="chapter" data-level="7.2.1" data-path="moving-beyond-linearity.html"><a href="moving-beyond-linearity.html#question-6-5"><i class="fa fa-check"></i><b>7.2.1</b> Question 6</a></li>
<li class="chapter" data-level="7.2.2" data-path="moving-beyond-linearity.html"><a href="moving-beyond-linearity.html#question-7-5"><i class="fa fa-check"></i><b>7.2.2</b> Question 7</a></li>
<li class="chapter" data-level="7.2.3" data-path="moving-beyond-linearity.html"><a href="moving-beyond-linearity.html#question-8-5"><i class="fa fa-check"></i><b>7.2.3</b> Question 8</a></li>
<li class="chapter" data-level="7.2.4" data-path="moving-beyond-linearity.html"><a href="moving-beyond-linearity.html#question-9-5"><i class="fa fa-check"></i><b>7.2.4</b> Question 9</a></li>
<li class="chapter" data-level="7.2.5" data-path="moving-beyond-linearity.html"><a href="moving-beyond-linearity.html#question-10-4"><i class="fa fa-check"></i><b>7.2.5</b> Question 10</a></li>
<li class="chapter" data-level="7.2.6" data-path="moving-beyond-linearity.html"><a href="moving-beyond-linearity.html#question-11-3"><i class="fa fa-check"></i><b>7.2.6</b> Question 11</a></li>
<li class="chapter" data-level="7.2.7" data-path="moving-beyond-linearity.html"><a href="moving-beyond-linearity.html#question-12-2"><i class="fa fa-check"></i><b>7.2.7</b> Question 12</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="8" data-path="tree-based-methods.html"><a href="tree-based-methods.html"><i class="fa fa-check"></i><b>8</b> Tree-Based Methods</a>
<ul>
<li class="chapter" data-level="8.1" data-path="tree-based-methods.html"><a href="tree-based-methods.html#conceptual-6"><i class="fa fa-check"></i><b>8.1</b> Conceptual</a>
<ul>
<li class="chapter" data-level="8.1.1" data-path="tree-based-methods.html"><a href="tree-based-methods.html#question-1-6"><i class="fa fa-check"></i><b>8.1.1</b> Question 1</a></li>
<li class="chapter" data-level="8.1.2" data-path="tree-based-methods.html"><a href="tree-based-methods.html#question-2-6"><i class="fa fa-check"></i><b>8.1.2</b> Question 2</a></li>
<li class="chapter" data-level="8.1.3" data-path="tree-based-methods.html"><a href="tree-based-methods.html#question-3-6"><i class="fa fa-check"></i><b>8.1.3</b> Question 3</a></li>
<li class="chapter" data-level="8.1.4" data-path="tree-based-methods.html"><a href="tree-based-methods.html#question-4-6"><i class="fa fa-check"></i><b>8.1.4</b> Question 4</a></li>
<li class="chapter" data-level="8.1.5" data-path="tree-based-methods.html"><a href="tree-based-methods.html#question-5-6"><i class="fa fa-check"></i><b>8.1.5</b> Question 5</a></li>
<li class="chapter" data-level="8.1.6" data-path="tree-based-methods.html"><a href="tree-based-methods.html#question-6-6"><i class="fa fa-check"></i><b>8.1.6</b> Question 6</a></li>
</ul></li>
<li class="chapter" data-level="8.2" data-path="tree-based-methods.html"><a href="tree-based-methods.html#applied-6"><i class="fa fa-check"></i><b>8.2</b> Applied</a>
<ul>
<li class="chapter" data-level="8.2.1" data-path="tree-based-methods.html"><a href="tree-based-methods.html#question-7-6"><i class="fa fa-check"></i><b>8.2.1</b> Question 7</a></li>
<li class="chapter" data-level="8.2.2" data-path="tree-based-methods.html"><a href="tree-based-methods.html#question-8-6"><i class="fa fa-check"></i><b>8.2.2</b> Question 8</a></li>
<li class="chapter" data-level="8.2.3" data-path="tree-based-methods.html"><a href="tree-based-methods.html#question-9-6"><i class="fa fa-check"></i><b>8.2.3</b> Question 9</a></li>
<li class="chapter" data-level="8.2.4" data-path="tree-based-methods.html"><a href="tree-based-methods.html#question-10-5"><i class="fa fa-check"></i><b>8.2.4</b> Question 10</a></li>
<li class="chapter" data-level="8.2.5" data-path="tree-based-methods.html"><a href="tree-based-methods.html#question-11-4"><i class="fa fa-check"></i><b>8.2.5</b> Question 11</a></li>
<li class="chapter" data-level="8.2.6" data-path="tree-based-methods.html"><a href="tree-based-methods.html#question-12-3"><i class="fa fa-check"></i><b>8.2.6</b> Question 12</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="9" data-path="support-vector-machines.html"><a href="support-vector-machines.html"><i class="fa fa-check"></i><b>9</b> Support Vector Machines</a>
<ul>
<li class="chapter" data-level="9.1" data-path="support-vector-machines.html"><a href="support-vector-machines.html#conceptual-7"><i class="fa fa-check"></i><b>9.1</b> Conceptual</a>
<ul>
<li class="chapter" data-level="9.1.1" data-path="support-vector-machines.html"><a href="support-vector-machines.html#question-1-7"><i class="fa fa-check"></i><b>9.1.1</b> Question 1</a></li>
<li class="chapter" data-level="9.1.2" data-path="support-vector-machines.html"><a href="support-vector-machines.html#question-2-7"><i class="fa fa-check"></i><b>9.1.2</b> Question 2</a></li>
<li class="chapter" data-level="9.1.3" data-path="support-vector-machines.html"><a href="support-vector-machines.html#question-3-7"><i class="fa fa-check"></i><b>9.1.3</b> Question 3</a></li>
</ul></li>
<li class="chapter" data-level="9.2" data-path="support-vector-machines.html"><a href="support-vector-machines.html#applied-7"><i class="fa fa-check"></i><b>9.2</b> Applied</a>
<ul>
<li class="chapter" data-level="9.2.1" data-path="support-vector-machines.html"><a href="support-vector-machines.html#question-4-7"><i class="fa fa-check"></i><b>9.2.1</b> Question 4</a></li>
<li class="chapter" data-level="9.2.2" data-path="support-vector-machines.html"><a href="support-vector-machines.html#question-5-7"><i class="fa fa-check"></i><b>9.2.2</b> Question 5</a></li>
<li class="chapter" data-level="9.2.3" data-path="support-vector-machines.html"><a href="support-vector-machines.html#question-6-7"><i class="fa fa-check"></i><b>9.2.3</b> Question 6</a></li>
<li class="chapter" data-level="9.2.4" data-path="support-vector-machines.html"><a href="support-vector-machines.html#question-7-7"><i class="fa fa-check"></i><b>9.2.4</b> Question 7</a></li>
<li class="chapter" data-level="9.2.5" data-path="support-vector-machines.html"><a href="support-vector-machines.html#question-8-7"><i class="fa fa-check"></i><b>9.2.5</b> Question 8</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="10" data-path="deep-learning.html"><a href="deep-learning.html"><i class="fa fa-check"></i><b>10</b> Deep Learning</a>
<ul>
<li class="chapter" data-level="10.1" data-path="deep-learning.html"><a href="deep-learning.html#conceptual-8"><i class="fa fa-check"></i><b>10.1</b> Conceptual</a>
<ul>
<li class="chapter" data-level="10.1.1" data-path="deep-learning.html"><a href="deep-learning.html#question-1-8"><i class="fa fa-check"></i><b>10.1.1</b> Question 1</a></li>
<li class="chapter" data-level="10.1.2" data-path="deep-learning.html"><a href="deep-learning.html#question-2-8"><i class="fa fa-check"></i><b>10.1.2</b> Question 2</a></li>
<li class="chapter" data-level="10.1.3" data-path="deep-learning.html"><a href="deep-learning.html#question-3-8"><i class="fa fa-check"></i><b>10.1.3</b> Question 3</a></li>
<li class="chapter" data-level="10.1.4" data-path="deep-learning.html"><a href="deep-learning.html#question-4-8"><i class="fa fa-check"></i><b>10.1.4</b> Question 4</a></li>
<li class="chapter" data-level="10.1.5" data-path="deep-learning.html"><a href="deep-learning.html#question-5-8"><i class="fa fa-check"></i><b>10.1.5</b> Question 5</a></li>
</ul></li>
<li class="chapter" data-level="10.2" data-path="deep-learning.html"><a href="deep-learning.html#applied-8"><i class="fa fa-check"></i><b>10.2</b> Applied</a>
<ul>
<li class="chapter" data-level="10.2.1" data-path="deep-learning.html"><a href="deep-learning.html#question-6-8"><i class="fa fa-check"></i><b>10.2.1</b> Question 6</a></li>
<li class="chapter" data-level="10.2.2" data-path="deep-learning.html"><a href="deep-learning.html#question-7-8"><i class="fa fa-check"></i><b>10.2.2</b> Question 7</a></li>
<li class="chapter" data-level="10.2.3" data-path="deep-learning.html"><a href="deep-learning.html#question-8-8"><i class="fa fa-check"></i><b>10.2.3</b> Question 8</a></li>
<li class="chapter" data-level="10.2.4" data-path="deep-learning.html"><a href="deep-learning.html#question-9-7"><i class="fa fa-check"></i><b>10.2.4</b> Question 9</a></li>
<li class="chapter" data-level="10.2.5" data-path="deep-learning.html"><a href="deep-learning.html#question-10-6"><i class="fa fa-check"></i><b>10.2.5</b> Question 10</a></li>
<li class="chapter" data-level="10.2.6" data-path="deep-learning.html"><a href="deep-learning.html#question-11-5"><i class="fa fa-check"></i><b>10.2.6</b> Question 11</a></li>
<li class="chapter" data-level="10.2.7" data-path="deep-learning.html"><a href="deep-learning.html#question-12-4"><i class="fa fa-check"></i><b>10.2.7</b> Question 12</a></li>
<li class="chapter" data-level="10.2.8" data-path="deep-learning.html"><a href="deep-learning.html#question-13-3"><i class="fa fa-check"></i><b>10.2.8</b> Question 13</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="11" data-path="survival-analysis-and-censored-data.html"><a href="survival-analysis-and-censored-data.html"><i class="fa fa-check"></i><b>11</b> Survival Analysis and Censored Data</a>
<ul>
<li class="chapter" data-level="11.1" data-path="survival-analysis-and-censored-data.html"><a href="survival-analysis-and-censored-data.html#conceptual-9"><i class="fa fa-check"></i><b>11.1</b> Conceptual</a>
<ul>
<li class="chapter" data-level="11.1.1" data-path="survival-analysis-and-censored-data.html"><a href="survival-analysis-and-censored-data.html#question-1-9"><i class="fa fa-check"></i><b>11.1.1</b> Question 1</a></li>
<li class="chapter" data-level="11.1.2" data-path="survival-analysis-and-censored-data.html"><a href="survival-analysis-and-censored-data.html#question-2-9"><i class="fa fa-check"></i><b>11.1.2</b> Question 2</a></li>
<li class="chapter" data-level="11.1.3" data-path="survival-analysis-and-censored-data.html"><a href="survival-analysis-and-censored-data.html#question-3-9"><i class="fa fa-check"></i><b>11.1.3</b> Question 3</a></li>
<li class="chapter" data-level="11.1.4" data-path="survival-analysis-and-censored-data.html"><a href="survival-analysis-and-censored-data.html#question-4-9"><i class="fa fa-check"></i><b>11.1.4</b> Question 4</a></li>
<li class="chapter" data-level="11.1.5" data-path="survival-analysis-and-censored-data.html"><a href="survival-analysis-and-censored-data.html#question-5-9"><i class="fa fa-check"></i><b>11.1.5</b> Question 5</a></li>
<li class="chapter" data-level="11.1.6" data-path="survival-analysis-and-censored-data.html"><a href="survival-analysis-and-censored-data.html#question-6-9"><i class="fa fa-check"></i><b>11.1.6</b> Question 6</a></li>
<li class="chapter" data-level="11.1.7" data-path="survival-analysis-and-censored-data.html"><a href="survival-analysis-and-censored-data.html#question-7-9"><i class="fa fa-check"></i><b>11.1.7</b> Question 7</a></li>
<li class="chapter" data-level="11.1.8" data-path="survival-analysis-and-censored-data.html"><a href="survival-analysis-and-censored-data.html#question-8-9"><i class="fa fa-check"></i><b>11.1.8</b> Question 8</a></li>
<li class="chapter" data-level="11.1.9" data-path="survival-analysis-and-censored-data.html"><a href="survival-analysis-and-censored-data.html#question-9-8"><i class="fa fa-check"></i><b>11.1.9</b> Question 9</a></li>
</ul></li>
<li class="chapter" data-level="11.2" data-path="survival-analysis-and-censored-data.html"><a href="survival-analysis-and-censored-data.html#applied-9"><i class="fa fa-check"></i><b>11.2</b> Applied</a>
<ul>
<li class="chapter" data-level="11.2.1" data-path="survival-analysis-and-censored-data.html"><a href="survival-analysis-and-censored-data.html#question-10-7"><i class="fa fa-check"></i><b>11.2.1</b> Question 10</a></li>
<li class="chapter" data-level="11.2.2" data-path="survival-analysis-and-censored-data.html"><a href="survival-analysis-and-censored-data.html#question-11-6"><i class="fa fa-check"></i><b>11.2.2</b> Question 11</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="12" data-path="unsupervised-learning.html"><a href="unsupervised-learning.html"><i class="fa fa-check"></i><b>12</b> Unsupervised Learning</a>
<ul>
<li class="chapter" data-level="12.1" data-path="unsupervised-learning.html"><a href="unsupervised-learning.html#conceptual-10"><i class="fa fa-check"></i><b>12.1</b> Conceptual</a>
<ul>
<li class="chapter" data-level="12.1.1" data-path="unsupervised-learning.html"><a href="unsupervised-learning.html#question-1-10"><i class="fa fa-check"></i><b>12.1.1</b> Question 1</a></li>
<li class="chapter" data-level="12.1.2" data-path="unsupervised-learning.html"><a href="unsupervised-learning.html#question-2-10"><i class="fa fa-check"></i><b>12.1.2</b> Question 2</a></li>
<li class="chapter" data-level="12.1.3" data-path="unsupervised-learning.html"><a href="unsupervised-learning.html#question-3-10"><i class="fa fa-check"></i><b>12.1.3</b> Question 3</a></li>
<li class="chapter" data-level="12.1.4" data-path="unsupervised-learning.html"><a href="unsupervised-learning.html#question-4-10"><i class="fa fa-check"></i><b>12.1.4</b> Question 4</a></li>
<li class="chapter" data-level="12.1.5" data-path="unsupervised-learning.html"><a href="unsupervised-learning.html#question-5-10"><i class="fa fa-check"></i><b>12.1.5</b> Question 5</a></li>
<li class="chapter" data-level="12.1.6" data-path="unsupervised-learning.html"><a href="unsupervised-learning.html#question-6-10"><i class="fa fa-check"></i><b>12.1.6</b> Question 6</a></li>
</ul></li>
<li class="chapter" data-level="12.2" data-path="unsupervised-learning.html"><a href="unsupervised-learning.html#applied-10"><i class="fa fa-check"></i><b>12.2</b> Applied</a>
<ul>
<li class="chapter" data-level="12.2.1" data-path="unsupervised-learning.html"><a href="unsupervised-learning.html#question-7-10"><i class="fa fa-check"></i><b>12.2.1</b> Question 7</a></li>
<li class="chapter" data-level="12.2.2" data-path="unsupervised-learning.html"><a href="unsupervised-learning.html#question-8-10"><i class="fa fa-check"></i><b>12.2.2</b> Question 8</a></li>
<li class="chapter" data-level="12.2.3" data-path="unsupervised-learning.html"><a href="unsupervised-learning.html#question-9-9"><i class="fa fa-check"></i><b>12.2.3</b> Question 9</a></li>
<li class="chapter" data-level="12.2.4" data-path="unsupervised-learning.html"><a href="unsupervised-learning.html#question-10-8"><i class="fa fa-check"></i><b>12.2.4</b> Question 10</a></li>
<li class="chapter" data-level="12.2.5" data-path="unsupervised-learning.html"><a href="unsupervised-learning.html#question-11-7"><i class="fa fa-check"></i><b>12.2.5</b> Question 11</a></li>
<li class="chapter" data-level="12.2.6" data-path="unsupervised-learning.html"><a href="unsupervised-learning.html#question-12-5"><i class="fa fa-check"></i><b>12.2.6</b> Question 12</a></li>
<li class="chapter" data-level="12.2.7" data-path="unsupervised-learning.html"><a href="unsupervised-learning.html#question-13-4"><i class="fa fa-check"></i><b>12.2.7</b> Question 13</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="13" data-path="multiple-testing.html"><a href="multiple-testing.html"><i class="fa fa-check"></i><b>13</b> Multiple Testing</a>
<ul>
<li class="chapter" data-level="13.1" data-path="multiple-testing.html"><a href="multiple-testing.html#conceptual-11"><i class="fa fa-check"></i><b>13.1</b> Conceptual</a>
<ul>
<li class="chapter" data-level="13.1.1" data-path="multiple-testing.html"><a href="multiple-testing.html#question-1-11"><i class="fa fa-check"></i><b>13.1.1</b> Question 1</a></li>
<li class="chapter" data-level="13.1.2" data-path="multiple-testing.html"><a href="multiple-testing.html#question-2-11"><i class="fa fa-check"></i><b>13.1.2</b> Question 2</a></li>
<li class="chapter" data-level="13.1.3" data-path="multiple-testing.html"><a href="multiple-testing.html#question-3-11"><i class="fa fa-check"></i><b>13.1.3</b> Question 3</a></li>
<li class="chapter" data-level="13.1.4" data-path="multiple-testing.html"><a href="multiple-testing.html#question-4-11"><i class="fa fa-check"></i><b>13.1.4</b> Question 4</a></li>
<li class="chapter" data-level="13.1.5" data-path="multiple-testing.html"><a href="multiple-testing.html#question-5-11"><i class="fa fa-check"></i><b>13.1.5</b> Question 5</a></li>
<li class="chapter" data-level="13.1.6" data-path="multiple-testing.html"><a href="multiple-testing.html#question-6-11"><i class="fa fa-check"></i><b>13.1.6</b> Question 6</a></li>
</ul></li>
<li class="chapter" data-level="13.2" data-path="multiple-testing.html"><a href="multiple-testing.html#applied-11"><i class="fa fa-check"></i><b>13.2</b> Applied</a>
<ul>
<li class="chapter" data-level="13.2.1" data-path="multiple-testing.html"><a href="multiple-testing.html#question-7-11"><i class="fa fa-check"></i><b>13.2.1</b> Question 7</a></li>
<li class="chapter" data-level="13.2.2" data-path="multiple-testing.html"><a href="multiple-testing.html#question-8-11"><i class="fa fa-check"></i><b>13.2.2</b> Question 8</a></li>
</ul></li>
</ul></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">An Introduction to Statistical Learning</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="classification" class="section level1 hasAnchor" number="4">
<h1><span class="header-section-number">4</span> Classification<a href="classification.html#classification" class="anchor-section" aria-label="Anchor link to header"></a></h1>
<div id="conceptual-2" class="section level2 hasAnchor" number="4.1">
<h2><span class="header-section-number">4.1</span> Conceptual<a href="classification.html#conceptual-2" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<div id="question-1-2" class="section level3 hasAnchor" number="4.1.1">
<h3><span class="header-section-number">4.1.1</span> Question 1<a href="classification.html#question-1-2" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<blockquote>
<p>Using a little bit of algebra, prove that (4.2) is equivalent to (4.3). In
other words, the logistic function representation and logit representation
for the logistic regression model are equivalent.</p>
</blockquote>
<p>We need to show that</p>
<p><span class="math display">\[
p(X) = \frac{e^{\beta_0 + \beta_1X}}{1 + e^{\beta_0 + \beta_1X}}
\]</span></p>
<p>is equivalent to</p>
<p><span class="math display">\[
\frac{p(X)}{1-p(X)} = e^{\beta_0 + \beta_1X}
\]</span></p>
<p>Letting <span class="math inline">\(x = e^{\beta_0 + \beta_1X}\)</span></p>
<p><span class="math display">\[\begin{align}
\frac{P(X)}{1-p(X)} &amp;= \frac{\frac{x}{1 + x}}
                            {1 - \frac{x}{1 + x}} \\
              &amp;= \frac{\frac{x}{1 + x}}
                      {\frac{1}{1 + x}} \\
              &amp;= x
\end{align}\]</span></p>
</div>
<div id="question-2-2" class="section level3 hasAnchor" number="4.1.2">
<h3><span class="header-section-number">4.1.2</span> Question 2<a href="classification.html#question-2-2" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<blockquote>
<p>It was stated in the text that classifying an observation to the class for
which (4.12) is largest is equivalent to classifying an observation to the
class for which (4.13) is largest. Prove that this is the case. In other
words, under the assumption that the observations in the <span class="math inline">\(k\)</span>th class are
drawn from a <span class="math inline">\(N(\mu_k,\sigma^2)\)</span> distribution, the Bayes’ classifier assigns
an observation to the class for which the discriminant function is maximized.</p>
</blockquote>
<p>4.12 is</p>
<p><span class="math display">\[
p_k(x) = \frac{\pi_k\frac{1}{\sqrt{2\pi\sigma}} \exp(-\frac{1}{2\sigma^2}(x - \mu_k)^2)}
              {\sum_{l=1}^k \pi_l\frac{1}{\sqrt{2\pi\sigma}} \exp(-\frac{1}{2\sigma^2}(x - \mu_l)^2)}
\]</span></p>
<p>and the discriminant function is</p>
<p><span class="math display">\[
\delta_k(x) = x.\frac{\mu_k}{\sigma^2} - \frac{\mu_k^2}{2\sigma_2} + \log(\pi_k)
\]</span></p>
<p>Since <span class="math inline">\(\sigma^2\)</span> is constant</p>
<p><span class="math display">\[
p_k(x) = \frac{\pi_k \exp\left(-\frac{1}{2\sigma^2}(x - \mu_k)^2\right)}
              {\sum_{l=1}^k \pi_l \exp\left(-\frac{1}{2\sigma^2}(x - \mu_l)^2\right)}
\]</span></p>
<p>Maximizing <span class="math inline">\(p_k(x)\)</span> also maximizes any monotonic function of <span class="math inline">\(p_k(X)\)</span>, and
therefore, we can consider maximizing <span class="math inline">\(\log(p_K(X))\)</span></p>
<p><span class="math display">\[
\log(p_k(x)) = \log(\pi_k) - \frac{1}{2\sigma^2}(x - \mu_k)^2 -
              \log\left(\sum_{l=1}^k \pi_l \exp\left(-\frac{1}{2\sigma^2}(x - \mu_l)^2\right)\right)
\]</span></p>
<p>Remember that we are maximizing over <span class="math inline">\(k\)</span>, and since the last term does not
vary with <span class="math inline">\(k\)</span> it can be ignored. So we just need to maximize</p>
<p><span class="math display">\[\begin{align}
f &amp;= \log(\pi_k) - \frac{1}{2\sigma^2} (x^2 - 2x\mu_k + \mu_k^2) \\
  &amp;= \log(\pi_k) - \frac{x^2}{2\sigma^2} + \frac{x\mu_k}{\sigma^2} - \frac{\mu_k^2}{2\sigma^2} \\
\end{align}\]</span></p>
<p>Since <span class="math inline">\(\frac{x^2}{2\sigma^2}\)</span> is also independent of <span class="math inline">\(k\)</span>, we just need to
maximize</p>
<p><span class="math display">\[
\log(\pi_k) + \frac{x\mu_k}{\sigma^2} - \frac{\mu_k^2}{2\sigma^2}
\]</span></p>
</div>
<div id="question-3-2" class="section level3 hasAnchor" number="4.1.3">
<h3><span class="header-section-number">4.1.3</span> Question 3<a href="classification.html#question-3-2" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<blockquote>
<p>This problem relates to the QDA model, in which the observations within each
class are drawn from a normal distribution with a class-specific mean vector
and a class specific covariance matrix. We consider the simple case where <span class="math inline">\(p = 1\)</span>; i.e. there is only one feature.</p>
<p>Suppose that we have <span class="math inline">\(K\)</span> classes, and that if an observation belongs to the
<span class="math inline">\(k\)</span>th class then <span class="math inline">\(X\)</span> comes from a one-dimensional normal distribution,
<span class="math inline">\(X \sim N(\mu_k,\sigma^2)\)</span>. Recall that the density function for the
one-dimensional normal distribution is given in (4.16). Prove that in this
case, the Bayes classifier is <em>not</em> linear. Argue that it is in fact
quadratic.</p>
<p><em>Hint: For this problem, you should follow the arguments laid out in</em>
<em>Section 4.4.1, but without making the assumption that</em>
<em><span class="math inline">\(\sigma_1^2 = ... = \sigma_K^2\)</span>.</em></p>
</blockquote>
<p>As above,</p>
<p><span class="math display">\[
p_k(x) = \frac{\pi_k\frac{1}{\sqrt{2\pi\sigma_k}} \exp(-\frac{1}{2\sigma_k^2}(x - \mu_k)^2)}
              {\sum_{l=1}^k \pi_l\frac{1}{\sqrt{2\pi\sigma_l}} \exp(-\frac{1}{2\sigma_l^2}(x - \mu_l)^2)}
\]</span></p>
<p>Now lets derive the Bayes classifier, without assuming
<span class="math inline">\(\sigma_1^2 = ... = \sigma_K^2\)</span></p>
<p>Maximizing <span class="math inline">\(p_k(x)\)</span> also maximizes any monotonic function of <span class="math inline">\(p_k(X)\)</span>, and
therefore, we can consider maximizing <span class="math inline">\(\log(p_K(X))\)</span></p>
<p><span class="math display">\[
\log(p_k(x)) = \log(\pi_k) + \log\left(\frac{1}{\sqrt{2\pi\sigma_k}}\right) - \frac{1}{2\sigma_k^2}(x - \mu_k)^2 -
              \log\left(\sum_{l=1}^k \frac{1}{\sqrt{2\pi\sigma_l}} \pi_l \exp\left(-\frac{1}{2\sigma_l^2}(x - \mu_l)^2\right)\right)
\]</span></p>
<p>Remember that we are maximizing over <span class="math inline">\(k\)</span>, and since the last term does not
vary with <span class="math inline">\(k\)</span> it can be ignored. So we just need to maximize</p>
<p><span class="math display">\[\begin{align}
f &amp;= \log(\pi_k) + \log\left(\frac{1}{\sqrt{2\pi\sigma_k}}\right) - \frac{1}{2\sigma_k^2}(x - \mu_k)^2 \\
  &amp;= \log(\pi_k) + \log\left(\frac{1}{\sqrt{2\pi\sigma_k}}\right) - \frac{x^2}{2\sigma_k^2} + \frac{x\mu_k}{\sigma_k^2} - \frac{\mu_k^2}{2\sigma_k^2}  \\
\end{align}\]</span></p>
<p>However, unlike in Q2, <span class="math inline">\(\frac{x^2}{2\sigma_k^2}\)</span> is not independent of <span class="math inline">\(k\)</span>, so
we retain the term with <span class="math inline">\(x^2\)</span>, hence <span class="math inline">\(f\)</span>, the Bayes’ classifier, is a
quadratic function of <span class="math inline">\(x\)</span>.</p>
</div>
<div id="question-4-2" class="section level3 hasAnchor" number="4.1.4">
<h3><span class="header-section-number">4.1.4</span> Question 4<a href="classification.html#question-4-2" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<blockquote>
<p>When the number of features <span class="math inline">\(p\)</span> is large, there tends to be a deterioration
in the performance of KNN and other <em>local</em> approaches that perform
prediction using only observations that are <em>near</em> the test observation for
which a prediction must be made. This phenomenon is known as the
<em>curse of dimensionality</em>, and it ties into the fact that non-parametric
approaches often perform poorly when <span class="math inline">\(p\)</span> is large. We will now investigate
this curse.</p>
<ol style="list-style-type: lower-alpha">
<li>Suppose that we have a set of observations, each with measurements on
<span class="math inline">\(p = 1\)</span> feature, <span class="math inline">\(X\)</span>. We assume that <span class="math inline">\(X\)</span> is uniformly (evenly) distributed
on <span class="math inline">\([0, 1]\)</span>. Associated with each observation is a response value. Suppose
that we wish to predict a test observation’s response using only
observations that are within 10% of the range of <span class="math inline">\(X\)</span> closest to that test
observation. For instance, in order to predict the response for a test
observation with <span class="math inline">\(X = 0.6\)</span>, we will use observations in the range
<span class="math inline">\([0.55, 0.65]\)</span>. On average, what fraction of the available observations
will we use to make the prediction?</li>
</ol>
</blockquote>
<p>For values in <span class="math inline">\(0...0.05\)</span>, we use less than 10% of observations (between 5% and
10%, 7.5% on average), similarly with values in <span class="math inline">\(0.95...1\)</span>. For values in
<span class="math inline">\(0.05...0.95\)</span> we use 10% of available observations. The (weighted) average is
then <span class="math inline">\(7.5 \times 0.1 + 10 \times 0.9 = 9.75\%\)</span>.</p>
<blockquote>
<ol start="2" style="list-style-type: lower-alpha">
<li>Now suppose that we have a set of observations, each with measurements on
<span class="math inline">\(p = 2\)</span> features, <span class="math inline">\(X_1\)</span> and <span class="math inline">\(X_2\)</span>. We assume that <span class="math inline">\((X_1, X_2)\)</span> are
uniformly distributed on <span class="math inline">\([0, 1] \times [0, 1]\)</span>. We wish to predict a test
observation’s response using only observations that are within 10% of the
range of <span class="math inline">\(X_1\)</span> <em>and</em> within 10% of the range of <span class="math inline">\(X_2\)</span> closest to that test
observation. For instance, in order to predict the response for a test
observation with <span class="math inline">\(X_1 = 0.6\)</span> and <span class="math inline">\(X_2 = 0.35\)</span>, we will use observations in
the range <span class="math inline">\([0.55, 0.65]\)</span> for <span class="math inline">\(X_1\)</span> and in the range <span class="math inline">\([0.3, 0.4]\)</span> for <span class="math inline">\(X_2\)</span>.
On average, what fraction of the available observations will we use to
make the prediction?</li>
</ol>
</blockquote>
<p>Since we need the observation to be within range for <span class="math inline">\(X_1\)</span> and <span class="math inline">\(X_2\)</span> we square
9.75% = <span class="math inline">\(0.0975^2 \times 100 = 0.95\%\)</span></p>
<blockquote>
<ol start="3" style="list-style-type: lower-alpha">
<li>Now suppose that we have a set of observations on <span class="math inline">\(p = 100\)</span> features. Again
the observations are uniformly distributed on each feature, and again each
feature ranges in value from 0 to 1. We wish to predict a test
observation’s response using observations within the 10% of each feature’s
range that is closest to that test observation. What fraction of the
available observations will we use to make the prediction?</li>
</ol>
</blockquote>
<p>Similar to above, we use: <span class="math inline">\(0.0975^{100} \times 100 = 8 \times 10^{-100}\%\)</span>,
essentially zero.</p>
<blockquote>
<ol start="4" style="list-style-type: lower-alpha">
<li>Using your answers to parts (a)–(c), argue that a drawback of KNN when
<span class="math inline">\(p\)</span> is large is that there are very few training observations “near” any
given test observation.</li>
</ol>
</blockquote>
<p>As <span class="math inline">\(p\)</span> increases, the fraction of observations near any given point rapidly
approaches zero. For instance, even if you use 50% of the nearest observations
for each <span class="math inline">\(p\)</span>, with <span class="math inline">\(p = 10\)</span>, only <span class="math inline">\(0.5^{10} \times 100 \approx 0.1\%\)</span> points are
“near”.</p>
<blockquote>
<ol start="5" style="list-style-type: lower-alpha">
<li>Now suppose that we wish to make a prediction for a test observation by
creating a <span class="math inline">\(p\)</span>-dimensional hypercube centered around the test observation
that contains, on average, 10% of the training observations. For
<span class="math inline">\(p = 1,2,\)</span> and <span class="math inline">\(100\)</span>, what is the length of each side of the hypercube?
Comment on your answer.</li>
</ol>
<p><em>Note: A hypercube is a generalization of a cube to an arbitrary number of</em>
<em>dimensions. When <span class="math inline">\(p = 1\)</span>, a hypercube is simply a line segment, when <span class="math inline">\(p = 2\)</span></em>
<em>it is a square, and when <span class="math inline">\(p = 100\)</span> it is a 100-dimensional cube.</em></p>
</blockquote>
<p>When <span class="math inline">\(p = 1\)</span>, clearly the length is 0.1.
When <span class="math inline">\(p = 2\)</span>, we need the value <span class="math inline">\(l\)</span> such that <span class="math inline">\(l^2 = 0.1\)</span>, so
<span class="math inline">\(l = \sqrt{0.1} = 0.32\)</span>.
When <span class="math inline">\(p = n\)</span>, <span class="math inline">\(l = 0.1^{1/n}\)</span>, so in the case of <span class="math inline">\(n = 100\)</span>, <span class="math inline">\(l = 0.98\)</span>.
Therefore, the length of each side of the hypercube rapidly approaches 1
(or 100%) of the range of each <span class="math inline">\(p\)</span>.</p>
</div>
<div id="question-5-2" class="section level3 hasAnchor" number="4.1.5">
<h3><span class="header-section-number">4.1.5</span> Question 5<a href="classification.html#question-5-2" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<blockquote>
<p>We now examine the differences between LDA and QDA.</p>
<ol style="list-style-type: lower-alpha">
<li>If the Bayes decision boundary is linear, do we expect LDA or QDA to
perform better on the training set? On the test set?</li>
</ol>
</blockquote>
<p>QDA, being a more flexible model, will always perform better on the training
set, but LDA would be expected to perform better on the test set.</p>
<blockquote>
<ol start="2" style="list-style-type: lower-alpha">
<li>If the Bayes decision boundary is non-linear, do we expect LDA or QDA to
perform better on the training set? On the test set?</li>
</ol>
</blockquote>
<p>QDA, being a more flexible model, will perform better on the training
set, and we would hope that extra flexibility translates to a better fit on
the test set.</p>
<blockquote>
<ol start="3" style="list-style-type: lower-alpha">
<li>In general, as the sample size <span class="math inline">\(n\)</span> increases, do we expect the test
prediction accuracy of QDA relative to LDA to improve, decline, or be
unchanged? Why?</li>
</ol>
</blockquote>
<p>As <span class="math inline">\(n\)</span> increases, we would expect the prediction accuracy of QDA relative to
LDA to improve as there is more data to fit to subtle effects in the data.</p>
<blockquote>
<ol start="4" style="list-style-type: lower-alpha">
<li>True or False: Even if the Bayes decision boundary for a given problem is
linear, we will probably achieve a superior test error rate using QDA
rather than LDA because QDA is flexible enough to model a linear decision
boundary. Justify your answer.</li>
</ol>
</blockquote>
<p>False. QDA can overfit leading to poorer test performance.</p>
</div>
<div id="question-6-2" class="section level3 hasAnchor" number="4.1.6">
<h3><span class="header-section-number">4.1.6</span> Question 6<a href="classification.html#question-6-2" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<blockquote>
<p>Suppose we collect data for a group of students in a statistics class with
variables <span class="math inline">\(X_1 =\)</span> hours studied, <span class="math inline">\(X_2 =\)</span> undergrad GPA, and <span class="math inline">\(Y =\)</span> receive an A.
We fit a logistic regression and produce estimated coefficient,
<span class="math inline">\(\hat\beta_0 = -6\)</span>, <span class="math inline">\(\hat\beta_1 = 0.05\)</span>, <span class="math inline">\(\hat\beta_2 = 1\)</span>.</p>
<ol style="list-style-type: lower-alpha">
<li>Estimate the probability that a student who studies for 40h and has an
undergrad GPA of 3.5 gets an A in the class.</li>
</ol>
</blockquote>
<p>The logistic model is:</p>
<p><span class="math display">\[
\log\left(\frac{p(X)}{1-p(x)}\right) = -6 + 0.05X_1 + X_2
\]</span></p>
<p>or</p>
<p><span class="math display">\[
p(X) = \frac{e^{-6 + 0.05X_1 + X_2}}{1 + e^{-6 + 0.05X_1 + X_2}}
\]</span></p>
<p>when <span class="math inline">\(X_1 = 40\)</span> and <span class="math inline">\(X_2 = 3.5\)</span>, <span class="math inline">\(p(X) = 0.38\)</span></p>
<blockquote>
<ol start="2" style="list-style-type: lower-alpha">
<li>How many hours would the student in part (a) need to study to have a 50%
chance of getting an A in the class?</li>
</ol>
</blockquote>
<p>We would like to solve for <span class="math inline">\(X_1\)</span> where <span class="math inline">\(p(X) = 0.5\)</span>. Taking the first equation
above, we need to solve <span class="math inline">\(0 = −6 + 0.05X_1 + 3.5\)</span>, so <span class="math inline">\(X_1 = 50\)</span> hours.</p>
</div>
<div id="question-7-2" class="section level3 hasAnchor" number="4.1.7">
<h3><span class="header-section-number">4.1.7</span> Question 7<a href="classification.html#question-7-2" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<blockquote>
<p>Suppose that we wish to predict whether a given stock will issue a dividend
this year (“Yes” or “No”) based on <span class="math inline">\(X\)</span>, last year’s percent profit. We
examine a large number of companies and discover that the mean value of <span class="math inline">\(X\)</span>
for companies that issued a dividend was <span class="math inline">\(\bar{X} = 10\)</span>, while the mean for
those that didn’t was <span class="math inline">\(\bar{X} = 0\)</span>. In addition, the variance of <span class="math inline">\(X\)</span> for
these two sets of companies was <span class="math inline">\(\hat{\sigma}^2 = 36\)</span>. Finally, 80% of
companies issued dividends. Assuming that <span class="math inline">\(X\)</span> follows a normal distribution,
predict the probability that a company will issue a dividend this year given
that its percentage profit was <span class="math inline">\(X = 4\)</span> last year.</p>
<p><em>Hint: Recall that the density function for a normal random variable is</em>
<em><span class="math inline">\(f(x) =\frac{1}{\sqrt{2\pi\sigma^2}}e^{-(x-\mu)^2/2\sigma^2}\)</span>.</em>
<em>You will need to use Bayes’ theorem.</em></p>
</blockquote>
<p>Value <span class="math inline">\(v\)</span> for companies (D) issuing a dividend = <span class="math inline">\(v_D \sim \mathcal{N}(10, 36)\)</span>.
Value <span class="math inline">\(v\)</span> for companies (N) not issuing a dividend = <span class="math inline">\(v_N \sim \mathcal{N}(0, 36)\)</span>
and <span class="math inline">\(p(D) = 0.8\)</span>.</p>
<p>We want to find <span class="math inline">\(p(D|v)\)</span> and we can calculate <span class="math inline">\(p(v|D)\)</span> from the Gaussian
density function. Note that since <span class="math inline">\(e^2\)</span> is equal between both classes, the
term <span class="math inline">\(\frac{1}{\sqrt{2\pi\sigma^2}}\)</span> cancels.</p>
<p><span class="math display">\[\begin{align}
p(D|v) &amp;= \frac{p(v|D) p(D)}{p(v|D)p(D) + p(v|N)p(N)} \\
       &amp;= \frac{\pi_D \frac{1}{\sqrt{2\pi\sigma^2}} e^{-(x-\mu_D)^2/2\sigma^2}}
               {\pi_D \frac{1}{\sqrt{2\pi\sigma^2}} e^{-(x-\mu_D)^2/2\sigma^2} +
                \pi_N \frac{1}{\sqrt{2\pi\sigma^2}} e^{-(x-\mu_N)^2/2\sigma^2}} \\
       &amp;= \frac{\pi_D e^{-(x-\mu_D)^2/2\sigma^2}}
               {\pi_D e^{-(x-\mu_D)^2/2\sigma^2} +
                \pi_N e^{-(x-\mu_N)^2/2\sigma^2}} \\
       &amp;= \frac{0.8 \times e^{-(4-10)^2/(2 \times 36)}}
               {0.8 \times e^{-(4-10)^2/(2 \times 36)} + 0.2 \times e^{-(4-0)^2/(2 \times 36)}} \\
       &amp;= \frac{0.8 e^{-1/2}}{0.8 e^{-1/2} + 0.2 e^{-2/9}}
\end{align}\]</span></p>
<div class="sourceCode" id="cb173"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb173-1"><a href="classification.html#cb173-1" aria-hidden="true"></a><span class="kw">exp</span>(<span class="op">-</span><span class="fl">0.5</span>) <span class="op">*</span><span class="st"> </span><span class="fl">0.8</span> <span class="op">/</span><span class="st"> </span>(<span class="kw">exp</span>(<span class="op">-</span><span class="fl">0.5</span>) <span class="op">*</span><span class="st"> </span><span class="fl">0.8</span> <span class="op">+</span><span class="st"> </span><span class="kw">exp</span>(<span class="op">-</span><span class="dv">2</span><span class="op">/</span><span class="dv">9</span>) <span class="op">*</span><span class="st"> </span><span class="fl">0.2</span>)</span></code></pre></div>
<pre><code>## [1] 0.7518525</code></pre>
</div>
<div id="question-8-2" class="section level3 hasAnchor" number="4.1.8">
<h3><span class="header-section-number">4.1.8</span> Question 8<a href="classification.html#question-8-2" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<blockquote>
<p>Suppose that we take a data set, divide it into equally-sized training and
test sets, and then try out two different classification procedures. First we
use logistic regression and get an error rate of 20% on the training data and
30% on the test data. Next we use 1-nearest neighbors (i.e. <span class="math inline">\(K = 1\)</span>) and get
an average error rate (averaged over both test and training data sets) of
18%. Based on these results, which method should we prefer to use for
classification of new observations? Why?</p>
</blockquote>
<p>For <span class="math inline">\(K = 1\)</span>, performance on the training set is perfect and the error rate
is zero, implying a test error rate of 36%. Logistic regression outperforms
1-nearest neighbor on the test set and therefore should be preferred.</p>
</div>
<div id="question-9-2" class="section level3 hasAnchor" number="4.1.9">
<h3><span class="header-section-number">4.1.9</span> Question 9<a href="classification.html#question-9-2" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<blockquote>
<p>This problem has to do with <em>odds</em>.</p>
<ol style="list-style-type: lower-alpha">
<li>On average, what fraction of people with an odds of 0.37 of defaulting on
their credit card payment will in fact default?</li>
</ol>
</blockquote>
<p>Odds is defined as <span class="math inline">\(p/(1-p)\)</span>.</p>
<p><span class="math display">\[0.37 = \frac{p(x)}{1 - p(x)}\]</span></p>
<p>therefore,</p>
<p><span class="math display">\[p(x) = \frac{0.37}{1 + 0.37} = 0.27\]</span></p>
<blockquote>
<ol start="2" style="list-style-type: lower-alpha">
<li>Suppose that an individual has a 16% chance of defaulting on her credit
card payment. What are the odds that she will default?</li>
</ol>
</blockquote>
<p><span class="math display">\[0.16 / (1 - 0.16)  = 0.19\]</span></p>
</div>
<div id="question-10-2" class="section level3 hasAnchor" number="4.1.10">
<h3><span class="header-section-number">4.1.10</span> Question 10<a href="classification.html#question-10-2" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<blockquote>
<p>Equation 4.32 derived an expression for <span class="math inline">\(\log(\frac{Pr(Y=k|X=x)}{Pr(Y=K|X=x)})\)</span>
in the setting where <span class="math inline">\(p &gt; 1\)</span>, so that the mean for the <span class="math inline">\(k\)</span>th class, <span class="math inline">\(\mu_k\)</span>,
is a <span class="math inline">\(p\)</span>-dimensional vector, and the shared covariance <span class="math inline">\(\Sigma\)</span> is a
<span class="math inline">\(p \times p\)</span> matrix. However, in the setting with <span class="math inline">\(p = 1\)</span>, (4.32) takes a
simpler form, since the means <span class="math inline">\(\mu_1, ..., \mu_k\)</span> and the variance <span class="math inline">\(\sigma^2\)</span>
are scalars. In this simpler setting, repeat the calculation in (4.32), and
provide expressions for <span class="math inline">\(a_k\)</span> and <span class="math inline">\(b_{kj}\)</span> in terms of
<span class="math inline">\(\pi_k, \pi_K, \mu_k, \mu_K,\)</span> and <span class="math inline">\(\sigma^2\)</span>.</p>
</blockquote>
<p><span class="math display">\[\begin{align*}
\log\left(\frac{Pr(Y=k|X=x)}{Pr(Y=K|X=x)}\right)
 &amp; = \log\left(\frac{\pi_k f_k(x)}{\pi_K f_K(x)}\right) \\
 &amp; = \log\left(\frac{\pi_k \exp(-1/2((x-\mu_k)/\sigma)^2)}{\pi_K \exp(-1/2((x-\mu_K)/\sigma)^2)}\right) \\
 &amp; = \log\left(\frac{\pi_k}{\pi_K}\right) - \frac{1}{2} \left(\frac{x-\mu_k}{\sigma}\right)^2 + \frac{1}{2} \left(\frac{x-\mu_K}{\sigma}\right)^2 \\
 &amp; = \log\left(\frac{\pi_k}{\pi_K}\right) - \frac{1}{2\sigma^2} (x-\mu_k)^2 + \frac{1}{2\sigma^2} (x-\mu_K)^2 \\
 &amp; = \log\left(\frac{\pi_k}{\pi_K}\right) - \frac{1}{2\sigma^2} \left((x-\mu_k)^2 - (x-\mu_K)^2\right) \\
 &amp; = \log\left(\frac{\pi_k}{\pi_K}\right) - \frac{1}{2\sigma^2} \left(x^2-2x\mu_k+\mu_k^2 - x^2 + 2x\mu_K - \mu_K^2\right) \\
 &amp; = \log\left(\frac{\pi_k}{\pi_K}\right) - \frac{1}{2\sigma^2} \left(2x(\mu_K - \mu_k) + \mu_k^2 -\mu_K^2\right) \\
 &amp; = \log\left(\frac{\pi_k}{\pi_K}\right) - \frac{\mu_k^2 -\mu_K^2}{2\sigma^2} + \frac{x(\mu_k - \mu_K)}{\sigma^2}
\end{align*}\]</span></p>
<p>Therefore,</p>
<p><span class="math display">\[a_k = \log\left(\frac{\pi_k}{\pi_K}\right) - \frac{\mu_k^2 -\mu_K^2}{2\sigma^2}\]</span></p>
<p>and</p>
<p><span class="math display">\[b_k = (\mu_k - \mu_K) / \sigma^2\]</span></p>
</div>
<div id="question-11-1" class="section level3 hasAnchor" number="4.1.11">
<h3><span class="header-section-number">4.1.11</span> Question 11<a href="classification.html#question-11-1" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>ToDo</p>
<blockquote>
<p>Work out the detailed forms of <span class="math inline">\(a_k\)</span>, <span class="math inline">\(b_{kj}\)</span>, and <span class="math inline">\(b_{kjl}\)</span> in (4.33).
Your answer should involve <span class="math inline">\(\pi_k\)</span>, <span class="math inline">\(\pi_K\)</span>, <span class="math inline">\(\mu_k\)</span>, <span class="math inline">\(\mu_K\)</span>, <span class="math inline">\(\Sigma_k\)</span>,
and <span class="math inline">\(\Sigma_K\)</span>.</p>
</blockquote>
</div>
<div id="question-12-1" class="section level3 hasAnchor" number="4.1.12">
<h3><span class="header-section-number">4.1.12</span> Question 12<a href="classification.html#question-12-1" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<blockquote>
<p>Suppose that you wish to classify an observation <span class="math inline">\(X \in \mathbb{R}\)</span> into
<code>apples</code> and <code>oranges</code>. You fit a logistic regression model and find that</p>
<p><span class="math display">\[
\hat{Pr}(Y=orange|X=x) = 
\frac{\exp(\hat\beta_0 + \hat\beta_1x)}{1 + \exp(\hat\beta_0 + \hat\beta_1x)}
\]</span></p>
<p>Your friend fits a logistic regression model to the same data using the
<em>softmax</em> formulation in (4.13), and finds that</p>
<p><span class="math display">\[
\hat{Pr}(Y=orange|X=x) = 
\frac{\exp(\hat\alpha_{orange0} + \hat\alpha_{orange1}x)}
{\exp(\hat\alpha_{orange0} + \hat\alpha_{orange1}x) + \exp(\hat\alpha_{apple0} + \hat\alpha_{apple1}x)}
\]</span></p>
<ol style="list-style-type: lower-alpha">
<li>What is the log odds of <code>orange</code> versus <code>apple</code> in your model?</li>
</ol>
</blockquote>
<p>The log odds is just <span class="math inline">\(\hat\beta_0 + \hat\beta_1x\)</span></p>
<blockquote>
<ol start="2" style="list-style-type: lower-alpha">
<li>What is the log odds of <code>orange</code> versus <code>apple</code> in your friend’s model?</li>
</ol>
</blockquote>
<p>From 4.14, log odds of our friend’s model is:</p>
<p><span class="math display">\[
(\hat\alpha_{orange0} - \hat\alpha_{apple0}) + (\hat\alpha_{orange1} - \hat\alpha_{apple1})x
\]</span></p>
<blockquote>
<ol start="3" style="list-style-type: lower-alpha">
<li>Suppose that in your model, <span class="math inline">\(\hat\beta_0 = 2\)</span> and <span class="math inline">\(\hat\beta = −1\)</span>. What
are the coefficient estimates in your friend’s model? Be as specific as
possible.</li>
</ol>
</blockquote>
<p>We can say that in our friend’s model <span class="math inline">\(\hat\alpha_{orange0} - \hat\alpha_{apple0} = 2\)</span> and <span class="math inline">\(\hat\alpha_{orange1} - \hat\alpha_{apple1} = -1\)</span>.</p>
<p>We are unable to know the specific value of each parameter however.</p>
<blockquote>
<ol start="4" style="list-style-type: lower-alpha">
<li>Now suppose that you and your friend fit the same two models on a different
data set. This time, your friend gets the coefficient estimates
<span class="math inline">\(\hat\alpha_{orange0} = 1.2\)</span>, <span class="math inline">\(\hat\alpha_{orange1} = −2\)</span>,
<span class="math inline">\(\hat\alpha_{apple0} = 3\)</span>, <span class="math inline">\(\hat\alpha_{apple1} = 0.6\)</span>. What are the
coefficient estimates in your model?</li>
</ol>
</blockquote>
<p>The coefficients in our model would be <span class="math inline">\(\hat\beta_0 = 1.2 - 3 = -1.8\)</span> and
<span class="math inline">\(\hat\beta_1 = -2 - 0.6 = -2.6\)</span></p>
<blockquote>
<ol start="5" style="list-style-type: lower-alpha">
<li>Finally, suppose you apply both models from (d) to a data set with 2,000
test observations. What fraction of the time do you expect the predicted
class labels from your model to agree with those from your friend’s model?
Explain your answer.</li>
</ol>
</blockquote>
<p>The models are identical with different parameterization so they should
perfectly agree.</p>
</div>
</div>
<div id="applied-2" class="section level2 hasAnchor" number="4.2">
<h2><span class="header-section-number">4.2</span> Applied<a href="classification.html#applied-2" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<div id="question-13-1" class="section level3 hasAnchor" number="4.2.1">
<h3><span class="header-section-number">4.2.1</span> Question 13<a href="classification.html#question-13-1" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<blockquote>
<p>This question should be answered using the <code>Weekly</code> data set, which is part
of the <code>ISLR2</code> package. This data is similar in nature to the <code>Smarket</code> data
from this chapter’s lab, except that it contains 1,089 weekly returns for 21
years, from the beginning of 1990 to the end of 2010.</p>
<ol style="list-style-type: lower-alpha">
<li>Produce some numerical and graphical summaries of the <code>Weekly</code> data. Do
there appear to be any patterns?</li>
</ol>
</blockquote>
<div class="sourceCode" id="cb175"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb175-1"><a href="classification.html#cb175-1" aria-hidden="true"></a><span class="kw">library</span>(MASS)</span>
<span id="cb175-2"><a href="classification.html#cb175-2" aria-hidden="true"></a><span class="kw">library</span>(class)</span>
<span id="cb175-3"><a href="classification.html#cb175-3" aria-hidden="true"></a><span class="kw">library</span>(tidyverse)</span>
<span id="cb175-4"><a href="classification.html#cb175-4" aria-hidden="true"></a><span class="kw">library</span>(corrplot)</span>
<span id="cb175-5"><a href="classification.html#cb175-5" aria-hidden="true"></a><span class="kw">library</span>(ISLR2)</span>
<span id="cb175-6"><a href="classification.html#cb175-6" aria-hidden="true"></a><span class="kw">library</span>(e1071)</span></code></pre></div>
<div class="sourceCode" id="cb176"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb176-1"><a href="classification.html#cb176-1" aria-hidden="true"></a><span class="kw">summary</span>(Weekly)</span></code></pre></div>
<pre><code>##       Year           Lag1               Lag2               Lag3         
##  Min.   :1990   Min.   :-18.1950   Min.   :-18.1950   Min.   :-18.1950  
##  1st Qu.:1995   1st Qu.: -1.1540   1st Qu.: -1.1540   1st Qu.: -1.1580  
##  Median :2000   Median :  0.2410   Median :  0.2410   Median :  0.2410  
##  Mean   :2000   Mean   :  0.1506   Mean   :  0.1511   Mean   :  0.1472  
##  3rd Qu.:2005   3rd Qu.:  1.4050   3rd Qu.:  1.4090   3rd Qu.:  1.4090  
##  Max.   :2010   Max.   : 12.0260   Max.   : 12.0260   Max.   : 12.0260  
##       Lag4               Lag5              Volume            Today         
##  Min.   :-18.1950   Min.   :-18.1950   Min.   :0.08747   Min.   :-18.1950  
##  1st Qu.: -1.1580   1st Qu.: -1.1660   1st Qu.:0.33202   1st Qu.: -1.1540  
##  Median :  0.2380   Median :  0.2340   Median :1.00268   Median :  0.2410  
##  Mean   :  0.1458   Mean   :  0.1399   Mean   :1.57462   Mean   :  0.1499  
##  3rd Qu.:  1.4090   3rd Qu.:  1.4050   3rd Qu.:2.05373   3rd Qu.:  1.4050  
##  Max.   : 12.0260   Max.   : 12.0260   Max.   :9.32821   Max.   : 12.0260  
##  Direction 
##  Down:484  
##  Up  :605  
##            
##            
##            
## </code></pre>
<div class="sourceCode" id="cb178"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb178-1"><a href="classification.html#cb178-1" aria-hidden="true"></a><span class="kw">corrplot</span>(<span class="kw">cor</span>(Weekly[, <span class="dv">-9</span>]), <span class="dt">type =</span> <span class="st">&quot;lower&quot;</span>, <span class="dt">diag =</span> <span class="ot">FALSE</span>, <span class="dt">method =</span> <span class="st">&quot;ellipse&quot;</span>)</span></code></pre></div>
<p><img src="04-classification_files/figure-html/unnamed-chunk-3-1.png" width="672" /></p>
<p>Volume is strongly positively correlated with Year. Other correlations are
week, but Lag1 is negatively correlated with Lag2 but positively correlated
with Lag3.</p>
<blockquote>
<ol start="2" style="list-style-type: lower-alpha">
<li>Use the full data set to perform a logistic regression with <code>Direction</code> as
the response and the five lag variables plus <code>Volume</code> as predictors. Use
the summary function to print the results. Do any of the predictors appear
to be statistically significant? If so, which ones?</li>
</ol>
</blockquote>
<div class="sourceCode" id="cb179"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb179-1"><a href="classification.html#cb179-1" aria-hidden="true"></a>fit &lt;-<span class="st"> </span><span class="kw">glm</span>(</span>
<span id="cb179-2"><a href="classification.html#cb179-2" aria-hidden="true"></a>  Direction <span class="op">~</span><span class="st"> </span>Lag1 <span class="op">+</span><span class="st"> </span>Lag2 <span class="op">+</span><span class="st"> </span>Lag3 <span class="op">+</span><span class="st"> </span>Lag4 <span class="op">+</span><span class="st"> </span>Lag5 <span class="op">+</span><span class="st"> </span>Volume,</span>
<span id="cb179-3"><a href="classification.html#cb179-3" aria-hidden="true"></a>  <span class="dt">data =</span> Weekly,</span>
<span id="cb179-4"><a href="classification.html#cb179-4" aria-hidden="true"></a>  <span class="dt">family =</span> binomial</span>
<span id="cb179-5"><a href="classification.html#cb179-5" aria-hidden="true"></a>)</span>
<span id="cb179-6"><a href="classification.html#cb179-6" aria-hidden="true"></a><span class="kw">summary</span>(fit)</span></code></pre></div>
<pre><code>## 
## Call:
## glm(formula = Direction ~ Lag1 + Lag2 + Lag3 + Lag4 + Lag5 + 
##     Volume, family = binomial, data = Weekly)
## 
## Coefficients:
##             Estimate Std. Error z value Pr(&gt;|z|)   
## (Intercept)  0.26686    0.08593   3.106   0.0019 **
## Lag1        -0.04127    0.02641  -1.563   0.1181   
## Lag2         0.05844    0.02686   2.175   0.0296 * 
## Lag3        -0.01606    0.02666  -0.602   0.5469   
## Lag4        -0.02779    0.02646  -1.050   0.2937   
## Lag5        -0.01447    0.02638  -0.549   0.5833   
## Volume      -0.02274    0.03690  -0.616   0.5377   
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## (Dispersion parameter for binomial family taken to be 1)
## 
##     Null deviance: 1496.2  on 1088  degrees of freedom
## Residual deviance: 1486.4  on 1082  degrees of freedom
## AIC: 1500.4
## 
## Number of Fisher Scoring iterations: 4</code></pre>
<p>Lag2 is significant.</p>
<blockquote>
<ol start="3" style="list-style-type: lower-alpha">
<li>Compute the confusion matrix and overall fraction of correct predictions.
Explain what the confusion matrix is telling you about the types of
mistakes made by logistic regression.</li>
</ol>
</blockquote>
<div class="sourceCode" id="cb181"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb181-1"><a href="classification.html#cb181-1" aria-hidden="true"></a><span class="kw">contrasts</span>(Weekly<span class="op">$</span>Direction)</span></code></pre></div>
<pre><code>##      Up
## Down  0
## Up    1</code></pre>
<div class="sourceCode" id="cb183"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb183-1"><a href="classification.html#cb183-1" aria-hidden="true"></a>pred &lt;-<span class="st"> </span><span class="kw">predict</span>(fit, <span class="dt">type =</span> <span class="st">&quot;response&quot;</span>) <span class="op">&gt;</span><span class="st"> </span><span class="fl">0.5</span></span>
<span id="cb183-2"><a href="classification.html#cb183-2" aria-hidden="true"></a>(t &lt;-<span class="st"> </span><span class="kw">table</span>(<span class="kw">ifelse</span>(pred, <span class="st">&quot;Up (pred)&quot;</span>, <span class="st">&quot;Down (pred)&quot;</span>), Weekly<span class="op">$</span>Direction))</span></code></pre></div>
<pre><code>##              
##               Down  Up
##   Down (pred)   54  48
##   Up (pred)    430 557</code></pre>
<div class="sourceCode" id="cb185"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb185-1"><a href="classification.html#cb185-1" aria-hidden="true"></a><span class="kw">sum</span>(<span class="kw">diag</span>(t)) <span class="op">/</span><span class="st"> </span><span class="kw">sum</span>(t)</span></code></pre></div>
<pre><code>## [1] 0.5610652</code></pre>
<p>The overall fraction of correct predictions is 0.56. Although logistic
regression correctly predicts upwards movements well, it incorrectly predicts
most downwards movements as up.</p>
<blockquote>
<ol start="4" style="list-style-type: lower-alpha">
<li>Now fit the logistic regression model using a training data period from
1990 to 2008, with <code>Lag2</code> as the only predictor. Compute the confusion
matrix and the overall fraction of correct predictions for the held out
data (that is, the data from 2009 and 2010).</li>
</ol>
</blockquote>
<div class="sourceCode" id="cb187"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb187-1"><a href="classification.html#cb187-1" aria-hidden="true"></a>train &lt;-<span class="st"> </span>Weekly<span class="op">$</span>Year <span class="op">&lt;</span><span class="st"> </span><span class="dv">2009</span></span>
<span id="cb187-2"><a href="classification.html#cb187-2" aria-hidden="true"></a></span>
<span id="cb187-3"><a href="classification.html#cb187-3" aria-hidden="true"></a>fit &lt;-<span class="st"> </span><span class="kw">glm</span>(Direction <span class="op">~</span><span class="st"> </span>Lag2, <span class="dt">data =</span> Weekly[train, ], <span class="dt">family =</span> binomial)</span>
<span id="cb187-4"><a href="classification.html#cb187-4" aria-hidden="true"></a>pred &lt;-<span class="st"> </span><span class="kw">predict</span>(fit, Weekly[<span class="op">!</span>train, ], <span class="dt">type =</span> <span class="st">&quot;response&quot;</span>) <span class="op">&gt;</span><span class="st"> </span><span class="fl">0.5</span></span>
<span id="cb187-5"><a href="classification.html#cb187-5" aria-hidden="true"></a>(t &lt;-<span class="st"> </span><span class="kw">table</span>(<span class="kw">ifelse</span>(pred, <span class="st">&quot;Up (pred)&quot;</span>, <span class="st">&quot;Down (pred)&quot;</span>), Weekly[<span class="op">!</span>train, ]<span class="op">$</span>Direction))</span></code></pre></div>
<pre><code>##              
##               Down Up
##   Down (pred)    9  5
##   Up (pred)     34 56</code></pre>
<div class="sourceCode" id="cb189"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb189-1"><a href="classification.html#cb189-1" aria-hidden="true"></a><span class="kw">sum</span>(<span class="kw">diag</span>(t)) <span class="op">/</span><span class="st"> </span><span class="kw">sum</span>(t)</span></code></pre></div>
<pre><code>## [1] 0.625</code></pre>
<blockquote>
<ol start="5" style="list-style-type: lower-alpha">
<li>Repeat (d) using LDA.</li>
</ol>
</blockquote>
<div class="sourceCode" id="cb191"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb191-1"><a href="classification.html#cb191-1" aria-hidden="true"></a>fit &lt;-<span class="st"> </span><span class="kw">lda</span>(Direction <span class="op">~</span><span class="st"> </span>Lag2, <span class="dt">data =</span> Weekly[train, ])</span>
<span id="cb191-2"><a href="classification.html#cb191-2" aria-hidden="true"></a>pred &lt;-<span class="st"> </span><span class="kw">predict</span>(fit, Weekly[<span class="op">!</span>train, ], <span class="dt">type =</span> <span class="st">&quot;response&quot;</span>)<span class="op">$</span>class</span>
<span id="cb191-3"><a href="classification.html#cb191-3" aria-hidden="true"></a>(t &lt;-<span class="st"> </span><span class="kw">table</span>(pred, Weekly[<span class="op">!</span>train, ]<span class="op">$</span>Direction))</span></code></pre></div>
<pre><code>##       
## pred   Down Up
##   Down    9  5
##   Up     34 56</code></pre>
<div class="sourceCode" id="cb193"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb193-1"><a href="classification.html#cb193-1" aria-hidden="true"></a><span class="kw">sum</span>(<span class="kw">diag</span>(t)) <span class="op">/</span><span class="st"> </span><span class="kw">sum</span>(t)</span></code></pre></div>
<pre><code>## [1] 0.625</code></pre>
<blockquote>
<ol start="6" style="list-style-type: lower-alpha">
<li>Repeat (d) using QDA.</li>
</ol>
</blockquote>
<div class="sourceCode" id="cb195"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb195-1"><a href="classification.html#cb195-1" aria-hidden="true"></a>fit &lt;-<span class="st"> </span><span class="kw">qda</span>(Direction <span class="op">~</span><span class="st"> </span>Lag2, <span class="dt">data =</span> Weekly[train, ])</span>
<span id="cb195-2"><a href="classification.html#cb195-2" aria-hidden="true"></a>pred &lt;-<span class="st"> </span><span class="kw">predict</span>(fit, Weekly[<span class="op">!</span>train, ], <span class="dt">type =</span> <span class="st">&quot;response&quot;</span>)<span class="op">$</span>class</span>
<span id="cb195-3"><a href="classification.html#cb195-3" aria-hidden="true"></a>(t &lt;-<span class="st"> </span><span class="kw">table</span>(pred, Weekly[<span class="op">!</span>train, ]<span class="op">$</span>Direction))</span></code></pre></div>
<pre><code>##       
## pred   Down Up
##   Down    0  0
##   Up     43 61</code></pre>
<div class="sourceCode" id="cb197"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb197-1"><a href="classification.html#cb197-1" aria-hidden="true"></a><span class="kw">sum</span>(<span class="kw">diag</span>(t)) <span class="op">/</span><span class="st"> </span><span class="kw">sum</span>(t)</span></code></pre></div>
<pre><code>## [1] 0.5865385</code></pre>
<blockquote>
<ol start="7" style="list-style-type: lower-alpha">
<li>Repeat (d) using KNN with <span class="math inline">\(K = 1\)</span>.</li>
</ol>
</blockquote>
<div class="sourceCode" id="cb199"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb199-1"><a href="classification.html#cb199-1" aria-hidden="true"></a>fit &lt;-<span class="st"> </span><span class="kw">knn</span>(</span>
<span id="cb199-2"><a href="classification.html#cb199-2" aria-hidden="true"></a>  Weekly[train, <span class="st">&quot;Lag2&quot;</span>, <span class="dt">drop =</span> <span class="ot">FALSE</span>],</span>
<span id="cb199-3"><a href="classification.html#cb199-3" aria-hidden="true"></a>  Weekly[<span class="op">!</span>train, <span class="st">&quot;Lag2&quot;</span>, <span class="dt">drop =</span> <span class="ot">FALSE</span>],</span>
<span id="cb199-4"><a href="classification.html#cb199-4" aria-hidden="true"></a>  Weekly<span class="op">$</span>Direction[train]</span>
<span id="cb199-5"><a href="classification.html#cb199-5" aria-hidden="true"></a>)</span>
<span id="cb199-6"><a href="classification.html#cb199-6" aria-hidden="true"></a>(t &lt;-<span class="st"> </span><span class="kw">table</span>(fit, Weekly[<span class="op">!</span>train, ]<span class="op">$</span>Direction))</span></code></pre></div>
<pre><code>##       
## fit    Down Up
##   Down   21 29
##   Up     22 32</code></pre>
<div class="sourceCode" id="cb201"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb201-1"><a href="classification.html#cb201-1" aria-hidden="true"></a><span class="kw">sum</span>(<span class="kw">diag</span>(t)) <span class="op">/</span><span class="st"> </span><span class="kw">sum</span>(t)</span></code></pre></div>
<pre><code>## [1] 0.5096154</code></pre>
<blockquote>
<ol start="8" style="list-style-type: lower-alpha">
<li>Repeat (d) using naive Bayes.</li>
</ol>
</blockquote>
<div class="sourceCode" id="cb203"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb203-1"><a href="classification.html#cb203-1" aria-hidden="true"></a>fit &lt;-<span class="st"> </span><span class="kw">naiveBayes</span>(Direction <span class="op">~</span><span class="st"> </span>Lag2, <span class="dt">data =</span> Smarket, <span class="dt">subset =</span> train)</span>
<span id="cb203-2"><a href="classification.html#cb203-2" aria-hidden="true"></a>pred &lt;-<span class="st"> </span><span class="kw">predict</span>(fit, Weekly[<span class="op">!</span>train, ], <span class="dt">type =</span> <span class="st">&quot;class&quot;</span>)</span>
<span id="cb203-3"><a href="classification.html#cb203-3" aria-hidden="true"></a>(t &lt;-<span class="st"> </span><span class="kw">table</span>(pred, Weekly[<span class="op">!</span>train, ]<span class="op">$</span>Direction))</span></code></pre></div>
<pre><code>##       
## pred   Down Up
##   Down   27 29
##   Up     16 32</code></pre>
<div class="sourceCode" id="cb205"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb205-1"><a href="classification.html#cb205-1" aria-hidden="true"></a><span class="kw">sum</span>(<span class="kw">diag</span>(t)) <span class="op">/</span><span class="st"> </span><span class="kw">sum</span>(t)</span></code></pre></div>
<pre><code>## [1] 0.5673077</code></pre>
<blockquote>
<ol style="list-style-type: lower-roman">
<li>Which of these methods appears to provide the best results on this data?</li>
</ol>
</blockquote>
<p>Logistic regression and LDA are the best performing.</p>
<blockquote>
<ol start="10" style="list-style-type: lower-alpha">
<li>Experiment with different combinations of predictors, including possible
transformations and interactions, for each of the methods. Report the
variables, method, and associated confusion matrix that appears to provide
the best results on the held out data. Note that you should also
experiment with values for <span class="math inline">\(K\)</span> in the KNN classifier.</li>
</ol>
</blockquote>
<div class="sourceCode" id="cb207"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb207-1"><a href="classification.html#cb207-1" aria-hidden="true"></a>fit &lt;-<span class="st"> </span><span class="kw">glm</span>(Direction <span class="op">~</span><span class="st"> </span>Lag1, <span class="dt">data =</span> Weekly[train, ], <span class="dt">family =</span> binomial)</span>
<span id="cb207-2"><a href="classification.html#cb207-2" aria-hidden="true"></a>pred &lt;-<span class="st"> </span><span class="kw">predict</span>(fit, Weekly[<span class="op">!</span>train, ], <span class="dt">type =</span> <span class="st">&quot;response&quot;</span>) <span class="op">&gt;</span><span class="st"> </span><span class="fl">0.5</span></span>
<span id="cb207-3"><a href="classification.html#cb207-3" aria-hidden="true"></a><span class="kw">mean</span>(<span class="kw">ifelse</span>(pred, <span class="st">&quot;Up&quot;</span>, <span class="st">&quot;Down&quot;</span>) <span class="op">==</span><span class="st"> </span>Weekly[<span class="op">!</span>train, ]<span class="op">$</span>Direction)</span></code></pre></div>
<pre><code>## [1] 0.5673077</code></pre>
<div class="sourceCode" id="cb209"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb209-1"><a href="classification.html#cb209-1" aria-hidden="true"></a>fit &lt;-<span class="st"> </span><span class="kw">glm</span>(Direction <span class="op">~</span><span class="st"> </span>Lag3, <span class="dt">data =</span> Weekly[train, ], <span class="dt">family =</span> binomial)</span>
<span id="cb209-2"><a href="classification.html#cb209-2" aria-hidden="true"></a>pred &lt;-<span class="st"> </span><span class="kw">predict</span>(fit, Weekly[<span class="op">!</span>train, ], <span class="dt">type =</span> <span class="st">&quot;response&quot;</span>) <span class="op">&gt;</span><span class="st"> </span><span class="fl">0.5</span></span>
<span id="cb209-3"><a href="classification.html#cb209-3" aria-hidden="true"></a><span class="kw">mean</span>(<span class="kw">ifelse</span>(pred, <span class="st">&quot;Up&quot;</span>, <span class="st">&quot;Down&quot;</span>) <span class="op">==</span><span class="st"> </span>Weekly[<span class="op">!</span>train, ]<span class="op">$</span>Direction)</span></code></pre></div>
<pre><code>## [1] 0.5865385</code></pre>
<div class="sourceCode" id="cb211"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb211-1"><a href="classification.html#cb211-1" aria-hidden="true"></a>fit &lt;-<span class="st"> </span><span class="kw">glm</span>(Direction <span class="op">~</span>Lag4, <span class="dt">data =</span> Weekly[train, ], <span class="dt">family =</span> binomial)</span>
<span id="cb211-2"><a href="classification.html#cb211-2" aria-hidden="true"></a>pred &lt;-<span class="st"> </span><span class="kw">predict</span>(fit, Weekly[<span class="op">!</span>train, ], <span class="dt">type =</span> <span class="st">&quot;response&quot;</span>) <span class="op">&gt;</span><span class="st"> </span><span class="fl">0.5</span></span>
<span id="cb211-3"><a href="classification.html#cb211-3" aria-hidden="true"></a><span class="kw">mean</span>(<span class="kw">ifelse</span>(pred, <span class="st">&quot;Up&quot;</span>, <span class="st">&quot;Down&quot;</span>) <span class="op">==</span><span class="st"> </span>Weekly[<span class="op">!</span>train, ]<span class="op">$</span>Direction)</span></code></pre></div>
<pre><code>## [1] 0.5865385</code></pre>
<div class="sourceCode" id="cb213"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb213-1"><a href="classification.html#cb213-1" aria-hidden="true"></a>fit &lt;-<span class="st"> </span><span class="kw">glm</span>(Direction <span class="op">~</span><span class="st"> </span>Lag1 <span class="op">+</span><span class="st"> </span>Lag2 <span class="op">+</span><span class="st"> </span>Lag3 <span class="op">+</span><span class="st"> </span>Lag4, <span class="dt">data =</span> Weekly[train, ], <span class="dt">family =</span> binomial)</span>
<span id="cb213-2"><a href="classification.html#cb213-2" aria-hidden="true"></a>pred &lt;-<span class="st"> </span><span class="kw">predict</span>(fit, Weekly[<span class="op">!</span>train, ], <span class="dt">type =</span> <span class="st">&quot;response&quot;</span>) <span class="op">&gt;</span><span class="st"> </span><span class="fl">0.5</span></span>
<span id="cb213-3"><a href="classification.html#cb213-3" aria-hidden="true"></a><span class="kw">mean</span>(<span class="kw">ifelse</span>(pred, <span class="st">&quot;Up&quot;</span>, <span class="st">&quot;Down&quot;</span>) <span class="op">==</span><span class="st"> </span>Weekly[<span class="op">!</span>train, ]<span class="op">$</span>Direction)</span></code></pre></div>
<pre><code>## [1] 0.5865385</code></pre>
<div class="sourceCode" id="cb215"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb215-1"><a href="classification.html#cb215-1" aria-hidden="true"></a>fit &lt;-<span class="st"> </span><span class="kw">glm</span>(Direction <span class="op">~</span><span class="st"> </span>Lag1 <span class="op">*</span><span class="st"> </span>Lag2 <span class="op">*</span><span class="st"> </span>Lag3 <span class="op">*</span><span class="st"> </span>Lag4, <span class="dt">data =</span> Weekly[train, ], <span class="dt">family =</span> binomial)</span>
<span id="cb215-2"><a href="classification.html#cb215-2" aria-hidden="true"></a>pred &lt;-<span class="st"> </span><span class="kw">predict</span>(fit, Weekly[<span class="op">!</span>train, ], <span class="dt">type =</span> <span class="st">&quot;response&quot;</span>) <span class="op">&gt;</span><span class="st"> </span><span class="fl">0.5</span></span>
<span id="cb215-3"><a href="classification.html#cb215-3" aria-hidden="true"></a><span class="kw">mean</span>(<span class="kw">ifelse</span>(pred, <span class="st">&quot;Up&quot;</span>, <span class="st">&quot;Down&quot;</span>) <span class="op">==</span><span class="st"> </span>Weekly[<span class="op">!</span>train, ]<span class="op">$</span>Direction)</span></code></pre></div>
<pre><code>## [1] 0.5961538</code></pre>
<div class="sourceCode" id="cb217"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb217-1"><a href="classification.html#cb217-1" aria-hidden="true"></a>fit &lt;-<span class="st"> </span><span class="kw">lda</span>(Direction <span class="op">~</span><span class="st"> </span>Lag1 <span class="op">+</span><span class="st"> </span>Lag2 <span class="op">+</span><span class="st"> </span>Lag3 <span class="op">+</span><span class="st"> </span>Lag4,<span class="dt">data =</span> Weekly[train, ])</span>
<span id="cb217-2"><a href="classification.html#cb217-2" aria-hidden="true"></a>pred &lt;-<span class="st"> </span><span class="kw">predict</span>(fit, Weekly[<span class="op">!</span>train, ], <span class="dt">type =</span> <span class="st">&quot;response&quot;</span>)<span class="op">$</span>class</span>
<span id="cb217-3"><a href="classification.html#cb217-3" aria-hidden="true"></a><span class="kw">mean</span>(pred <span class="op">==</span><span class="st"> </span>Weekly[<span class="op">!</span>train, ]<span class="op">$</span>Direction)</span></code></pre></div>
<pre><code>## [1] 0.5769231</code></pre>
<div class="sourceCode" id="cb219"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb219-1"><a href="classification.html#cb219-1" aria-hidden="true"></a>fit &lt;-<span class="st"> </span><span class="kw">qda</span>(Direction <span class="op">~</span><span class="st"> </span>Lag1 <span class="op">+</span><span class="st"> </span>Lag2 <span class="op">+</span><span class="st"> </span>Lag3 <span class="op">+</span><span class="st"> </span>Lag4, <span class="dt">data =</span> Weekly[train, ])</span>
<span id="cb219-2"><a href="classification.html#cb219-2" aria-hidden="true"></a>pred &lt;-<span class="st"> </span><span class="kw">predict</span>(fit, Weekly[<span class="op">!</span>train, ], <span class="dt">type =</span> <span class="st">&quot;response&quot;</span>)<span class="op">$</span>class</span>
<span id="cb219-3"><a href="classification.html#cb219-3" aria-hidden="true"></a><span class="kw">mean</span>(pred <span class="op">==</span><span class="st"> </span>Weekly[<span class="op">!</span>train, ]<span class="op">$</span>Direction)</span></code></pre></div>
<pre><code>## [1] 0.5192308</code></pre>
<div class="sourceCode" id="cb221"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb221-1"><a href="classification.html#cb221-1" aria-hidden="true"></a>fit &lt;-<span class="st"> </span><span class="kw">naiveBayes</span>(Direction <span class="op">~</span><span class="st"> </span>Lag1 <span class="op">+</span><span class="st"> </span>Lag2 <span class="op">+</span><span class="st"> </span>Lag3 <span class="op">+</span><span class="st"> </span>Lag4, <span class="dt">data =</span> Weekly[train, ])</span>
<span id="cb221-2"><a href="classification.html#cb221-2" aria-hidden="true"></a>pred &lt;-<span class="st"> </span><span class="kw">predict</span>(fit, Weekly[<span class="op">!</span>train, ], <span class="dt">type =</span> <span class="st">&quot;class&quot;</span>)</span>
<span id="cb221-3"><a href="classification.html#cb221-3" aria-hidden="true"></a><span class="kw">mean</span>(pred <span class="op">==</span><span class="st"> </span>Weekly[<span class="op">!</span>train, ]<span class="op">$</span>Direction)</span></code></pre></div>
<pre><code>## [1] 0.5096154</code></pre>
<div class="sourceCode" id="cb223"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb223-1"><a href="classification.html#cb223-1" aria-hidden="true"></a><span class="kw">set.seed</span>(<span class="dv">1</span>)</span>
<span id="cb223-2"><a href="classification.html#cb223-2" aria-hidden="true"></a>res &lt;-<span class="st"> </span><span class="kw">sapply</span>(<span class="dv">1</span><span class="op">:</span><span class="dv">30</span>, <span class="cf">function</span>(k) {</span>
<span id="cb223-3"><a href="classification.html#cb223-3" aria-hidden="true"></a>  fit &lt;-<span class="st"> </span><span class="kw">knn</span>(</span>
<span id="cb223-4"><a href="classification.html#cb223-4" aria-hidden="true"></a>    Weekly[train, <span class="dv">2</span><span class="op">:</span><span class="dv">4</span>, <span class="dt">drop =</span> <span class="ot">FALSE</span>],</span>
<span id="cb223-5"><a href="classification.html#cb223-5" aria-hidden="true"></a>    Weekly[<span class="op">!</span>train, <span class="dv">2</span><span class="op">:</span><span class="dv">4</span>, <span class="dt">drop =</span> <span class="ot">FALSE</span>],</span>
<span id="cb223-6"><a href="classification.html#cb223-6" aria-hidden="true"></a>    Weekly<span class="op">$</span>Direction[train],</span>
<span id="cb223-7"><a href="classification.html#cb223-7" aria-hidden="true"></a>    <span class="dt">k =</span> k</span>
<span id="cb223-8"><a href="classification.html#cb223-8" aria-hidden="true"></a>  )</span>
<span id="cb223-9"><a href="classification.html#cb223-9" aria-hidden="true"></a>  <span class="kw">mean</span>(fit <span class="op">==</span><span class="st"> </span>Weekly[<span class="op">!</span>train, ]<span class="op">$</span>Direction)</span>
<span id="cb223-10"><a href="classification.html#cb223-10" aria-hidden="true"></a>})</span>
<span id="cb223-11"><a href="classification.html#cb223-11" aria-hidden="true"></a><span class="kw">plot</span>(<span class="dv">1</span><span class="op">:</span><span class="dv">30</span>, res, <span class="dt">type =</span> <span class="st">&quot;o&quot;</span>, <span class="dt">xlab =</span> <span class="st">&quot;k&quot;</span>, <span class="dt">ylab =</span> <span class="st">&quot;Fraction correct&quot;</span>)</span></code></pre></div>
<p><img src="04-classification_files/figure-html/unnamed-chunk-11-1.png" width="672" /></p>
<div class="sourceCode" id="cb224"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb224-1"><a href="classification.html#cb224-1" aria-hidden="true"></a>(k &lt;-<span class="st"> </span><span class="kw">which.max</span>(res))</span></code></pre></div>
<pre><code>## [1] 26</code></pre>
<div class="sourceCode" id="cb226"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb226-1"><a href="classification.html#cb226-1" aria-hidden="true"></a>fit &lt;-<span class="st"> </span><span class="kw">knn</span>(</span>
<span id="cb226-2"><a href="classification.html#cb226-2" aria-hidden="true"></a>  Weekly[train, <span class="dv">2</span><span class="op">:</span><span class="dv">4</span>, <span class="dt">drop =</span> <span class="ot">FALSE</span>],</span>
<span id="cb226-3"><a href="classification.html#cb226-3" aria-hidden="true"></a>  Weekly[<span class="op">!</span>train, <span class="dv">2</span><span class="op">:</span><span class="dv">4</span>, <span class="dt">drop =</span> <span class="ot">FALSE</span>],</span>
<span id="cb226-4"><a href="classification.html#cb226-4" aria-hidden="true"></a>  Weekly<span class="op">$</span>Direction[train],</span>
<span id="cb226-5"><a href="classification.html#cb226-5" aria-hidden="true"></a>  <span class="dt">k =</span> k</span>
<span id="cb226-6"><a href="classification.html#cb226-6" aria-hidden="true"></a>)</span>
<span id="cb226-7"><a href="classification.html#cb226-7" aria-hidden="true"></a><span class="kw">table</span>(fit, Weekly[<span class="op">!</span>train, ]<span class="op">$</span>Direction)</span></code></pre></div>
<pre><code>##       
## fit    Down Up
##   Down   23 18
##   Up     20 43</code></pre>
<div class="sourceCode" id="cb228"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb228-1"><a href="classification.html#cb228-1" aria-hidden="true"></a><span class="kw">mean</span>(fit <span class="op">==</span><span class="st"> </span>Weekly[<span class="op">!</span>train, ]<span class="op">$</span>Direction)</span></code></pre></div>
<pre><code>## [1] 0.6346154</code></pre>
<p>KNN using the first 3 Lag variables performs marginally better than logistic
regression with <code>Lag2</code> if we tune <span class="math inline">\(k\)</span> to be <span class="math inline">\(k = 26\)</span>.</p>
</div>
<div id="question-14-1" class="section level3 hasAnchor" number="4.2.2">
<h3><span class="header-section-number">4.2.2</span> Question 14<a href="classification.html#question-14-1" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<blockquote>
<p>In this problem, you will develop a model to predict whether a given car gets
high or low gas mileage based on the <code>Auto</code> data set.</p>
<ol style="list-style-type: lower-alpha">
<li>Create a binary variable, <code>mpg01</code>, that contains a 1 if <code>mpg</code> contains a
value above its median, and a 0 if <code>mpg</code> contains a value below its
median. You can compute the median using the <code>median()</code> function. Note you
may find it helpful to use the <code>data.frame()</code> function to create a single
data set containing both <code>mpg01</code> and the other <code>Auto</code> variables.</li>
</ol>
</blockquote>
<div class="sourceCode" id="cb230"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb230-1"><a href="classification.html#cb230-1" aria-hidden="true"></a>x &lt;-<span class="st"> </span><span class="kw">cbind</span>(Auto[, <span class="dv">-1</span>], <span class="kw">data.frame</span>(<span class="st">&quot;mpg01&quot;</span> =<span class="st"> </span>Auto<span class="op">$</span>mpg <span class="op">&gt;</span><span class="st"> </span><span class="kw">median</span>(Auto<span class="op">$</span>mpg)))</span></code></pre></div>
<blockquote>
<ol start="2" style="list-style-type: lower-alpha">
<li>Explore the data graphically in order to investigate the association
between <code>mpg01</code> and the other features. Which of the other features seem
most likely to be useful in predicting <code>mpg01</code>? Scatterplots and boxplots
may be useful tools to answer this question. Describe your findings.</li>
</ol>
</blockquote>
<div class="sourceCode" id="cb231"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb231-1"><a href="classification.html#cb231-1" aria-hidden="true"></a><span class="kw">par</span>(<span class="dt">mfrow =</span> <span class="kw">c</span>(<span class="dv">2</span>, <span class="dv">4</span>))</span>
<span id="cb231-2"><a href="classification.html#cb231-2" aria-hidden="true"></a><span class="cf">for</span> (i <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span><span class="dv">7</span>) <span class="kw">hist</span>(x[, i], <span class="dt">breaks =</span> <span class="dv">20</span>, <span class="dt">main =</span> <span class="kw">colnames</span>(x)[i])</span>
<span id="cb231-3"><a href="classification.html#cb231-3" aria-hidden="true"></a></span>
<span id="cb231-4"><a href="classification.html#cb231-4" aria-hidden="true"></a><span class="kw">par</span>(<span class="dt">mfrow =</span> <span class="kw">c</span>(<span class="dv">2</span>, <span class="dv">4</span>))</span></code></pre></div>
<p><img src="04-classification_files/figure-html/unnamed-chunk-13-1.png" width="672" /></p>
<div class="sourceCode" id="cb232"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb232-1"><a href="classification.html#cb232-1" aria-hidden="true"></a><span class="cf">for</span> (i <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span><span class="dv">7</span>) <span class="kw">boxplot</span>(x[, i] <span class="op">~</span><span class="st"> </span>x<span class="op">$</span>mpg01, <span class="dt">main =</span> <span class="kw">colnames</span>(x)[i])</span>
<span id="cb232-2"><a href="classification.html#cb232-2" aria-hidden="true"></a></span>
<span id="cb232-3"><a href="classification.html#cb232-3" aria-hidden="true"></a><span class="kw">pairs</span>(x[, <span class="dv">1</span><span class="op">:</span><span class="dv">7</span>])</span></code></pre></div>
<p><img src="04-classification_files/figure-html/unnamed-chunk-13-2.png" width="672" /><img src="04-classification_files/figure-html/unnamed-chunk-13-3.png" width="672" /></p>
<p>Most variables show an association with <code>mpg01</code> category, and several
variables are colinear.</p>
<blockquote>
<ol start="3" style="list-style-type: lower-alpha">
<li>Split the data into a training set and a test set.</li>
</ol>
</blockquote>
<div class="sourceCode" id="cb233"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb233-1"><a href="classification.html#cb233-1" aria-hidden="true"></a><span class="kw">set.seed</span>(<span class="dv">1</span>)</span>
<span id="cb233-2"><a href="classification.html#cb233-2" aria-hidden="true"></a>train &lt;-<span class="st"> </span><span class="kw">sample</span>(<span class="kw">seq_len</span>(<span class="kw">nrow</span>(x)), <span class="kw">nrow</span>(x) <span class="op">*</span><span class="st"> </span><span class="dv">2</span><span class="op">/</span><span class="dv">3</span>)</span></code></pre></div>
<blockquote>
<ol start="4" style="list-style-type: lower-alpha">
<li>Perform LDA on the training data in order to predict <code>mpg01</code> using the
variables that seemed most associated with <code>mpg01</code> in (b). What is the
test error of the model obtained?</li>
</ol>
</blockquote>
<div class="sourceCode" id="cb234"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb234-1"><a href="classification.html#cb234-1" aria-hidden="true"></a><span class="kw">sort</span>(<span class="kw">sapply</span>(<span class="dv">1</span><span class="op">:</span><span class="dv">7</span>, <span class="cf">function</span>(i) {</span>
<span id="cb234-2"><a href="classification.html#cb234-2" aria-hidden="true"></a>  <span class="kw">setNames</span>(<span class="kw">abs</span>(<span class="kw">t.test</span>(x[, i] <span class="op">~</span><span class="st"> </span>x<span class="op">$</span>mpg01)<span class="op">$</span>statistic), <span class="kw">colnames</span>(x)[i])</span>
<span id="cb234-3"><a href="classification.html#cb234-3" aria-hidden="true"></a>}))</span></code></pre></div>
<pre><code>## acceleration         year       origin   horsepower displacement       weight 
##     7.302430     9.403221    11.824099    17.681939    22.632004    22.932777 
##    cylinders 
##    23.035328</code></pre>
<div class="sourceCode" id="cb236"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb236-1"><a href="classification.html#cb236-1" aria-hidden="true"></a>fit &lt;-<span class="st"> </span><span class="kw">lda</span>(mpg01 <span class="op">~</span><span class="st"> </span>cylinders <span class="op">+</span><span class="st"> </span>weight <span class="op">+</span><span class="st"> </span>displacement, <span class="dt">data =</span> x[train, ])</span>
<span id="cb236-2"><a href="classification.html#cb236-2" aria-hidden="true"></a>pred &lt;-<span class="st"> </span><span class="kw">predict</span>(fit, x[<span class="op">-</span>train, ], <span class="dt">type =</span> <span class="st">&quot;response&quot;</span>)<span class="op">$</span>class</span>
<span id="cb236-3"><a href="classification.html#cb236-3" aria-hidden="true"></a><span class="kw">mean</span>(pred <span class="op">!=</span><span class="st"> </span>x[<span class="op">-</span>train, ]<span class="op">$</span>mpg01)</span></code></pre></div>
<pre><code>## [1] 0.1068702</code></pre>
<blockquote>
<ol start="5" style="list-style-type: lower-alpha">
<li>Perform QDA on the training data in order to predict <code>mpg01</code> using the
variables that seemed most associated with <code>mpg01</code> in (b). What is the
test error of the model obtained?</li>
</ol>
</blockquote>
<div class="sourceCode" id="cb238"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb238-1"><a href="classification.html#cb238-1" aria-hidden="true"></a>fit &lt;-<span class="st"> </span><span class="kw">qda</span>(mpg01 <span class="op">~</span><span class="st"> </span>cylinders <span class="op">+</span><span class="st"> </span>weight <span class="op">+</span><span class="st"> </span>displacement, <span class="dt">data =</span> x[train, ])</span>
<span id="cb238-2"><a href="classification.html#cb238-2" aria-hidden="true"></a>pred &lt;-<span class="st"> </span><span class="kw">predict</span>(fit, x[<span class="op">-</span>train, ], <span class="dt">type =</span> <span class="st">&quot;response&quot;</span>)<span class="op">$</span>class</span>
<span id="cb238-3"><a href="classification.html#cb238-3" aria-hidden="true"></a><span class="kw">mean</span>(pred <span class="op">!=</span><span class="st"> </span>x[<span class="op">-</span>train, ]<span class="op">$</span>mpg01)</span></code></pre></div>
<pre><code>## [1] 0.09923664</code></pre>
<blockquote>
<ol start="6" style="list-style-type: lower-alpha">
<li>Perform logistic regression on the training data in order to predict
<code>mpg01</code> using the variables that seemed most associated with <code>mpg01</code> in
(b). What is the test error of the model obtained?</li>
</ol>
</blockquote>
<div class="sourceCode" id="cb240"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb240-1"><a href="classification.html#cb240-1" aria-hidden="true"></a>fit &lt;-<span class="st"> </span><span class="kw">glm</span>(mpg01 <span class="op">~</span><span class="st"> </span>cylinders <span class="op">+</span><span class="st"> </span>weight <span class="op">+</span><span class="st"> </span>displacement, <span class="dt">data =</span> x[train, ], <span class="dt">family =</span> binomial)</span>
<span id="cb240-2"><a href="classification.html#cb240-2" aria-hidden="true"></a>pred &lt;-<span class="st"> </span><span class="kw">predict</span>(fit, x[<span class="op">-</span>train, ], <span class="dt">type =</span> <span class="st">&quot;response&quot;</span>) <span class="op">&gt;</span><span class="st"> </span><span class="fl">0.5</span></span>
<span id="cb240-3"><a href="classification.html#cb240-3" aria-hidden="true"></a><span class="kw">mean</span>(pred <span class="op">!=</span><span class="st"> </span>x[<span class="op">-</span>train, ]<span class="op">$</span>mpg01)</span></code></pre></div>
<pre><code>## [1] 0.1145038</code></pre>
<blockquote>
<ol start="7" style="list-style-type: lower-alpha">
<li>Perform naive Bayes on the training data in order to predict <code>mpg01</code> using
the variables that seemed most associated with <code>mpg01</code> in (b). What is the
test error of the model obtained?</li>
</ol>
</blockquote>
<div class="sourceCode" id="cb242"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb242-1"><a href="classification.html#cb242-1" aria-hidden="true"></a>fit &lt;-<span class="st"> </span><span class="kw">naiveBayes</span>(mpg01 <span class="op">~</span><span class="st"> </span>cylinders <span class="op">+</span><span class="st"> </span>weight <span class="op">+</span><span class="st"> </span>displacement, <span class="dt">data =</span> x[train, ])</span>
<span id="cb242-2"><a href="classification.html#cb242-2" aria-hidden="true"></a>pred &lt;-<span class="st"> </span><span class="kw">predict</span>(fit, x[<span class="op">-</span>train, ], <span class="dt">type =</span> <span class="st">&quot;class&quot;</span>)</span>
<span id="cb242-3"><a href="classification.html#cb242-3" aria-hidden="true"></a><span class="kw">mean</span>(pred <span class="op">!=</span><span class="st"> </span>x[<span class="op">-</span>train, ]<span class="op">$</span>mpg01)</span></code></pre></div>
<pre><code>## [1] 0.09923664</code></pre>
<blockquote>
<ol start="8" style="list-style-type: lower-alpha">
<li>Perform KNN on the training data, with several values of <span class="math inline">\(K\)</span>, in order to
predict <code>mpg01</code>. Use only the variables that seemed most associated with
<code>mpg01</code> in (b). What test errors do you obtain? Which value of <span class="math inline">\(K\)</span> seems
to perform the best on this data set?</li>
</ol>
</blockquote>
<div class="sourceCode" id="cb244"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb244-1"><a href="classification.html#cb244-1" aria-hidden="true"></a>res &lt;-<span class="st"> </span><span class="kw">sapply</span>(<span class="dv">1</span><span class="op">:</span><span class="dv">50</span>, <span class="cf">function</span>(k) {</span>
<span id="cb244-2"><a href="classification.html#cb244-2" aria-hidden="true"></a>  fit &lt;-<span class="st"> </span><span class="kw">knn</span>(x[train, <span class="kw">c</span>(<span class="dv">1</span>, <span class="dv">4</span>, <span class="dv">2</span>)], x[<span class="op">-</span>train, <span class="kw">c</span>(<span class="dv">1</span>, <span class="dv">4</span>, <span class="dv">2</span>)], x<span class="op">$</span>mpg01[train], <span class="dt">k =</span> k)</span>
<span id="cb244-3"><a href="classification.html#cb244-3" aria-hidden="true"></a>  <span class="kw">mean</span>(fit <span class="op">!=</span><span class="st"> </span>x[<span class="op">-</span>train, ]<span class="op">$</span>mpg01)</span>
<span id="cb244-4"><a href="classification.html#cb244-4" aria-hidden="true"></a>})</span>
<span id="cb244-5"><a href="classification.html#cb244-5" aria-hidden="true"></a><span class="kw">names</span>(res) &lt;-<span class="st"> </span><span class="dv">1</span><span class="op">:</span><span class="dv">50</span></span>
<span id="cb244-6"><a href="classification.html#cb244-6" aria-hidden="true"></a><span class="kw">plot</span>(res, <span class="dt">type =</span> <span class="st">&quot;o&quot;</span>)</span></code></pre></div>
<p><img src="04-classification_files/figure-html/unnamed-chunk-19-1.png" width="672" /></p>
<div class="sourceCode" id="cb245"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb245-1"><a href="classification.html#cb245-1" aria-hidden="true"></a>res[<span class="kw">which.min</span>(res)]</span></code></pre></div>
<pre><code>##         3 
## 0.1068702</code></pre>
<p>For the models tested here, <span class="math inline">\(k = 32\)</span> appears to perform best. QDA has a lower
error rate overall, performing slightly better than LDA.</p>
</div>
<div id="question-15-1" class="section level3 hasAnchor" number="4.2.3">
<h3><span class="header-section-number">4.2.3</span> Question 15<a href="classification.html#question-15-1" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<blockquote>
<p>This problem involves writing functions.</p>
<ol style="list-style-type: lower-alpha">
<li><p>Write a function, <code>Power()</code>, that prints out the result of raising 2 to
the 3rd power. In other words, your function should compute <span class="math inline">\(2^3\)</span> and
print out the results.</p>
<p><em>Hint: Recall that <code>x^a</code> raises <code>x</code> to the power <code>a</code>. Use the <code>print()</code></em>
<em>function to output the result.</em></p></li>
</ol>
</blockquote>
<div class="sourceCode" id="cb247"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb247-1"><a href="classification.html#cb247-1" aria-hidden="true"></a>Power &lt;-<span class="st"> </span><span class="cf">function</span>() <span class="kw">print</span>(<span class="dv">2</span><span class="op">^</span><span class="dv">3</span>)</span></code></pre></div>
<blockquote>
<ol start="2" style="list-style-type: lower-alpha">
<li><p>Create a new function, <code>Power2()</code>, that allows you to pass any two
numbers, <code>x</code> and <code>a</code>, and prints out the value of <code>x^a</code>. You can do this
by beginning your function with the line</p>
<div class="sourceCode" id="cb248"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb248-1"><a href="classification.html#cb248-1" aria-hidden="true"></a><span class="op">&gt;</span><span class="st"> </span>Power2=<span class="cf">function</span>(x,a) {</span></code></pre></div>
<p>You should be able to call your function by entering, for instance,</p>
<div class="sourceCode" id="cb249"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb249-1"><a href="classification.html#cb249-1" aria-hidden="true"></a><span class="op">&gt;</span><span class="st"> </span><span class="kw">Power2</span>(<span class="dv">3</span>, <span class="dv">8</span>)</span></code></pre></div>
<p>on the command line. This should output the value of <span class="math inline">\(3^8\)</span>, namely, 6,561.</p></li>
</ol>
</blockquote>
<div class="sourceCode" id="cb250"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb250-1"><a href="classification.html#cb250-1" aria-hidden="true"></a>Power2 &lt;-<span class="st"> </span><span class="cf">function</span>(x, a) <span class="kw">print</span>(x<span class="op">^</span>a)</span></code></pre></div>
<blockquote>
<ol start="3" style="list-style-type: lower-alpha">
<li>Using the <code>Power2()</code> function that you just wrote, compute <span class="math inline">\(10^3\)</span>,
<span class="math inline">\(8^{17}\)</span>, and <span class="math inline">\(131^3\)</span>.</li>
</ol>
</blockquote>
<div class="sourceCode" id="cb251"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb251-1"><a href="classification.html#cb251-1" aria-hidden="true"></a><span class="kw">c</span>(<span class="kw">Power2</span>(<span class="dv">10</span>, <span class="dv">3</span>), <span class="kw">Power2</span>(<span class="dv">8</span>, <span class="dv">17</span>), <span class="kw">Power2</span>(<span class="dv">131</span>, <span class="dv">3</span>))</span></code></pre></div>
<pre><code>## [1] 1000
## [1] 2.2518e+15
## [1] 2248091</code></pre>
<pre><code>## [1] 1.000000e+03 2.251800e+15 2.248091e+06</code></pre>
<blockquote>
<ol start="4" style="list-style-type: lower-alpha">
<li><p>Now create a new function, <code>Power3()</code>, that actually returns the result
<code>x^a</code> as an <code>R</code> object, rather than simply printing it to the screen. That
is, if you store the value <code>x^a</code> in an object called result within your
function, then you can simply <code>return()</code> this result, using the following
line:</p>
<div class="sourceCode" id="cb254"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb254-1"><a href="classification.html#cb254-1" aria-hidden="true"></a><span class="op">&gt;</span><span class="st"> </span><span class="kw">return</span>(result)</span></code></pre></div>
<p>The line above should be the last line in your function, before the <code>}</code>
symbol.</p></li>
</ol>
</blockquote>
<div class="sourceCode" id="cb255"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb255-1"><a href="classification.html#cb255-1" aria-hidden="true"></a>Power3 &lt;-<span class="st"> </span><span class="cf">function</span>(x, a) {</span>
<span id="cb255-2"><a href="classification.html#cb255-2" aria-hidden="true"></a>  result &lt;-<span class="st"> </span>x<span class="op">^</span>a</span>
<span id="cb255-3"><a href="classification.html#cb255-3" aria-hidden="true"></a>  <span class="kw">return</span>(result)</span>
<span id="cb255-4"><a href="classification.html#cb255-4" aria-hidden="true"></a>}</span></code></pre></div>
<blockquote>
<ol start="5" style="list-style-type: lower-alpha">
<li>Now using the <code>Power3()</code> function, create a plot of <span class="math inline">\(f(x) = x^2\)</span>. The
<span class="math inline">\(x\)</span>-axis should display a range of integers from 1 to 10, and the <span class="math inline">\(y\)</span>-axis
should display <span class="math inline">\(x^2\)</span>. Label the axes appropriately, and use an appropriate
title for the figure. Consider displaying either the <span class="math inline">\(x\)</span>-axis, the
<span class="math inline">\(y\)</span>-axis, or both on the log-scale. You can do this by using <code>log = "x"</code>,
<code>log = "y"</code>, or <code>log = "xy"</code> as arguments to the <code>plot()</code> function.</li>
</ol>
</blockquote>
<div class="sourceCode" id="cb256"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb256-1"><a href="classification.html#cb256-1" aria-hidden="true"></a><span class="kw">plot</span>(<span class="dv">1</span><span class="op">:</span><span class="dv">10</span>, <span class="kw">Power3</span>(<span class="dv">1</span><span class="op">:</span><span class="dv">10</span>, <span class="dv">2</span>), </span>
<span id="cb256-2"><a href="classification.html#cb256-2" aria-hidden="true"></a>  <span class="dt">xlab =</span> <span class="st">&quot;x&quot;</span>, </span>
<span id="cb256-3"><a href="classification.html#cb256-3" aria-hidden="true"></a>  <span class="dt">ylab =</span> <span class="kw">expression</span>(<span class="kw">paste</span>(<span class="st">&quot;x&quot;</span><span class="op">^</span><span class="st">&quot;2&quot;</span>)),</span>
<span id="cb256-4"><a href="classification.html#cb256-4" aria-hidden="true"></a>  <span class="dt">log =</span> <span class="st">&quot;y&quot;</span></span>
<span id="cb256-5"><a href="classification.html#cb256-5" aria-hidden="true"></a>)</span></code></pre></div>
<p><img src="04-classification_files/figure-html/unnamed-chunk-24-1.png" width="672" /></p>
<blockquote>
<ol start="6" style="list-style-type: lower-alpha">
<li><p>Create a function, <code>PlotPower()</code>, that allows you to create a plot of <code>x</code>
against <code>x^a</code> for a fixed <code>a</code> and for a range of values of <code>x</code>. For
instance, if you call</p>
<div class="sourceCode" id="cb257"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb257-1"><a href="classification.html#cb257-1" aria-hidden="true"></a><span class="op">&gt;</span><span class="st"> </span><span class="kw">PlotPower</span>(<span class="dv">1</span><span class="op">:</span><span class="dv">10</span>, <span class="dv">3</span>)</span></code></pre></div>
<p>then a plot should be created with an <span class="math inline">\(x\)</span>-axis taking on values
<span class="math inline">\(1,2,...,10\)</span>, and a <span class="math inline">\(y\)</span>-axis taking on values <span class="math inline">\(1^3,2^3,...,10^3\)</span>.</p></li>
</ol>
</blockquote>
<div class="sourceCode" id="cb258"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb258-1"><a href="classification.html#cb258-1" aria-hidden="true"></a>PlotPower &lt;-<span class="st"> </span><span class="cf">function</span>(x, a, <span class="dt">log =</span> <span class="st">&quot;y&quot;</span>) {</span>
<span id="cb258-2"><a href="classification.html#cb258-2" aria-hidden="true"></a>  <span class="kw">plot</span>(x, <span class="kw">Power3</span>(x, a),</span>
<span id="cb258-3"><a href="classification.html#cb258-3" aria-hidden="true"></a>    <span class="dt">xlab =</span> <span class="st">&quot;x&quot;</span>, </span>
<span id="cb258-4"><a href="classification.html#cb258-4" aria-hidden="true"></a>    <span class="dt">ylab =</span> <span class="kw">substitute</span>(<span class="st">&quot;x&quot;</span><span class="op">^</span>a, <span class="kw">list</span>(<span class="dt">a =</span> a)),</span>
<span id="cb258-5"><a href="classification.html#cb258-5" aria-hidden="true"></a>    <span class="dt">log =</span> log</span>
<span id="cb258-6"><a href="classification.html#cb258-6" aria-hidden="true"></a>  )</span>
<span id="cb258-7"><a href="classification.html#cb258-7" aria-hidden="true"></a>}</span>
<span id="cb258-8"><a href="classification.html#cb258-8" aria-hidden="true"></a></span>
<span id="cb258-9"><a href="classification.html#cb258-9" aria-hidden="true"></a><span class="kw">PlotPower</span>(<span class="dv">1</span><span class="op">:</span><span class="dv">10</span>, <span class="dv">3</span>)</span></code></pre></div>
<p><img src="04-classification_files/figure-html/unnamed-chunk-25-1.png" width="672" /></p>
</div>
<div id="question-13-2" class="section level3 hasAnchor" number="4.2.4">
<h3><span class="header-section-number">4.2.4</span> Question 13<a href="classification.html#question-13-2" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<blockquote>
<p>Using the <code>Boston</code> data set, fit classification models in order to predict
whether a given census tract has a crime rate above or below the median.
Explore logistic regression, LDA, naive Bayes and KNN models using various
sub-sets of the predictors. Describe your findings.</p>
<p><em>Hint: You will have to create the response variable yourself, using the</em>
<em>variables that are contained in the <code>Boston</code> data set.</em></p>
</blockquote>
<div class="sourceCode" id="cb259"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb259-1"><a href="classification.html#cb259-1" aria-hidden="true"></a>x &lt;-<span class="st"> </span><span class="kw">cbind</span>(</span>
<span id="cb259-2"><a href="classification.html#cb259-2" aria-hidden="true"></a>  ISLR2<span class="op">::</span>Boston[, <span class="dv">-1</span>], </span>
<span id="cb259-3"><a href="classification.html#cb259-3" aria-hidden="true"></a>  <span class="kw">data.frame</span>(<span class="st">&quot;highcrim&quot;</span> =<span class="st"> </span>Boston<span class="op">$</span>crim <span class="op">&gt;</span><span class="st"> </span><span class="kw">median</span>(Boston<span class="op">$</span>crim))</span>
<span id="cb259-4"><a href="classification.html#cb259-4" aria-hidden="true"></a>)</span>
<span id="cb259-5"><a href="classification.html#cb259-5" aria-hidden="true"></a><span class="kw">set.seed</span>(<span class="dv">1</span>)</span>
<span id="cb259-6"><a href="classification.html#cb259-6" aria-hidden="true"></a>train &lt;-<span class="st"> </span><span class="kw">sample</span>(<span class="kw">seq_len</span>(<span class="kw">nrow</span>(x)), <span class="kw">nrow</span>(x) <span class="op">*</span><span class="st"> </span><span class="dv">2</span><span class="op">/</span><span class="dv">3</span>)</span></code></pre></div>
<p>We can find the most associated variables by performing wilcox tests.</p>
<div class="sourceCode" id="cb260"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb260-1"><a href="classification.html#cb260-1" aria-hidden="true"></a>ord &lt;-<span class="st"> </span><span class="kw">order</span>(<span class="kw">sapply</span>(<span class="dv">1</span><span class="op">:</span><span class="dv">12</span>, <span class="cf">function</span>(i) {</span>
<span id="cb260-2"><a href="classification.html#cb260-2" aria-hidden="true"></a>  p &lt;-<span class="st"> </span><span class="kw">wilcox.test</span>(<span class="kw">as.numeric</span>(x[train, i]) <span class="op">~</span><span class="st"> </span>x[train, ]<span class="op">$</span>highcrim)<span class="op">$</span>p.value</span>
<span id="cb260-3"><a href="classification.html#cb260-3" aria-hidden="true"></a>  <span class="kw">setNames</span>(<span class="kw">log10</span>(p), <span class="kw">colnames</span>(x)[i])</span>
<span id="cb260-4"><a href="classification.html#cb260-4" aria-hidden="true"></a>}))</span>
<span id="cb260-5"><a href="classification.html#cb260-5" aria-hidden="true"></a>ord &lt;-<span class="st"> </span><span class="kw">names</span>(x)[ord]</span>
<span id="cb260-6"><a href="classification.html#cb260-6" aria-hidden="true"></a>ord</span></code></pre></div>
<pre><code>##  [1] &quot;nox&quot;     &quot;dis&quot;     &quot;indus&quot;   &quot;tax&quot;     &quot;age&quot;     &quot;rad&quot;     &quot;zn&quot;     
##  [8] &quot;lstat&quot;   &quot;medv&quot;    &quot;ptratio&quot; &quot;rm&quot;      &quot;chas&quot;</code></pre>
<p>Variables <code>nox</code> (nitrogen oxides concentration) followed by <code>dis</code> (distance to
employment center) appear to be most associated with high crime.</p>
<p>Let’s reorder columns by those most associated with highcrim (in the training
data)</p>
<div class="sourceCode" id="cb262"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb262-1"><a href="classification.html#cb262-1" aria-hidden="true"></a>x &lt;-<span class="st"> </span>x[, <span class="kw">c</span>(ord, <span class="st">&quot;highcrim&quot;</span>)]</span></code></pre></div>
<p>Let’s look at univariate associations with <code>highcrim</code> (in the training data)</p>
<div class="sourceCode" id="cb263"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb263-1"><a href="classification.html#cb263-1" aria-hidden="true"></a>x[train, ] <span class="op">|</span><span class="er">&gt;</span></span>
<span id="cb263-2"><a href="classification.html#cb263-2" aria-hidden="true"></a><span class="st">  </span><span class="kw">pivot_longer</span>(<span class="op">!</span>highcrim) <span class="op">|</span><span class="er">&gt;</span></span>
<span id="cb263-3"><a href="classification.html#cb263-3" aria-hidden="true"></a><span class="st">  </span><span class="kw">mutate</span>(<span class="dt">name =</span> <span class="kw">factor</span>(name, <span class="dt">levels =</span> ord)) <span class="op">|</span><span class="er">&gt;</span></span>
<span id="cb263-4"><a href="classification.html#cb263-4" aria-hidden="true"></a><span class="st">  </span><span class="kw">ggplot</span>(<span class="kw">aes</span>(highcrim, value)) <span class="op">+</span><span class="st"> </span></span>
<span id="cb263-5"><a href="classification.html#cb263-5" aria-hidden="true"></a><span class="st">  </span><span class="kw">geom_boxplot</span>() <span class="op">+</span><span class="st"> </span></span>
<span id="cb263-6"><a href="classification.html#cb263-6" aria-hidden="true"></a><span class="st">  </span><span class="kw">facet_wrap</span>(<span class="op">~</span>name, <span class="dt">scale =</span> <span class="st">&quot;free&quot;</span>)</span></code></pre></div>
<p><img src="04-classification_files/figure-html/unnamed-chunk-29-1.png" width="672" /></p>
<p>Fit lda, logistic regression, naive Bayes and KNN models (with k = 1..50) for a
set of specific predictors and return the error rate. We fit models using
increasing numbers of predictors: column 1, then columns 1 and 2 etc.</p>
<div class="sourceCode" id="cb264"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb264-1"><a href="classification.html#cb264-1" aria-hidden="true"></a>fit_models &lt;-<span class="st"> </span><span class="cf">function</span>(cols, <span class="dt">k_vals =</span> <span class="dv">1</span><span class="op">:</span><span class="dv">50</span>) {</span>
<span id="cb264-2"><a href="classification.html#cb264-2" aria-hidden="true"></a>  dat_train &lt;-<span class="st"> </span>x[train, cols, drop =<span class="st"> </span><span class="ot">FALSE</span>]</span>
<span id="cb264-3"><a href="classification.html#cb264-3" aria-hidden="true"></a>  dat_test &lt;-<span class="st"> </span>x[<span class="op">-</span>train, cols, drop =<span class="st"> </span><span class="ot">FALSE</span>]</span>
<span id="cb264-4"><a href="classification.html#cb264-4" aria-hidden="true"></a></span>
<span id="cb264-5"><a href="classification.html#cb264-5" aria-hidden="true"></a>  fit &lt;-<span class="st"> </span><span class="kw">lda</span>(x<span class="op">$</span>highcrim[train] <span class="op">~</span><span class="st"> </span>., <span class="dt">data =</span> dat_train)</span>
<span id="cb264-6"><a href="classification.html#cb264-6" aria-hidden="true"></a>  pred &lt;-<span class="st"> </span><span class="kw">predict</span>(fit, dat_test, <span class="dt">type =</span> <span class="st">&quot;response&quot;</span>)<span class="op">$</span>class</span>
<span id="cb264-7"><a href="classification.html#cb264-7" aria-hidden="true"></a>  lda_err &lt;-<span class="st"> </span><span class="kw">mean</span>(pred <span class="op">!=</span><span class="st"> </span>x<span class="op">$</span>highcrim[<span class="op">-</span>train])</span>
<span id="cb264-8"><a href="classification.html#cb264-8" aria-hidden="true"></a></span>
<span id="cb264-9"><a href="classification.html#cb264-9" aria-hidden="true"></a>  fit &lt;-<span class="st"> </span><span class="kw">glm</span>(x<span class="op">$</span>highcrim[train] <span class="op">~</span><span class="st"> </span>., <span class="dt">data =</span> dat_train, <span class="dt">family =</span> binomial)</span>
<span id="cb264-10"><a href="classification.html#cb264-10" aria-hidden="true"></a>  pred &lt;-<span class="st"> </span><span class="kw">predict</span>(fit, dat_test, <span class="dt">type =</span> <span class="st">&quot;response&quot;</span>) <span class="op">&gt;</span><span class="st"> </span><span class="fl">0.5</span></span>
<span id="cb264-11"><a href="classification.html#cb264-11" aria-hidden="true"></a>  logreg_err &lt;-<span class="st"> </span><span class="kw">mean</span>(pred <span class="op">!=</span><span class="st"> </span>x<span class="op">$</span>highcrim[<span class="op">-</span>train])</span>
<span id="cb264-12"><a href="classification.html#cb264-12" aria-hidden="true"></a></span>
<span id="cb264-13"><a href="classification.html#cb264-13" aria-hidden="true"></a>  fit &lt;-<span class="st"> </span><span class="kw">naiveBayes</span>(x<span class="op">$</span>highcrim[train] <span class="op">~</span><span class="st"> </span>., <span class="dt">data =</span> dat_train)</span>
<span id="cb264-14"><a href="classification.html#cb264-14" aria-hidden="true"></a>  pred &lt;-<span class="st"> </span><span class="kw">predict</span>(fit, dat_test, <span class="dt">type =</span> <span class="st">&quot;class&quot;</span>)</span>
<span id="cb264-15"><a href="classification.html#cb264-15" aria-hidden="true"></a>  nb_err &lt;-<span class="st"> </span><span class="kw">mean</span>(pred <span class="op">!=</span><span class="st"> </span>x<span class="op">$</span>highcrim[<span class="op">-</span>train])</span>
<span id="cb264-16"><a href="classification.html#cb264-16" aria-hidden="true"></a></span>
<span id="cb264-17"><a href="classification.html#cb264-17" aria-hidden="true"></a>  res &lt;-<span class="st"> </span><span class="kw">sapply</span>(k_vals, <span class="cf">function</span>(k) {</span>
<span id="cb264-18"><a href="classification.html#cb264-18" aria-hidden="true"></a>    fit &lt;-<span class="st"> </span><span class="kw">knn</span>(dat_train, dat_test, x<span class="op">$</span>highcrim[train], <span class="dt">k =</span> k)</span>
<span id="cb264-19"><a href="classification.html#cb264-19" aria-hidden="true"></a>    <span class="kw">mean</span>(fit <span class="op">!=</span><span class="st"> </span>x<span class="op">$</span>highcrim[<span class="op">-</span>train])</span>
<span id="cb264-20"><a href="classification.html#cb264-20" aria-hidden="true"></a>  })</span>
<span id="cb264-21"><a href="classification.html#cb264-21" aria-hidden="true"></a>  knn_err &lt;-<span class="st"> </span><span class="kw">min</span>(res)</span>
<span id="cb264-22"><a href="classification.html#cb264-22" aria-hidden="true"></a></span>
<span id="cb264-23"><a href="classification.html#cb264-23" aria-hidden="true"></a>  <span class="kw">c</span>(<span class="st">&quot;LDA&quot;</span> =<span class="st"> </span>lda_err, <span class="st">&quot;LR&quot;</span> =<span class="st"> </span>logreg_err, <span class="st">&quot;NB&quot;</span> =<span class="st"> </span>nb_err, <span class="st">&quot;KNN&quot;</span> =<span class="st"> </span>knn_err)</span>
<span id="cb264-24"><a href="classification.html#cb264-24" aria-hidden="true"></a>}</span></code></pre></div>
<div class="sourceCode" id="cb265"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb265-1"><a href="classification.html#cb265-1" aria-hidden="true"></a>res &lt;-<span class="st"> </span><span class="kw">sapply</span>(<span class="dv">1</span><span class="op">:</span><span class="dv">12</span>, <span class="cf">function</span>(max) <span class="kw">fit_models</span>(<span class="dv">1</span><span class="op">:</span>max))</span>
<span id="cb265-2"><a href="classification.html#cb265-2" aria-hidden="true"></a>res &lt;-<span class="st"> </span><span class="kw">as_tibble</span>(<span class="kw">t</span>(res))</span>
<span id="cb265-3"><a href="classification.html#cb265-3" aria-hidden="true"></a>res<span class="op">$</span>n_var &lt;-<span class="st"> </span><span class="dv">1</span><span class="op">:</span><span class="dv">12</span></span>
<span id="cb265-4"><a href="classification.html#cb265-4" aria-hidden="true"></a><span class="kw">pivot_longer</span>(res, <span class="dt">cols =</span> <span class="op">!</span>n_var) <span class="op">|</span><span class="er">&gt;</span></span>
<span id="cb265-5"><a href="classification.html#cb265-5" aria-hidden="true"></a><span class="st">  </span><span class="kw">ggplot</span>(<span class="kw">aes</span>(n_var, value, <span class="dt">col =</span> name)) <span class="op">+</span><span class="st"> </span></span>
<span id="cb265-6"><a href="classification.html#cb265-6" aria-hidden="true"></a><span class="st">  </span><span class="kw">geom_line</span>() <span class="op">+</span><span class="st"> </span></span>
<span id="cb265-7"><a href="classification.html#cb265-7" aria-hidden="true"></a><span class="st">  </span><span class="kw">xlab</span>(<span class="st">&quot;Number of predictors&quot;</span>) <span class="op">+</span><span class="st"> </span></span>
<span id="cb265-8"><a href="classification.html#cb265-8" aria-hidden="true"></a><span class="st">  </span><span class="kw">ylab</span>(<span class="st">&quot;Error rate&quot;</span>)</span></code></pre></div>
<p><img src="04-classification_files/figure-html/unnamed-chunk-31-1.png" width="672" /></p>
<p>KNN appears to perform better (if we tune <span class="math inline">\(k\)</span>) for all numbers of predictors.</p>
<div class="sourceCode" id="cb266"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb266-1"><a href="classification.html#cb266-1" aria-hidden="true"></a>fit &lt;-<span class="st"> </span><span class="kw">knn</span>(</span>
<span id="cb266-2"><a href="classification.html#cb266-2" aria-hidden="true"></a>  x[train, <span class="st">&quot;nox&quot;</span>, <span class="dt">drop =</span> <span class="ot">FALSE</span>],</span>
<span id="cb266-3"><a href="classification.html#cb266-3" aria-hidden="true"></a>  x[<span class="op">-</span>train, <span class="st">&quot;nox&quot;</span>, <span class="dt">drop =</span> <span class="ot">FALSE</span>],</span>
<span id="cb266-4"><a href="classification.html#cb266-4" aria-hidden="true"></a>  x<span class="op">$</span>highcrim[train],</span>
<span id="cb266-5"><a href="classification.html#cb266-5" aria-hidden="true"></a>  <span class="dt">k =</span> <span class="dv">1</span></span>
<span id="cb266-6"><a href="classification.html#cb266-6" aria-hidden="true"></a>)</span>
<span id="cb266-7"><a href="classification.html#cb266-7" aria-hidden="true"></a><span class="kw">table</span>(fit, x[<span class="op">-</span>train, ]<span class="op">$</span>highcrim)</span></code></pre></div>
<pre><code>##        
## fit     FALSE TRUE
##   FALSE    78    2
##   TRUE      3   86</code></pre>
<div class="sourceCode" id="cb268"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb268-1"><a href="classification.html#cb268-1" aria-hidden="true"></a><span class="kw">mean</span>(fit <span class="op">!=</span><span class="st"> </span>x[<span class="op">-</span>train, ]<span class="op">$</span>highcrim) <span class="op">*</span><span class="st"> </span><span class="dv">100</span></span></code></pre></div>
<pre><code>## [1] 2.95858</code></pre>
<p>Surprisingly, the best model (with an error rate of &lt;5%) uses <span class="math inline">\(k = 1\)</span> and
assigns crime rate categories based on the town with the single most similar
nitrogen oxide concentration (<code>nox</code>). This might be, for example, because nearby
towns have similar crime rates, and we can obtain good predictions by predicting
crime rate based on a nearby town.</p>
<p>But what if we only consider <span class="math inline">\(k = 20\)</span>.</p>
<div class="sourceCode" id="cb270"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb270-1"><a href="classification.html#cb270-1" aria-hidden="true"></a>res &lt;-<span class="st"> </span><span class="kw">sapply</span>(<span class="dv">1</span><span class="op">:</span><span class="dv">12</span>, <span class="cf">function</span>(max) <span class="kw">fit_models</span>(<span class="dv">1</span><span class="op">:</span>max, <span class="dt">k_vals =</span> <span class="dv">20</span>))</span>
<span id="cb270-2"><a href="classification.html#cb270-2" aria-hidden="true"></a>res &lt;-<span class="st"> </span><span class="kw">as_tibble</span>(<span class="kw">t</span>(res))</span>
<span id="cb270-3"><a href="classification.html#cb270-3" aria-hidden="true"></a>res<span class="op">$</span>n_var &lt;-<span class="st"> </span><span class="dv">1</span><span class="op">:</span><span class="dv">12</span></span>
<span id="cb270-4"><a href="classification.html#cb270-4" aria-hidden="true"></a><span class="kw">pivot_longer</span>(res, <span class="dt">cols =</span> <span class="op">!</span>n_var) <span class="op">|</span><span class="er">&gt;</span></span>
<span id="cb270-5"><a href="classification.html#cb270-5" aria-hidden="true"></a><span class="st">  </span><span class="kw">ggplot</span>(<span class="kw">aes</span>(n_var, value, <span class="dt">col =</span> name)) <span class="op">+</span></span>
<span id="cb270-6"><a href="classification.html#cb270-6" aria-hidden="true"></a><span class="st">  </span><span class="kw">geom_line</span>() <span class="op">+</span></span>
<span id="cb270-7"><a href="classification.html#cb270-7" aria-hidden="true"></a><span class="st">  </span><span class="kw">xlab</span>(<span class="st">&quot;Number of predictors&quot;</span>) <span class="op">+</span></span>
<span id="cb270-8"><a href="classification.html#cb270-8" aria-hidden="true"></a><span class="st">  </span><span class="kw">ylab</span>(<span class="st">&quot;Error rate&quot;</span>)</span></code></pre></div>
<p><img src="04-classification_files/figure-html/unnamed-chunk-33-1.png" width="672" /></p>
<p>KNN still performs best with a single predictor (<code>nox</code>), but logistic regression
with 12 predictors also performs well and has an error rate of ~12%.</p>
<div class="sourceCode" id="cb271"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb271-1"><a href="classification.html#cb271-1" aria-hidden="true"></a>vars &lt;-<span class="st"> </span><span class="kw">names</span>(x)[<span class="dv">1</span><span class="op">:</span><span class="dv">12</span>]</span>
<span id="cb271-2"><a href="classification.html#cb271-2" aria-hidden="true"></a>dat_train &lt;-<span class="st"> </span>x[train, vars]</span>
<span id="cb271-3"><a href="classification.html#cb271-3" aria-hidden="true"></a>dat_test &lt;-<span class="st"> </span>x[<span class="op">-</span>train, vars]</span>
<span id="cb271-4"><a href="classification.html#cb271-4" aria-hidden="true"></a></span>
<span id="cb271-5"><a href="classification.html#cb271-5" aria-hidden="true"></a>fit &lt;-<span class="st"> </span><span class="kw">glm</span>(x<span class="op">$</span>highcrim[train] <span class="op">~</span><span class="st"> </span>., <span class="dt">data =</span> dat_train, <span class="dt">family =</span> binomial)</span>
<span id="cb271-6"><a href="classification.html#cb271-6" aria-hidden="true"></a>pred &lt;-<span class="st"> </span><span class="kw">predict</span>(fit, dat_test, <span class="dt">type =</span> <span class="st">&quot;response&quot;</span>) <span class="op">&gt;</span><span class="st"> </span><span class="fl">0.5</span></span>
<span id="cb271-7"><a href="classification.html#cb271-7" aria-hidden="true"></a><span class="kw">table</span>(pred, x[<span class="op">-</span>train, ]<span class="op">$</span>highcrim)</span></code></pre></div>
<pre><code>##        
## pred    FALSE TRUE
##   FALSE    70    9
##   TRUE     11   79</code></pre>
<div class="sourceCode" id="cb273"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb273-1"><a href="classification.html#cb273-1" aria-hidden="true"></a><span class="kw">mean</span>(pred <span class="op">!=</span><span class="st"> </span>x<span class="op">$</span>highcrim[<span class="op">-</span>train]) <span class="op">*</span><span class="st"> </span><span class="dv">100</span></span></code></pre></div>
<pre><code>## [1] 11.83432</code></pre>
<div class="sourceCode" id="cb275"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb275-1"><a href="classification.html#cb275-1" aria-hidden="true"></a><span class="kw">summary</span>(fit)</span></code></pre></div>
<pre><code>## 
## Call:
## glm(formula = x$highcrim[train] ~ ., family = binomial, data = dat_train)
## 
## Coefficients:
##               Estimate Std. Error z value Pr(&gt;|z|)    
## (Intercept) -44.525356   7.935621  -5.611 2.01e-08 ***
## nox          55.062428  10.281556   5.355 8.53e-08 ***
## dis           1.080847   0.304084   3.554 0.000379 ***
## indus        -0.067493   0.058547  -1.153 0.248997    
## tax          -0.005336   0.003138  -1.700 0.089060 .  
## age           0.020965   0.014190   1.477 0.139556    
## rad           0.678196   0.192193   3.529 0.000418 ***
## zn           -0.099558   0.045914  -2.168 0.030134 *  
## lstat         0.134035   0.058623   2.286 0.022231 *  
## medv          0.213114   0.088922   2.397 0.016547 *  
## ptratio       0.294396   0.155285   1.896 0.057981 .  
## rm           -0.518115   0.896423  -0.578 0.563278    
## chas          0.139557   0.798632   0.175 0.861280    
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## (Dispersion parameter for binomial family taken to be 1)
## 
##     Null deviance: 467.04  on 336  degrees of freedom
## Residual deviance: 135.80  on 324  degrees of freedom
## AIC: 161.8
## 
## Number of Fisher Scoring iterations: 9</code></pre>

</div>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="linear-regression.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="resampling-methods.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": true,
"facebook": false,
"twitter": false,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"whatsapp": false,
"all": {}
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/danhalligan/ISLRv2-solutions/edit/main/04-classification.Rmd",
"text": "Edit"
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": null,
"search": {
"engine": "fuse",
"options": null
},
"toc": {
"collapse": "section"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/latest.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
