[["index.html", "An Introduction to Statistical Learning Exercise solutions in R 1 Introduction", " An Introduction to Statistical Learning Exercise solutions in R 1 Introduction This bookdown document provides solutions for exercises in the book “An Introduction to Statistical Learning” by Gareth James, Daniela Witten, Trevor Hastie and Robert Tibshirani. "],["statistical-learning.html", "2 Statistical Learning 2.1 Conceptual 2.2 Applied", " 2 Statistical Learning 2.1 Conceptual 2.1.1 Question 1 For each of parts (a) through (d), indicate whether we would generally expect the performance of a flexible statistical learning method to be better or worse than an inflexible method. Justify your answer. The sample size \\(n\\) is extremely large, and the number of predictors \\(p\\) is small. Flexible best - opposite of b. The number of predictors \\(p\\) is extremely large, and the number of observations \\(n\\) is small. Inflexible best - high chance of some predictors being randomly associated. The relationship between the predictors and response is highly non-linear. Flexible best - inflexible leads to high bias. The variance of the error terms, i.e. \\(\\sigma^2 = Var(\\epsilon)\\), is extremely high. Inflexible best - opposite of c. 2.1.2 Question 2 Explain whether each scenario is a classification or regression problem, and indicate whether we are most interested in inference or prediction. Finally, provide \\(n\\) and \\(p\\). We collect a set of data on the top 500 firms in the US. For each firm we record profit, number of employees, industry and the CEO salary. We are interested in understanding which factors affect CEO salary. \\(n=500\\), \\(p=3\\), regression, inference. We are considering launching a new product and wish to know whether it will be a success or a failure. We collect data on 20 similar products that were previously launched. For each product we have recorded whether it was a success or failure, price charged for the product, marketing budget, competition price, and ten other variables. \\(n=20\\), \\(p=13\\), classification, prediction. We are interested in predicting the % change in the USD/Euro exchange rate in relation to the weekly changes in the world stock markets. Hence we collect weekly data for all of 2012. For each week we record the % change in the USD/Euro, the % change in the US market, the % change in the British market, and the % change in the German market. \\(n=52\\), \\(p=3\\), regression, prediction. 2.1.3 Question 3 We now revisit the bias-variance decomposition. Provide a sketch of typical (squared) bias, variance, training error, test error, and Bayes (or irreducible) error curves, on a single plot, as we go from less flexible statistical learning methods towards more flexible approaches. The x-axis should represent the amount of flexibility in the method, and the y-axis should represent the values for each curve. There should be five curves. Make sure to label each one. Explain why each of the five curves has the shape displayed in part (a). (squared) bias: Decreases with increasing flexibility (Generally, more flexible methods result in less bias). variance: Increases with increasing flexibility (In general, more flexible statistical methods have higher variance). training error: Decreases with model flexibility (More complex models will better fit the training data). test error: Decreases initially, then increases due to overfitting (less bias but more training error). Bayes (irreducible) error: fixed (does not change with model). 2.1.4 Question 4 You will now think of some real-life applications for statistical learning. Describe three real-life applications in which classification might be useful. Describe the response, as well as the predictors. Is the goal of each application inference or prediction? Explain your answer. Coffee machine cleaned? (day of week, person assigned), inference. Is a flight delayed? (airline, airport etc), inference. Beer type (IPA, pilsner etc.), prediction. Describe three real-life applications in which regression might be useful. Describe the response, as well as the predictors. Is the goal of each application inference or prediction? Explain your answer. Amount of bonus paid (profitability, client feedback), prediction. Person’s height, prediction. House price, inference. Describe three real-life applications in which cluster analysis might be useful. RNAseq tumour gene expression data. SNPs in human populations. Frequencies of mutations (with base pair context) in somatic mutation data. 2.1.5 Question 5 What are the advantages and disadvantages of a very flexible (versus a less flexible) approach for regression or classification? Under what circumstances might a more flexible approach be preferred to a less flexible approach? When might a less flexible approach be preferred? Inflexible is more interpretable, fewer observations required, can be biased. Flexible can overfit (high error variance). In cases where we have high \\(n\\) or non-linear patterns flexible will be preferred. 2.1.6 Question 6 Describe the differences between a parametric and a non-parametric statistical learning approach. What are the advantages of a parametric approach to regression or classification (as opposed to a non-parametric approach)? What are its disadvantages? Parametric uses (model) parameters. Parametric models can be more interpretable as there is a model behind how data is generated. However, the disadvantage is that the model might not reflect reality. If the model is too far from the truth, estimates will be poor and more flexible models can fit many different forms and require more parameters (leading to overfitting). Non-parametric approaches do not estimate a small number of parameters, so a large number of observations may be needed to obtain accurate estimates. 2.1.7 Question 7 The table below provides a training data set containing six observations, three predictors, and one qualitative response variable. Obs. \\(X_1\\) \\(X_2\\) \\(X_3\\) \\(Y\\) 1 0 3 0 Red 2 2 0 0 Red 3 0 1 3 Red 4 0 1 2 Green 5 -1 0 1 Green 6 1 1 1 Red Suppose we wish to use this data set to make a prediction for \\(Y\\) when \\(X_1 = X_2 = X_3 = 0\\) using \\(K\\)-nearest neighbors. Compute the Euclidean distance between each observation and the test point, \\(X_1 = X_2 = X_3 = 0\\). dat &lt;- data.frame( &quot;x1&quot; = c(0, 2, 0, 0, -1, 1), &quot;x2&quot; = c(3, 0, 1, 1, 0, 1), &quot;x3&quot; = c(0, 0, 3, 2, 1, 1), &quot;y&quot; = c(&quot;Red&quot;, &quot;Red&quot;, &quot;Red&quot;, &quot;Green&quot;, &quot;Green&quot;, &quot;Red&quot;) ) # Euclidean distance between points and c(0, 0, 0) dist &lt;- sqrt(dat[[&quot;x1&quot;]]^2 + dat[[&quot;x2&quot;]]^2 + dat[[&quot;x3&quot;]]^2) signif(dist, 3) ## [1] 3.00 2.00 3.16 2.24 1.41 1.73 What is our prediction with \\(K = 1\\)? Why? knn &lt;- function(k) { names(which.max(table(dat[[&quot;y&quot;]][order(dist)[1:k]]))) } knn(1) ## [1] &quot;Green&quot; Green (based on data point 5 only) What is our prediction with \\(K = 3\\)? Why? knn(3) ## [1] &quot;Red&quot; Red (based on data points 2, 5, 6) If the Bayes decision boundary in this problem is highly non-linear, then would we expect the best value for \\(K\\) to be large or small? Why? Small (high \\(k\\) leads to linear boundaries due to averaging) 2.2 Applied 2.2.1 Question 8 This exercise relates to the College data set, which can be found in the file College.csv. It contains a number of variables for 777 different universities and colleges in the US. The variables are Private : Public/private indicator Apps : Number of applications received Accept : Number of applicants accepted Enroll : Number of new students enrolled Top10perc : New students from top 10% of high school class Top25perc : New students from top 25% of high school class F.Undergrad : Number of full-time undergraduates P.Undergrad : Number of part-time undergraduates Outstate : Out-of-state tuition Room.Board : Room and board costs Books : Estimated book costs Personal : Estimated personal spending PhD : Percent of faculty with Ph.D.’s Terminal : Percent of faculty with terminal degree S.F.Ratio : Student/faculty ratio perc.alumni : Percent of alumni who donate Expend : Instructional expenditure per student Grad.Rate : Graduation rate Before reading the data into R, it can be viewed in Excel or a text editor. Use the read.csv() function to read the data into R. Call the loaded data college. Make sure that you have the directory set to the correct location for the data. college &lt;- read.csv(&quot;data/College.csv&quot;) Look at the data using the View() function. You should notice that the first column is just the name of each university. We don’t really want R to treat this as data. However, it may be handy to have these names for later. Try the following commands: rownames(college) &lt;- college[, 1] View(college) You should see that there is now a row.names column with the name of each university recorded. This means that R has given each row a name corresponding to the appropriate university. R will not try to perform calculations on the row names. However, we still need to eliminate the first column in the data where the names are stored. Try college &lt;- college [, -1] View(college) Now you should see that the first data column is Private. Note that another column labeled row.names now appears before the Private column. However, this is not a data column but rather the name that R is giving to each row. rownames(college) &lt;- college[, 1] college &lt;- college[, -1] Use the summary() function to produce a numerical summary of the variables in the data set. Use the pairs() function to produce a scatterplot matrix of the first ten columns or variables of the data. Recall that you can reference the first ten columns of a matrix A using A[,1:10]. Use the plot() function to produce side-by-side boxplots of Outstate versus Private. Create a new qualitative variable, called Elite, by binning the Top10perc variable. We are going to divide universities into two groups based on whether or not the proportion of students coming from the top 10% of their high school classes exceeds 50%. &gt; Elite &lt;- rep(&quot;No&quot;, nrow(college)) &gt; Elite[college$Top10perc &gt; 50] &lt;- &quot;Yes&quot; &gt; Elite &lt;- as.factor(Elite) &gt; college &lt;- data.frame(college, Elite) Use the summary() function to see how many elite universities there are. Now use the plot() function to produce side-by-side boxplots of Outstate versus Elite. Use the hist() function to produce some histograms with differing numbers of bins for a few of the quantitative variables. You may find the command par(mfrow=c(2,2)) useful: it will divide the print window into four regions so that four plots can be made simultaneously. Modifying the arguments to this function will divide the screen in other ways. Continue exploring the data, and provide a brief summary of what you discover. summary(college) ## Private Apps Accept Enroll ## Length:777 Min. : 81 Min. : 72 Min. : 35 ## Class :character 1st Qu.: 776 1st Qu.: 604 1st Qu.: 242 ## Mode :character Median : 1558 Median : 1110 Median : 434 ## Mean : 3002 Mean : 2019 Mean : 780 ## 3rd Qu.: 3624 3rd Qu.: 2424 3rd Qu.: 902 ## Max. :48094 Max. :26330 Max. :6392 ## Top10perc Top25perc F.Undergrad P.Undergrad ## Min. : 1.00 Min. : 9.0 Min. : 139 Min. : 1.0 ## 1st Qu.:15.00 1st Qu.: 41.0 1st Qu.: 992 1st Qu.: 95.0 ## Median :23.00 Median : 54.0 Median : 1707 Median : 353.0 ## Mean :27.56 Mean : 55.8 Mean : 3700 Mean : 855.3 ## 3rd Qu.:35.00 3rd Qu.: 69.0 3rd Qu.: 4005 3rd Qu.: 967.0 ## Max. :96.00 Max. :100.0 Max. :31643 Max. :21836.0 ## Outstate Room.Board Books Personal ## Min. : 2340 Min. :1780 Min. : 96.0 Min. : 250 ## 1st Qu.: 7320 1st Qu.:3597 1st Qu.: 470.0 1st Qu.: 850 ## Median : 9990 Median :4200 Median : 500.0 Median :1200 ## Mean :10441 Mean :4358 Mean : 549.4 Mean :1341 ## 3rd Qu.:12925 3rd Qu.:5050 3rd Qu.: 600.0 3rd Qu.:1700 ## Max. :21700 Max. :8124 Max. :2340.0 Max. :6800 ## PhD Terminal S.F.Ratio perc.alumni ## Min. : 8.00 Min. : 24.0 Min. : 2.50 Min. : 0.00 ## 1st Qu.: 62.00 1st Qu.: 71.0 1st Qu.:11.50 1st Qu.:13.00 ## Median : 75.00 Median : 82.0 Median :13.60 Median :21.00 ## Mean : 72.66 Mean : 79.7 Mean :14.09 Mean :22.74 ## 3rd Qu.: 85.00 3rd Qu.: 92.0 3rd Qu.:16.50 3rd Qu.:31.00 ## Max. :103.00 Max. :100.0 Max. :39.80 Max. :64.00 ## Expend Grad.Rate ## Min. : 3186 Min. : 10.00 ## 1st Qu.: 6751 1st Qu.: 53.00 ## Median : 8377 Median : 65.00 ## Mean : 9660 Mean : 65.46 ## 3rd Qu.:10830 3rd Qu.: 78.00 ## Max. :56233 Max. :118.00 college$Private &lt;- college$Private == &quot;Yes&quot; pairs(college[, 1:10], cex = 0.2) plot(college$Outstate ~ factor(college$Private), xlab = &quot;Private&quot;, ylab = &quot;Outstate&quot;) college$Elite &lt;- factor(ifelse(college$Top10perc &gt; 50, &quot;Yes&quot;, &quot;No&quot;)) summary(college$Elite) ## No Yes ## 699 78 plot(college$Outstate ~ college$Elite, xlab = &quot;Elite&quot;, ylab = &quot;Outstate&quot;) par(mfrow = c(2,2)) for (n in c(5, 10, 20, 50)) { hist(college$Enroll, breaks = n, main = paste(&quot;n =&quot;, n), xlab = &quot;Enroll&quot;) } chisq.test(college$Private, college$Elite) ## ## Pearson&#39;s Chi-squared test with Yates&#39; continuity correction ## ## data: college$Private and college$Elite ## X-squared = 4.3498, df = 1, p-value = 0.03701 Whether a college is Private and Elite is not random! 2.2.2 Question 9 This exercise involves the Auto data set studied in the lab. Make sure that the missing values have been removed from the data. x &lt;- read.table(&quot;data/Auto.data&quot;, header = TRUE, na.strings = &quot;?&quot;) x &lt;- na.omit(x) Which of the predictors are quantitative, and which are qualitative? sapply(x, class) ## mpg cylinders displacement horsepower weight acceleration ## &quot;numeric&quot; &quot;integer&quot; &quot;numeric&quot; &quot;numeric&quot; &quot;numeric&quot; &quot;numeric&quot; ## year origin name ## &quot;integer&quot; &quot;integer&quot; &quot;character&quot; numeric &lt;- which(sapply(x, class) == &quot;numeric&quot;) names(numeric) ## [1] &quot;mpg&quot; &quot;displacement&quot; &quot;horsepower&quot; &quot;weight&quot; &quot;acceleration&quot; What is the range of each quantitative predictor? You can answer this using the range() function. sapply(x[, numeric], function(x) diff(range(x))) ## mpg displacement horsepower weight acceleration ## 37.6 387.0 184.0 3527.0 16.8 What is the mean and standard deviation of each quantitative predictor? library(tidyverse) ## ── Attaching packages ─────────────────────────────────────── tidyverse 1.3.2 ── ## ✔ ggplot2 3.3.6 ✔ purrr 0.3.5 ## ✔ tibble 3.1.8 ✔ dplyr 1.0.10 ## ✔ tidyr 1.2.1 ✔ stringr 1.4.1 ## ✔ readr 2.1.3 ✔ forcats 0.5.2 ## ── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ── ## ✖ dplyr::filter() masks stats::filter() ## ✖ dplyr::lag() masks stats::lag() library(knitr) x[, numeric] |&gt; pivot_longer(everything()) |&gt; group_by(name) |&gt; summarise( Mean = mean(value), SD = sd(value) ) |&gt; kable() name Mean SD acceleration 15.54133 2.758864 displacement 194.41199 104.644004 horsepower 104.46939 38.491160 mpg 23.44592 7.805008 weight 2977.58418 849.402560 Now remove the 10th through 85th observations. What is the range, mean, and standard deviation of each predictor in the subset of the data that remains? x[-(10:85), numeric] |&gt; pivot_longer(everything()) |&gt; group_by(name) |&gt; summarise( Range = diff(range(value)), Mean = mean(value), SD = sd(value) ) |&gt; kable() name Range Mean SD acceleration 16.3 15.72690 2.693721 displacement 387.0 187.24051 99.678367 horsepower 184.0 100.72152 35.708853 mpg 35.6 24.40443 7.867283 weight 3348.0 2935.97152 811.300208 Using the full data set, investigate the predictors graphically, using scatterplots or other tools of your choice. Create some plots highlighting the relationships among the predictors. Comment on your findings. pairs(x[, numeric], cex = 0.2) cor(x[, numeric]) |&gt; kable() mpg displacement horsepower weight acceleration mpg 1.0000000 -0.8051269 -0.7784268 -0.8322442 0.4233285 displacement -0.8051269 1.0000000 0.8972570 0.9329944 -0.5438005 horsepower -0.7784268 0.8972570 1.0000000 0.8645377 -0.6891955 weight -0.8322442 0.9329944 0.8645377 1.0000000 -0.4168392 acceleration 0.4233285 -0.5438005 -0.6891955 -0.4168392 1.0000000 heatmap(cor(x[, numeric]), cexRow = 1.1, cexCol = 1.1, margins = c(8, 8)) Many of the variables appear to be highly (positively or negatively) correlated with some relationships being non-linear. Suppose that we wish to predict gas mileage (mpg) on the basis of the other variables. Do your plots suggest that any of the other variables might be useful in predicting mpg? Justify your answer. Yes, since other variables are correlated. However, horsepower, weight and displacement are highly related. 2.2.3 Question 10 This exercise involves the Boston housing data set. To begin, load in the Boston data set. The Boston data set is part of the ISLR2 library in R. &gt; library(ISLR2) Now the data set is contained in the object Boston. &gt; Boston Read about the data set: &gt; ?Boston How many rows are in this data set? How many columns? What do the rows and columns represent? library(ISLR2) dim(Boston) ## [1] 506 13 Make some pairwise scatterplots of the predictors (columns) in this data set. Describe your findings. library(ggplot2) library(tidyverse) ggplot(Boston, aes(nox, rm)) + geom_point() ggplot(Boston, aes(ptratio, rm)) + geom_point() heatmap(cor(Boston, method = &quot;spearman&quot;), cexRow = 1.1, cexCol = 1.1) Are any of the predictors associated with per capita crime rate? If so, explain the relationship. Yes Do any of the census tracts of Boston appear to have particularly high crime rates? Tax rates? Pupil-teacher ratios? Comment on the range of each predictor. Boston |&gt; pivot_longer(cols = 1:13) |&gt; filter(name %in% c(&quot;crim&quot;, &quot;tax&quot;, &quot;ptratio&quot;)) |&gt; ggplot(aes(value)) + geom_histogram(bins = 20) + facet_wrap(~name, scales=&quot;free&quot;, ncol= 1) Yes, particularly crime and tax rates. How many of the census tracts in this data set bound the Charles river? sum(Boston$chas) ## [1] 35 What is the median pupil-teacher ratio among the towns in this data set? median(Boston$ptratio) ## [1] 19.05 Which census tract of Boston has lowest median value of owner-occupied homes? What are the values of the other predictors for that census tract, and how do those values compare to the overall ranges for those predictors? Comment on your findings. Boston[Boston$medv == min(Boston$medv), ] |&gt; kable() crim zn indus chas nox rm age dis rad tax ptratio lstat medv 399 38.3518 0 18.1 0 0.693 5.453 100 1.4896 24 666 20.2 30.59 5 406 67.9208 0 18.1 0 0.693 5.683 100 1.4254 24 666 20.2 22.98 5 sapply(Boston, quantile) |&gt; kable() crim zn indus chas nox rm age dis rad tax ptratio lstat medv 0% 0.006320 0.0 0.46 0 0.385 3.5610 2.900 1.129600 1 187 12.60 1.730 5.000 25% 0.082045 0.0 5.19 0 0.449 5.8855 45.025 2.100175 4 279 17.40 6.950 17.025 50% 0.256510 0.0 9.69 0 0.538 6.2085 77.500 3.207450 5 330 19.05 11.360 21.200 75% 3.677083 12.5 18.10 0 0.624 6.6235 94.075 5.188425 24 666 20.20 16.955 25.000 100% 88.976200 100.0 27.74 1 0.871 8.7800 100.000 12.126500 24 711 22.00 37.970 50.000 In this data set, how many of the census tract average more than seven rooms per dwelling? More than eight rooms per dwelling? Comment on the census tracts that average more than eight rooms per dwelling. sum(Boston$rm &gt; 7) ## [1] 64 sum(Boston$rm &gt; 8) ## [1] 13 Let’s compare median statistics for those census tracts with more than eight rooms per dwelling on average, with the statistics for those with fewer. Boston |&gt; mutate( `log(crim)` = log(crim), `log(zn)` = log(zn) ) |&gt; select(-c(crim, zn)) |&gt; pivot_longer(!rm) |&gt; mutate(&quot;&gt;8 rooms&quot; = rm &gt; 8) |&gt; ggplot(aes(`&gt;8 rooms`, value)) + geom_boxplot() + facet_wrap(~name, scales = &quot;free&quot;) ## Warning: Removed 372 rows containing non-finite values (stat_boxplot). Census tracts with big average properties (more than eight rooms per dwelling) have higher median value (medv), a lower proportion of non-retail business acres (indus), a lower pupil-teacher ratio (ptratio), a lower status of the population (lstat) among other differences. "],["linear-regression.html", "3 Linear Regression 3.1 Conceptual 3.2 Applied", " 3 Linear Regression 3.1 Conceptual 3.1.1 Question 1 Describe the null hypotheses to which the p-values given in Table 3.4 correspond. Explain what conclusions you can draw based on these p-values. Your explanation should be phrased in terms of sales, TV, radio, and newspaper, rather than in terms of the coefficients of the linear model. For intercept, that \\(\\beta_0 = 0\\) For the others, that \\(\\beta_n = 0\\) (for \\(n = 1, 2, 3\\)) We can conclude that that without any spending, there are still some sales (the intercept is not 0). Furthermore, we can conclude that money spent on TV and radio are significantly associated with increased sales, but the same cannot be said of newspaper spending. 3.1.2 Question 2 Carefully explain the differences between the KNN classifier and KNN regression methods. The KNN classifier is categorical and assigns a value based on the most frequent observed category among \\(K\\) nearest neighbors, whereas KNN regression assigns a continuous variable, the average of the response variables for the \\(K\\) nearest neighbors. 3.1.3 Question 3 Suppose we have a data set with five predictors, \\(X_1\\) = GPA, \\(X_2\\) = IQ, \\(X_3\\) = Level (1 for College and 0 for High School), \\(X_4\\) = Interaction between GPA and IQ, and \\(X_5\\) = Interaction between GPA and Level. The response is starting salary after graduation (in thousands of dollars). Suppose we use least squares to fit the model, and get \\(\\hat\\beta_0 = 50\\), \\(\\hat\\beta_1 = 20\\), \\(\\hat\\beta_2 = 0.07\\), \\(\\hat\\beta_3 = 35\\), \\(\\hat\\beta_4 = 0.01\\), \\(\\hat\\beta_5 = -10\\). Which answer is correct, and why? For a fixed value of IQ and GPA, high school graduates earn more on average than college graduates. For a fixed value of IQ and GPA, college graduates earn more on average than high school graduates. For a fixed value of IQ and GPA, high school graduates earn more on average than college graduates provided that the GPA is high enough. For a fixed value of IQ and GPA, college graduates earn more on average than high school graduates provided that the GPA is high enough. library(plotly) model &lt;- function(gpa, iq, level) { 50 + gpa * 20 + iq * 0.07 + level * 35 + gpa * iq * 0.01 + gpa * level * -10 } x &lt;- seq(1, 5, length = 10) y &lt;- seq(1, 200, length = 20) college &lt;- t(outer(x, y, model, level = 1)) high_school &lt;- t(outer(x, y, model, level = 0)) plot_ly(x = x, y = y) |&gt; add_surface( z = ~college, colorscale = list(c(0, 1), c(&quot;rgb(107,184,214)&quot;, &quot;rgb(0,90,124)&quot;)), colorbar = list(title = &quot;College&quot;)) |&gt; add_surface( z = ~high_school, colorscale = list(c(0, 1), c(&quot;rgb(255,112,184)&quot;, &quot;rgb(128,0,64)&quot;)), colorbar = list(title = &quot;High school&quot;)) |&gt; layout(scene = list( xaxis = list(title = &quot;GPA&quot;), yaxis = list(title = &quot;IQ&quot;), zaxis = list(title = &quot;Salary&quot;))) Option iii correct. Predict the salary of a college graduate with IQ of 110 and a GPA of 4.0. model(gpa = 4, iq = 110, level = 1) ## [1] 137.1 True or false: Since the coefficient for the GPA/IQ interaction term is very small, there is very little evidence of an interaction effect. Justify your answer. This is false. It is important to remember that GPA and IQ vary over different scales. It is better to explicitly test the significance of the interaction effect, and/or visualize or quantify the effect on sales under realistic ranges of GPA/IQ values. 3.1.4 Question 4 I collect a set of data (\\(n = 100\\) observations) containing a single predictor and a quantitative response. I then fit a linear regression model to the data, as well as a separate cubic regression, i.e. \\(Y = \\beta_0 + \\beta_1X + \\beta_2X^2 + \\beta_3X^3 + \\epsilon\\). Suppose that the true relationship between \\(X\\) and \\(Y\\) is linear, i.e. \\(Y = \\beta_0 + \\beta_1X + \\epsilon\\). Consider the training residual sum of squares (RSS) for the linear regression, and also the training RSS for the cubic regression. Would we expect one to be lower than the other, would we expect them to be the same, or is there not enough information to tell? Justify your answer. You would expect the cubic regression to have lower RSS since it is at least as flexible as the linear regression. Answer (a) using test rather than training RSS. Though we could not be certain, the test RSS would likely be higher due to overfitting. Suppose that the true relationship between \\(X\\) and \\(Y\\) is not linear, but we don’t know how far it is from linear. Consider the training RSS for the linear regression, and also the training RSS for the cubic regression. Would we expect one to be lower than the other, would we expect them to be the same, or is there not enough information to tell? Justify your answer. You would expect the cubic regression to have lower RSS since it is at least as flexible as the linear regression. Answer (c) using test rather than training RSS. There is not enough information to tell, it depends on how non-linear the true relationship is. 3.1.5 Question 5 Consider the fitted values that result from performing linear regression without an intercept. In this setting, the ith fitted value takes the form \\[\\hat{y}_i = x_i\\hat\\beta,\\] where \\[\\hat{\\beta} = \\left(\\sum_{i=1}^nx_iy_i\\right) / \\left(\\sum_{i&#39; = 1}^n x^2_{i&#39;}\\right).\\] show that we can write \\[\\hat{y}_i = \\sum_{i&#39; = 1}^na_{i&#39;}y_{i&#39;}\\] What is \\(a_{i&#39;}\\)? Note: We interpret this result by saying that the fitted values from linear regression are linear combinations of the response values. \\[\\begin{align} \\hat{y}_i &amp; = x_i \\frac{\\sum_{i=1}^nx_iy_i}{\\sum_{i&#39; = 1}^n x^2_{i&#39;}} \\\\ &amp; = x_i \\frac{\\sum_{i&#39;=1}^nx_{i&#39;}y_{i&#39;}}{\\sum_{i&#39;&#39; = 1}^n x^2_{i&#39;&#39;}} \\\\ &amp; = \\frac{\\sum_{i&#39;=1}^n x_i x_{i&#39;}y_{i&#39;}}{\\sum_{i&#39;&#39; = 1}^n x^2_{i&#39;&#39;}} \\\\ &amp; = \\sum_{i&#39;=1}^n \\frac{ x_i x_{i&#39;}y_{i&#39;}}{\\sum_{i&#39;&#39; = 1}^n x^2_{i&#39;&#39;}} \\\\ &amp; = \\sum_{i&#39;=1}^n \\frac{ x_i x_{i&#39;}}{\\sum_{i&#39;&#39; = 1}^n x^2_{i&#39;&#39;}} y_{i&#39;} \\end{align}\\] therefore, \\[a_{i&#39;} = \\frac{ x_i x_{i&#39;}}{\\sum x^2}\\] 3.1.6 Question 6 Using (3.4), argue that in the case of simple linear regression, the least squares line always passes through the point \\((\\bar{x}, \\bar{y})\\). when \\(x = \\bar{x}\\) what is \\(y\\)? \\[\\begin{align} y &amp;= \\hat\\beta_0 + \\hat\\beta_1\\bar{x} \\\\ &amp;= \\bar{y} - \\hat\\beta_1\\bar{x} + \\hat\\beta_1\\bar{x} \\\\ &amp;= \\bar{y} \\end{align}\\] 3.1.7 Question 7 It is claimed in the text that in the case of simple linear regression of \\(Y\\) onto \\(X\\), the \\(R^2\\) statistic (3.17) is equal to the square of the correlation between \\(X\\) and \\(Y\\) (3.18). Prove that this is the case. For simplicity, you may assume that \\(\\bar{x} = \\bar{y} = 0\\). We have the following equations: \\[ R^2 = \\frac{\\textit{TSS} - \\textit{RSS}}{\\textit{TSS}} \\] \\[ Cor(x,y) = \\frac{\\sum_i (x_i-\\bar{x})(y_i - \\bar{y})}{\\sqrt{\\sum_i(x_i - \\bar{x})^2}\\sqrt{\\sum_i(y_i - \\bar{y})^2}} \\] As above, its important to remember \\(\\sum_i x_i = \\sum_j x_j\\) when \\(\\bar{x} = \\bar{y} = 0\\) \\[ Cor(x,y)^2 = \\frac{(\\sum_ix_iy_i)^2}{\\sum_ix_i^2 \\sum_iy_i^2} \\] Also note that: \\[\\hat{y}_i = \\hat\\beta_o + \\hat\\beta_1x_i = x_i\\frac{\\sum_j{x_jy_j}}{\\sum_jx_j^2}\\] Therefore, given that \\(RSS = \\sum_i(y_i - \\hat{y}_i)^2\\) and \\(\\textit{TSS} = \\sum_i(y_i - \\bar{y})^2 = \\sum_iy_i^2\\) \\[\\begin{align} R^2 &amp;= \\frac{\\sum_iy_i^2 - \\sum_i(y_i - x_i\\frac{\\sum_j{x_jy_j}}{\\sum_jx_j^2})^2} {\\sum_iy_i^2} \\\\ &amp;= \\frac{\\sum_iy_i^2 - \\sum_i( y_i^2 - 2y_ix_i\\frac{\\sum_j{x_jy_j}}{\\sum_jx_j^2} + x_i^2 (\\frac{\\sum_j{x_jy_j}}{\\sum_jx_j^2})^2 )}{\\sum_iy_i^2} \\\\ &amp;= \\frac{ 2\\sum_i(y_ix_i\\frac{\\sum_j{x_jy_j}}{\\sum_jx_j^2}) - \\sum_i(x_i^2 (\\frac{\\sum_j{x_jy_j}}{\\sum_jx_j^2})^2) }{\\sum_iy_i^2} \\\\ &amp;= \\frac{ 2\\sum_i(y_ix_i) \\frac{\\sum_j{x_jy_j}}{\\sum_jx_j^2} - \\sum_i(x_i^2) \\frac{(\\sum_j{x_jy_j})^2}{(\\sum_jx_j^2)^2} }{\\sum_iy_i^2} \\\\ &amp;= \\frac{ 2\\frac{(\\sum_i{x_iy_i})^2}{\\sum_jx_j^2} - \\frac{(\\sum_i{x_iy_i})^2}{\\sum_jx_j^2} }{\\sum_iy_i^2} \\\\ &amp;= \\frac{(\\sum_i{x_iy_i})^2}{\\sum_ix_i^2 \\sum_iy_i^2} \\end{align}\\] 3.2 Applied 3.2.1 Question 8 This question involves the use of simple linear regression on the Auto data set. Use the lm() function to perform a simple linear regression with mpg as the response and horsepower as the predictor. Use the summary() function to print the results. Comment on the output. For example: Is there a relationship between the predictor and the response? How strong is the relationship between the predictor and the response? Is the relationship between the predictor and the response positive or negative? What is the predicted mpg associated with a horsepower of 98? What are the associated 95% confidence and prediction intervals? library(ISLR2) fit &lt;- lm(mpg ~ horsepower, data = Auto) summary(fit) ## ## Call: ## lm(formula = mpg ~ horsepower, data = Auto) ## ## Residuals: ## Min 1Q Median 3Q Max ## -13.5710 -3.2592 -0.3435 2.7630 16.9240 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 39.935861 0.717499 55.66 &lt;2e-16 *** ## horsepower -0.157845 0.006446 -24.49 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 4.906 on 390 degrees of freedom ## Multiple R-squared: 0.6059, Adjusted R-squared: 0.6049 ## F-statistic: 599.7 on 1 and 390 DF, p-value: &lt; 2.2e-16 Yes, there is a significant relationship between predictor and response. For every unit increase in horsepower, mpg reduces by 0.16 (a negative relationship). predict(fit, data.frame(horsepower = 98), interval = &quot;confidence&quot;) ## fit lwr upr ## 1 24.46708 23.97308 24.96108 predict(fit, data.frame(horsepower = 98), interval = &quot;prediction&quot;) ## fit lwr upr ## 1 24.46708 14.8094 34.12476 Plot the response and the predictor. Use the abline() function to display the least squares regression line. plot(Auto$horsepower, Auto$mpg, xlab = &quot;horsepower&quot;, ylab = &quot;mpg&quot;) abline(fit) Use the plot() function to produce diagnostic plots of the least squares regression fit. Comment on any problems you see with the fit. par(mfrow = c(2, 2)) plot(fit, cex = 0.2) The residuals show a trend with respect to the fitted values suggesting a non-linear relationship. 3.2.2 Question 9 This question involves the use of multiple linear regression on the Auto data set. Produce a scatterplot matrix which includes all of the variables in the data set. pairs(Auto, cex = 0.2) Compute the matrix of correlations between the variables using the function cor(). You will need to exclude the name variable, name which is qualitative. x &lt;- subset(Auto, select = -name) cor(x) ## mpg cylinders displacement horsepower weight ## mpg 1.0000000 -0.7776175 -0.8051269 -0.7784268 -0.8322442 ## cylinders -0.7776175 1.0000000 0.9508233 0.8429834 0.8975273 ## displacement -0.8051269 0.9508233 1.0000000 0.8972570 0.9329944 ## horsepower -0.7784268 0.8429834 0.8972570 1.0000000 0.8645377 ## weight -0.8322442 0.8975273 0.9329944 0.8645377 1.0000000 ## acceleration 0.4233285 -0.5046834 -0.5438005 -0.6891955 -0.4168392 ## year 0.5805410 -0.3456474 -0.3698552 -0.4163615 -0.3091199 ## origin 0.5652088 -0.5689316 -0.6145351 -0.4551715 -0.5850054 ## acceleration year origin ## mpg 0.4233285 0.5805410 0.5652088 ## cylinders -0.5046834 -0.3456474 -0.5689316 ## displacement -0.5438005 -0.3698552 -0.6145351 ## horsepower -0.6891955 -0.4163615 -0.4551715 ## weight -0.4168392 -0.3091199 -0.5850054 ## acceleration 1.0000000 0.2903161 0.2127458 ## year 0.2903161 1.0000000 0.1815277 ## origin 0.2127458 0.1815277 1.0000000 Use the lm() function to perform a multiple linear regression with mpg as the response and all other variables except name as the predictors. Use the summary() function to print the results. Comment on the output. For instance: Is there a relationship between the predictors and the response? Which predictors appear to have a statistically significant relationship to the response? What does the coefficient for the year variable suggest? fit &lt;- lm(mpg ~ ., data = x) summary(fit) ## ## Call: ## lm(formula = mpg ~ ., data = x) ## ## Residuals: ## Min 1Q Median 3Q Max ## -9.5903 -2.1565 -0.1169 1.8690 13.0604 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -17.218435 4.644294 -3.707 0.00024 *** ## cylinders -0.493376 0.323282 -1.526 0.12780 ## displacement 0.019896 0.007515 2.647 0.00844 ** ## horsepower -0.016951 0.013787 -1.230 0.21963 ## weight -0.006474 0.000652 -9.929 &lt; 2e-16 *** ## acceleration 0.080576 0.098845 0.815 0.41548 ## year 0.750773 0.050973 14.729 &lt; 2e-16 *** ## origin 1.426141 0.278136 5.127 4.67e-07 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 3.328 on 384 degrees of freedom ## Multiple R-squared: 0.8215, Adjusted R-squared: 0.8182 ## F-statistic: 252.4 on 7 and 384 DF, p-value: &lt; 2.2e-16 Yes, there is a relationship between some predictors and response, notably “displacement” (positive), “weight” (negative), “year” (positive) and “origin” (positive). The coefficient for year (which is positive \\(~0.75\\)) suggests that mpg increases by about this amount every year on average. Use the plot() function to produce diagnostic plots of the linear regression fit. Comment on any problems you see with the fit. Do the residual plots suggest any unusually large outliers? Does the leverage plot identify any observations with unusually high leverage? par(mfrow = c(2, 2)) plot(fit, cex = 0.2) One point has high leverage, the residuals also show a trend with fitted values. Use the * and : symbols to fit linear regression models with interaction effects. Do any interactions appear to be statistically significant? summary(lm(mpg ~ . + weight:horsepower, data = x)) ## ## Call: ## lm(formula = mpg ~ . + weight:horsepower, data = x) ## ## Residuals: ## Min 1Q Median 3Q Max ## -8.589 -1.617 -0.184 1.541 12.001 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 2.876e+00 4.511e+00 0.638 0.524147 ## cylinders -2.955e-02 2.881e-01 -0.103 0.918363 ## displacement 5.950e-03 6.750e-03 0.881 0.378610 ## horsepower -2.313e-01 2.363e-02 -9.791 &lt; 2e-16 *** ## weight -1.121e-02 7.285e-04 -15.393 &lt; 2e-16 *** ## acceleration -9.019e-02 8.855e-02 -1.019 0.309081 ## year 7.695e-01 4.494e-02 17.124 &lt; 2e-16 *** ## origin 8.344e-01 2.513e-01 3.320 0.000986 *** ## horsepower:weight 5.529e-05 5.227e-06 10.577 &lt; 2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 2.931 on 383 degrees of freedom ## Multiple R-squared: 0.8618, Adjusted R-squared: 0.859 ## F-statistic: 298.6 on 8 and 383 DF, p-value: &lt; 2.2e-16 summary(lm(mpg ~ . + acceleration:horsepower, data = x)) ## ## Call: ## lm(formula = mpg ~ . + acceleration:horsepower, data = x) ## ## Residuals: ## Min 1Q Median 3Q Max ## -9.0329 -1.8177 -0.1183 1.7247 12.4870 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -32.499820 4.923380 -6.601 1.36e-10 *** ## cylinders 0.083489 0.316913 0.263 0.792350 ## displacement -0.007649 0.008161 -0.937 0.349244 ## horsepower 0.127188 0.024746 5.140 4.40e-07 *** ## weight -0.003976 0.000716 -5.552 5.27e-08 *** ## acceleration 0.983282 0.161513 6.088 2.78e-09 *** ## year 0.755919 0.048179 15.690 &lt; 2e-16 *** ## origin 1.035733 0.268962 3.851 0.000138 *** ## horsepower:acceleration -0.012139 0.001772 -6.851 2.93e-11 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 3.145 on 383 degrees of freedom ## Multiple R-squared: 0.841, Adjusted R-squared: 0.8376 ## F-statistic: 253.2 on 8 and 383 DF, p-value: &lt; 2.2e-16 summary(lm(mpg ~ . + cylinders:weight, data = x)) ## ## Call: ## lm(formula = mpg ~ . + cylinders:weight, data = x) ## ## Residuals: ## Min 1Q Median 3Q Max ## -10.9484 -1.7133 -0.1809 1.4530 12.4137 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 7.3143478 5.0076737 1.461 0.14494 ## cylinders -5.0347425 0.5795767 -8.687 &lt; 2e-16 *** ## displacement 0.0156444 0.0068409 2.287 0.02275 * ## horsepower -0.0314213 0.0126216 -2.489 0.01322 * ## weight -0.0150329 0.0011125 -13.513 &lt; 2e-16 *** ## acceleration 0.1006438 0.0897944 1.121 0.26306 ## year 0.7813453 0.0464139 16.834 &lt; 2e-16 *** ## origin 0.8030154 0.2617333 3.068 0.00231 ** ## cylinders:weight 0.0015058 0.0001657 9.088 &lt; 2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 3.022 on 383 degrees of freedom ## Multiple R-squared: 0.8531, Adjusted R-squared: 0.8501 ## F-statistic: 278.1 on 8 and 383 DF, p-value: &lt; 2.2e-16 There are at least three cases where the interactions appear to be highly significant. Try a few different transformations of the variables, such as \\(log(X)\\), \\(\\sqrt{X}\\), \\(X^2\\). Comment on your findings. Here I’ll just consider transformations for horsepower. par(mfrow = c(2, 2)) plot(Auto$horsepower, Auto$mpg, cex = 0.2) plot(log(Auto$horsepower), Auto$mpg, cex = 0.2) plot(sqrt(Auto$horsepower), Auto$mpg, cex = 0.2) plot(Auto$horsepower ^ 2, Auto$mpg, cex = 0.2) x &lt;- subset(Auto, select = -name) x$horsepower &lt;- log(x$horsepower) fit &lt;- lm(mpg ~ ., data = x) summary(fit) ## ## Call: ## lm(formula = mpg ~ ., data = x) ## ## Residuals: ## Min 1Q Median 3Q Max ## -9.3115 -2.0041 -0.1726 1.8393 12.6579 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 27.254005 8.589614 3.173 0.00163 ** ## cylinders -0.486206 0.306692 -1.585 0.11372 ## displacement 0.019456 0.006876 2.830 0.00491 ** ## horsepower -9.506436 1.539619 -6.175 1.69e-09 *** ## weight -0.004266 0.000694 -6.148 1.97e-09 *** ## acceleration -0.292088 0.103804 -2.814 0.00515 ** ## year 0.705329 0.048456 14.556 &lt; 2e-16 *** ## origin 1.482435 0.259347 5.716 2.19e-08 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 3.18 on 384 degrees of freedom ## Multiple R-squared: 0.837, Adjusted R-squared: 0.834 ## F-statistic: 281.6 on 7 and 384 DF, p-value: &lt; 2.2e-16 par(mfrow = c(2, 2)) plot(fit, cex = 0.2) A log transformation of horsepower appears to give a more linear relationship with mpg. 3.2.3 Question 10 This question should be answered using the Carseats data set. Fit a multiple regression model to predict Sales using Price, Urban, and US. fit &lt;- lm(Sales ~ Price + Urban + US, data = Carseats) Provide an interpretation of each coefficient in the model. Be careful—some of the variables in the model are qualitative! summary(fit) ## ## Call: ## lm(formula = Sales ~ Price + Urban + US, data = Carseats) ## ## Residuals: ## Min 1Q Median 3Q Max ## -6.9206 -1.6220 -0.0564 1.5786 7.0581 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 13.043469 0.651012 20.036 &lt; 2e-16 *** ## Price -0.054459 0.005242 -10.389 &lt; 2e-16 *** ## UrbanYes -0.021916 0.271650 -0.081 0.936 ## USYes 1.200573 0.259042 4.635 4.86e-06 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 2.472 on 396 degrees of freedom ## Multiple R-squared: 0.2393, Adjusted R-squared: 0.2335 ## F-statistic: 41.52 on 3 and 396 DF, p-value: &lt; 2.2e-16 Write out the model in equation form, being careful to handle the qualitative variables properly. \\[ \\textit{Sales} = 13 + -0.054 \\times \\textit{Price} + \\begin{cases} -0.022, &amp; \\text{if $\\textit{Urban}$ is Yes, $\\textit{US}$ is No} \\\\ 1.20, &amp; \\text{if $\\textit{Urban}$ is No, $\\textit{US}$ is Yes} \\\\ 1.18, &amp; \\text{if $\\textit{Urban}$ and $\\textit{US}$ is Yes} \\\\ 0, &amp; \\text{Otherwise} \\end{cases} \\] For which of the predictors can you reject the null hypothesis \\(H_0 : \\beta_j = 0\\)? Price and US (Urban shows no significant difference between “No” and “Yes”) On the basis of your response to the previous question, fit a smaller model that only uses the predictors for which there is evidence of association with the outcome. fit2 &lt;- lm(Sales ~ Price + US, data = Carseats) How well do the models in (a) and (e) fit the data? summary(fit) ## ## Call: ## lm(formula = Sales ~ Price + Urban + US, data = Carseats) ## ## Residuals: ## Min 1Q Median 3Q Max ## -6.9206 -1.6220 -0.0564 1.5786 7.0581 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 13.043469 0.651012 20.036 &lt; 2e-16 *** ## Price -0.054459 0.005242 -10.389 &lt; 2e-16 *** ## UrbanYes -0.021916 0.271650 -0.081 0.936 ## USYes 1.200573 0.259042 4.635 4.86e-06 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 2.472 on 396 degrees of freedom ## Multiple R-squared: 0.2393, Adjusted R-squared: 0.2335 ## F-statistic: 41.52 on 3 and 396 DF, p-value: &lt; 2.2e-16 summary(fit2) ## ## Call: ## lm(formula = Sales ~ Price + US, data = Carseats) ## ## Residuals: ## Min 1Q Median 3Q Max ## -6.9269 -1.6286 -0.0574 1.5766 7.0515 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 13.03079 0.63098 20.652 &lt; 2e-16 *** ## Price -0.05448 0.00523 -10.416 &lt; 2e-16 *** ## USYes 1.19964 0.25846 4.641 4.71e-06 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 2.469 on 397 degrees of freedom ## Multiple R-squared: 0.2393, Adjusted R-squared: 0.2354 ## F-statistic: 62.43 on 2 and 397 DF, p-value: &lt; 2.2e-16 anova(fit, fit2) ## Analysis of Variance Table ## ## Model 1: Sales ~ Price + Urban + US ## Model 2: Sales ~ Price + US ## Res.Df RSS Df Sum of Sq F Pr(&gt;F) ## 1 396 2420.8 ## 2 397 2420.9 -1 -0.03979 0.0065 0.9357 They have similar \\(R^2\\) and the model containing the extra variable “Urban” is non-significantly better. Using the model from (e), obtain 95% confidence intervals for the coefficient(s). confint(fit2) ## 2.5 % 97.5 % ## (Intercept) 11.79032020 14.27126531 ## Price -0.06475984 -0.04419543 ## USYes 0.69151957 1.70776632 Is there evidence of outliers or high leverage observations in the model from (e)? par(mfrow = c(2, 2)) plot(fit2, cex = 0.2) Yes, somewhat. 3.2.4 Question 11 In this problem we will investigate the t-statistic for the null hypothesis \\(H_0 : \\beta = 0\\) in simple linear regression without an intercept. To begin, we generate a predictor x and a response y as follows. set.seed(1) x &lt;- rnorm(100) y &lt;- 2 * x + rnorm(100) set.seed(1) x &lt;- rnorm(100) y &lt;- 2 * x + rnorm(100) Perform a simple linear regression of y onto x, without an intercept. Report the coefficient estimate \\(\\hat{\\beta}\\), the standard error of this coefficient estimate, and the t-statistic and p-value associated with the null hypothesis \\(H_0 : \\beta = 0\\). Comment on these results. (You can perform regression without an intercept using the command lm(y~x+0).) fit &lt;- lm(y ~ x + 0) coef(summary(fit)) ## Estimate Std. Error t value Pr(&gt;|t|) ## x 1.993876 0.1064767 18.72593 2.642197e-34 There’s a significant positive relationship between \\(y\\) and \\(x\\). \\(y\\) values are predicted to be (a little below) twice the \\(x\\) values. Now perform a simple linear regression of x onto y without an intercept, and report the coefficient estimate, its standard error, and the corresponding t-statistic and p-values associated with the null hypothesis \\(H_0 : \\beta = 0\\). Comment on these results. fit &lt;- lm(x ~ y + 0) coef(summary(fit)) ## Estimate Std. Error t value Pr(&gt;|t|) ## y 0.3911145 0.02088625 18.72593 2.642197e-34 There’s a significant positive relationship between \\(x\\) and \\(y\\). \\(x\\) values are predicted to be (a little below) half the \\(y\\) values. What is the relationship between the results obtained in (a) and (b)? Without error, the coefficients would be the inverse of each other (2 and 1/2). The t-statistic and p-values are the same. For the regression of \\(Y\\) onto \\(X\\) without an intercept, the t-statistic for \\(H_0 : \\beta = 0\\) takes the form \\(\\hat{\\beta}/SE(\\hat{\\beta})\\), where \\(\\hat{\\beta}\\) is given by (3.38), and where \\[ SE(\\hat\\beta) = \\sqrt{\\frac{\\sum_{i=1}^n(y_i - x_i\\hat\\beta)^2}{(n-1)\\sum_{i&#39;=1}^nx_{i&#39;}^2}}. \\] (These formulas are slightly different from those given in Sections 3.1.1 and 3.1.2, since here we are performing regression without an intercept.) Show algebraically, and confirm numerically in R, that the t-statistic can be written as \\[ \\frac{(\\sqrt{n-1}) \\sum_{i-1}^nx_iy_i)} {\\sqrt{(\\sum_{i=1}^nx_i^2)(\\sum_{i&#39;=1}^ny_{i&#39;}^2)-(\\sum_{i&#39;=1}^nx_{i&#39;}y_{i&#39;})^2}} \\] \\[ \\beta = \\sum_i x_i y_i / \\sum_{i&#39;} x_{i&#39;}^2 ,\\] therefore \\[\\begin{align} t &amp;= \\frac{\\sum_i x_i y_i \\sqrt{n-1} \\sqrt{\\sum_ix_i^2}} {\\sum_i x_i^2 \\sqrt{\\sum_i(y_i - x_i \\beta)^2}} \\\\ &amp;= \\frac{\\sum_i x_i y_i \\sqrt{n-1}} {\\sqrt{\\sum_ix_i^2 \\sum_i(y_i - x_i \\beta)^2}} \\\\ &amp;= \\frac{\\sum_i x_i y_i \\sqrt{n-1}} {\\sqrt{ \\sum_ix_i^2 \\sum_i(y_i^2 - 2 y_i x_i \\beta + x_i^2 \\beta^2) }} \\\\ &amp;= \\frac{\\sum_i x_i y_i \\sqrt{n-1}} {\\sqrt{ \\sum_ix_i^2 \\sum_iy_i^2 - \\beta \\sum_ix_i^2 (2 \\sum_i y_i x_i -\\beta \\sum_i x_i^2) }} \\\\ &amp;= \\frac{\\sum_i x_i y_i \\sqrt{n-1}} {\\sqrt{ \\sum_ix_i^2 \\sum_iy_i^2 - \\sum_i x_i y_i (2 \\sum_i y_i x_i - \\sum_i x_i y_i) }} \\\\ &amp;= \\frac{\\sum_i x_i y_i \\sqrt{n-1}} {\\sqrt{\\sum_ix_i^2 \\sum_iy_i^2 - (\\sum_i x_i y_i)^2}} \\\\ \\end{align}\\] We can show this numerically in R by computing \\(t\\) using the above equation. n &lt;- length(x) sqrt(n - 1) * sum(x * y) / sqrt(sum(x ^ 2) * sum(y ^ 2) - sum(x * y) ^ 2) ## [1] 18.72593 Using the results from (d), argue that the t-statistic for the regression of y onto x is the same as the t-statistic for the regression of x onto y. Swapping \\(x_i\\) for \\(y_i\\) in the formula for \\(t\\) will give the same result. In R, show that when regression is performed with an intercept, the t-statistic for \\(H_0 : \\beta_1 = 0\\) is the same for the regression of y onto x as it is for the regression of x onto y. coef(summary(lm(y ~ x))) ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -0.03769261 0.09698729 -0.3886346 6.983896e-01 ## x 1.99893961 0.10772703 18.5555993 7.723851e-34 coef(summary(lm(x ~ y))) ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 0.03880394 0.04266144 0.9095787 3.652764e-01 ## y 0.38942451 0.02098690 18.5555993 7.723851e-34 3.2.5 Question 12 This problem involves simple linear regression without an intercept. Recall that the coefficient estimate \\(\\hat{\\beta}\\) for the linear regression of \\(Y\\) onto \\(X\\) without an intercept is given by (3.38). Under what circumstance is the coefficient estimate for the regression of \\(X\\) onto \\(Y\\) the same as the coefficient estimate for the regression of \\(Y\\) onto \\(X\\)? \\[ \\hat\\beta = \\sum_i x_iy_i / \\sum_{i&#39;} x_{i&#39;}^2 \\] The coefficient for the regression of X onto Y swaps the \\(x\\) and \\(y\\) variables: \\[ \\hat\\beta = \\sum_i x_iy_i / \\sum_{i&#39;} y_{i&#39;}^2 \\] So they are the same when \\(\\sum_{i} x_{i}^2 = \\sum_{i} y_{i}^2\\) Generate an example in R with \\(n = 100\\) observations in which the coefficient estimate for the regression of \\(X\\) onto \\(Y\\) is different from the coefficient estimate for the regression of \\(Y\\) onto \\(X\\). x &lt;- rnorm(100) y &lt;- 2 * x + rnorm(100, 0, 0.1) c(sum(x^2), sum(y^2)) ## [1] 105.9889 429.4924 c(coef(lm(y ~ x))[2], coef(lm(x ~ y))[2]) ## x y ## 2.0106218 0.4962439 Generate an example in R with \\(n = 100\\) observations in which the coefficient estimate for the regression of \\(X\\) onto \\(Y\\) is the same as the coefficient estimate for the regression of \\(Y\\) onto \\(X\\). x &lt;- rnorm(100) y &lt;- x + rnorm(100, 0, 0.1) c(sum(x^2), sum(y^2)) ## [1] 135.5844 134.5153 c(coef(lm(y ~ x))[2], coef(lm(x ~ y))[2]) ## x y ## 0.9925051 1.0006765 3.2.6 Question 13 In this exercise you will create some simulated data and will fit simple linear regression models to it. Make sure to use set.seed(1) prior to starting part (a) to ensure consistent results. set.seed(1) Using the rnorm() function, create a vector, x, containing 100 observations drawn from a \\(N(0, 1)\\) distribution. This represents a feature, \\(X\\). x &lt;- rnorm(100, 0, 1) Using the rnorm() function, create a vector, eps, containing 100 observations drawn from a \\(N(0, 0.25)\\) distribution—a normal distribution with mean zero and variance 0.25. eps &lt;- rnorm(100, 0, sqrt(0.25)) Using x and eps, generate a vector y according to the model \\[Y = -1 + 0.5X + \\epsilon\\] What is the length of the vector y? What are the values of \\(\\beta_0\\) and \\(\\beta_1\\) in this linear model? y &lt;- -1 + 0.5 * x + eps length(y) ## [1] 100 \\(\\beta_0 = -1\\) and \\(\\beta_1 = 0.5\\) Create a scatterplot displaying the relationship between x and y. Comment on what you observe. plot(x, y) There is a linear relationship between \\(x\\) and \\(y\\) (with some error). Fit a least squares linear model to predict y using x. Comment on the model obtained. How do \\(\\hat\\beta_0\\) and \\(\\hat\\beta_1\\) compare to \\(\\beta_0\\) and \\(\\beta_1\\)? fit &lt;- lm(y ~ x) summary(fit) ## ## Call: ## lm(formula = y ~ x) ## ## Residuals: ## Min 1Q Median 3Q Max ## -0.93842 -0.30688 -0.06975 0.26970 1.17309 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -1.01885 0.04849 -21.010 &lt; 2e-16 *** ## x 0.49947 0.05386 9.273 4.58e-15 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.4814 on 98 degrees of freedom ## Multiple R-squared: 0.4674, Adjusted R-squared: 0.4619 ## F-statistic: 85.99 on 1 and 98 DF, p-value: 4.583e-15 \\(\\beta_0\\) and \\(\\beta_1\\) are close to their population values. Display the least squares line on the scatterplot obtained in (d). Draw the population regression line on the plot, in a different color. Use the legend() command to create an appropriate legend. plot(x, y) abline(fit) abline(-1, 0.5, col = &quot;red&quot;, lty = 2) legend(&quot;topleft&quot;, c(&quot;model fit&quot;, &quot;population regression&quot;), col = c(&quot;black&quot;, &quot;red&quot;), lty = c(1, 2) ) Now fit a polynomial regression model that predicts y using x and x^2. Is there evidence that the quadratic term improves the model fit? Explain your answer. fit2 &lt;- lm(y ~ poly(x, 2)) anova(fit2, fit) ## Analysis of Variance Table ## ## Model 1: y ~ poly(x, 2) ## Model 2: y ~ x ## Res.Df RSS Df Sum of Sq F Pr(&gt;F) ## 1 97 22.257 ## 2 98 22.709 -1 -0.45163 1.9682 0.1638 There is no evidence for an improved fit, since the F-test is non-significant. Repeat (a)–(f) after modifying the data generation process in such a way that there is less noise in the data. The model (3.39) should remain the same. You can do this by decreasing the variance of the normal distribution used to generate the error term \\(\\epsilon\\) in (b). Describe your results. x &lt;- rnorm(100, 0, 1) y &lt;- -1 + 0.5 * x + rnorm(100, 0, sqrt(0.05)) fit2 &lt;- lm(y ~ x) summary(fit2) ## ## Call: ## lm(formula = y ~ x) ## ## Residuals: ## Min 1Q Median 3Q Max ## -0.61308 -0.12553 -0.00391 0.15199 0.41332 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -0.98917 0.02216 -44.64 &lt;2e-16 *** ## x 0.52375 0.02152 24.33 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.2215 on 98 degrees of freedom ## Multiple R-squared: 0.858, Adjusted R-squared: 0.8565 ## F-statistic: 592.1 on 1 and 98 DF, p-value: &lt; 2.2e-16 plot(x, y) abline(fit2) abline(-1, 0.5, col = &quot;red&quot;, lty = 2) legend(&quot;topleft&quot;, c(&quot;model fit&quot;, &quot;population regression&quot;), col = c(&quot;black&quot;, &quot;red&quot;), lty = c(1, 2) ) The data shows less variability and the \\(R^2\\) is higher. Repeat (a)–(f) after modifying the data generation process in such a way that there is more noise in the data. The model (3.39) should remain the same. You can do this by increasing the variance of the normal distribution used to generate the error term \\(\\epsilon\\) in (b). Describe your results. x &lt;- rnorm(100, 0, 1) y &lt;- -1 + 0.5 * x + rnorm(100, 0, 1) fit3 &lt;- lm(y ~ x) summary(fit3) ## ## Call: ## lm(formula = y ~ x) ## ## Residuals: ## Min 1Q Median 3Q Max ## -2.51014 -0.60549 0.02065 0.70483 2.08980 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -1.04745 0.09676 -10.825 &lt; 2e-16 *** ## x 0.42505 0.08310 5.115 1.56e-06 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.9671 on 98 degrees of freedom ## Multiple R-squared: 0.2107, Adjusted R-squared: 0.2027 ## F-statistic: 26.16 on 1 and 98 DF, p-value: 1.56e-06 plot(x, y) abline(fit3) abline(-1, 0.5, col = &quot;red&quot;, lty = 2) legend(&quot;topleft&quot;, c(&quot;model fit&quot;, &quot;population regression&quot;), col = c(&quot;black&quot;, &quot;red&quot;), lty = c(1, 2) ) The data shows more variability. The \\(R^2\\) is lower. What are the confidence intervals for \\(\\beta_0\\) and \\(\\beta_1\\) based on the original data set, the noisier data set, and the less noisy data set? Comment on your results. confint(fit) ## 2.5 % 97.5 % ## (Intercept) -1.1150804 -0.9226122 ## x 0.3925794 0.6063602 confint(fit2) ## 2.5 % 97.5 % ## (Intercept) -1.033141 -0.9451916 ## x 0.481037 0.5664653 confint(fit3) ## 2.5 % 97.5 % ## (Intercept) -1.2394772 -0.8554276 ## x 0.2601391 0.5899632 The confidence intervals for the coefficients are smaller when there is less error. 3.2.7 Question 14 This problem focuses on the collinearity problem. Perform the following commands in R : &gt; set.seed(1) &gt; x1 &lt;- runif(100) &gt; x2 &lt;- 0.5 * x1 + rnorm(100) / 10 &gt; y &lt;- 2 + 2 * x1 + 0.3 * x2 + rnorm(100) The last line corresponds to creating a linear model in which y is a function of x1 and x2. Write out the form of the linear model. What are the regression coefficients? set.seed(1) x1 &lt;- runif(100) x2 &lt;- 0.5 * x1 + rnorm(100) / 10 y &lt;- 2 + 2 * x1 + 0.3 * x2 + rnorm(100) The model is of the form: \\[Y = \\beta_0 + \\beta_1X_1 + \\beta_2X_2 + \\epsilon\\] The coefficients are \\(\\beta_0 = 2\\), \\(\\beta_1 = 2\\), \\(\\beta_3 = 0.3\\). What is the correlation between x1 and x2? Create a scatterplot displaying the relationship between the variables. cor(x1, x2) ## [1] 0.8351212 plot(x1, x2) Using this data, fit a least squares regression to predict y using x1 and x2. Describe the results obtained. What are \\(\\hat\\beta_0\\), \\(\\hat\\beta_1\\), and \\(\\hat\\beta_2\\)? How do these relate to the true \\(\\beta_0\\), \\(\\beta_1\\), and _2$? Can you reject the null hypothesis \\(H_0 : \\beta_1\\) = 0$? How about the null hypothesis \\(H_0 : \\beta_2 = 0\\)? summary(lm(y ~ x1 + x2)) ## ## Call: ## lm(formula = y ~ x1 + x2) ## ## Residuals: ## Min 1Q Median 3Q Max ## -2.8311 -0.7273 -0.0537 0.6338 2.3359 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 2.1305 0.2319 9.188 7.61e-15 *** ## x1 1.4396 0.7212 1.996 0.0487 * ## x2 1.0097 1.1337 0.891 0.3754 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 1.056 on 97 degrees of freedom ## Multiple R-squared: 0.2088, Adjusted R-squared: 0.1925 ## F-statistic: 12.8 on 2 and 97 DF, p-value: 1.164e-05 \\(\\hat\\beta_0 = 2.13\\), \\(\\hat\\beta_1 = 1.43\\), and \\(\\hat\\beta_2 = 1.01\\). These are relatively poor estimates of the true values. We can reject the hypothesis that \\(H_0 : \\beta_1\\) at a p-value of 0.05 (just about). We cannot reject the hypothesis that \\(H_0 : \\beta_2 = 0\\). Now fit a least squares regression to predict y using only x1. Comment on your results. Can you reject the null hypothesis \\(H 0 : \\beta_1 = 0\\)? summary(lm(y ~ x1)) ## ## Call: ## lm(formula = y ~ x1) ## ## Residuals: ## Min 1Q Median 3Q Max ## -2.89495 -0.66874 -0.07785 0.59221 2.45560 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 2.1124 0.2307 9.155 8.27e-15 *** ## x1 1.9759 0.3963 4.986 2.66e-06 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 1.055 on 98 degrees of freedom ## Multiple R-squared: 0.2024, Adjusted R-squared: 0.1942 ## F-statistic: 24.86 on 1 and 98 DF, p-value: 2.661e-06 We can reject \\(H_0 : \\beta_1 = 0\\). The p-value is much more significant for \\(\\beta_1\\) compared to when x2 is included in the model. Now fit a least squares regression to predict y using only x2. Comment on your results. Can you reject the null hypothesis \\(H_0 : \\beta_1 = 0\\)? summary(lm(y ~ x2)) ## ## Call: ## lm(formula = y ~ x2) ## ## Residuals: ## Min 1Q Median 3Q Max ## -2.62687 -0.75156 -0.03598 0.72383 2.44890 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 2.3899 0.1949 12.26 &lt; 2e-16 *** ## x2 2.8996 0.6330 4.58 1.37e-05 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 1.072 on 98 degrees of freedom ## Multiple R-squared: 0.1763, Adjusted R-squared: 0.1679 ## F-statistic: 20.98 on 1 and 98 DF, p-value: 1.366e-05 Similarly, we can reject \\(H_0 : \\beta_2 = 0\\). The p-value is much more significant for \\(\\beta_2\\) compared to when x1 is included in the model. Do the results obtained in (c)–(e) contradict each other? Explain your answer. No they do not contradict each other. Both x1 and x2 individually are capable of explaining much of the variation observed in y, however since they are correlated, it is very difficult to tease apart their separate contributions. Now suppose we obtain one additional observation, which was unfortunately mismeasured. &gt; x1 &lt;- c(x1, 0.1) &gt; x2 &lt;- c(x2, 0.8) &gt; y &lt;- c(y, 6) Re-fit the linear models from (c) to (e) using this new data. What effect does this new observation have on the each of the models? In each model, is this observation an outlier? A high-leverage point? Both? Explain your answers. x1 &lt;- c(x1 , 0.1) x2 &lt;- c(x2 , 0.8) y &lt;- c(y ,6) summary(lm(y ~ x1 + x2)) ## ## Call: ## lm(formula = y ~ x1 + x2) ## ## Residuals: ## Min 1Q Median 3Q Max ## -2.73348 -0.69318 -0.05263 0.66385 2.30619 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 2.2267 0.2314 9.624 7.91e-16 *** ## x1 0.5394 0.5922 0.911 0.36458 ## x2 2.5146 0.8977 2.801 0.00614 ** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 1.075 on 98 degrees of freedom ## Multiple R-squared: 0.2188, Adjusted R-squared: 0.2029 ## F-statistic: 13.72 on 2 and 98 DF, p-value: 5.564e-06 summary(lm(y ~ x1)) ## ## Call: ## lm(formula = y ~ x1) ## ## Residuals: ## Min 1Q Median 3Q Max ## -2.8897 -0.6556 -0.0909 0.5682 3.5665 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 2.2569 0.2390 9.445 1.78e-15 *** ## x1 1.7657 0.4124 4.282 4.29e-05 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 1.111 on 99 degrees of freedom ## Multiple R-squared: 0.1562, Adjusted R-squared: 0.1477 ## F-statistic: 18.33 on 1 and 99 DF, p-value: 4.295e-05 summary(lm(y ~ x2)) ## ## Call: ## lm(formula = y ~ x2) ## ## Residuals: ## Min 1Q Median 3Q Max ## -2.64729 -0.71021 -0.06899 0.72699 2.38074 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 2.3451 0.1912 12.264 &lt; 2e-16 *** ## x2 3.1190 0.6040 5.164 1.25e-06 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 1.074 on 99 degrees of freedom ## Multiple R-squared: 0.2122, Adjusted R-squared: 0.2042 ## F-statistic: 26.66 on 1 and 99 DF, p-value: 1.253e-06 par(mfrow = c(2, 2)) plot(lm(y ~ x1 + x2), cex = 0.2) par(mfrow = c(2, 2)) plot(lm(y ~ x1), cex = 0.2) par(mfrow = c(2, 2)) plot(lm(y ~ x2), cex = 0.2) In the first model (with both predictors), the new point has very high leverage (since it is an outlier in terms of the joint x1 and x2 distribution), however it is not an outlier. In the model that includes x1, it is an outlier but does not have high leverage. In the model that includes x2, it has high leverage but is not an outlier. It is useful to consider the scatterplot of x1 and x2. plot(x1, x2) points(0.1, 0.8, col = &quot;red&quot;, pch = 19) 3.2.8 Question 15 This problem involves the Boston data set, which we saw in the lab for this chapter. We will now try to predict per capita crime rate using the other variables in this data set. In other words, per capita crime rate is the response, and the other variables are the predictors. We are trying to predict crim. pred &lt;- subset(Boston, select = -crim) For each predictor, fit a simple linear regression model to predict the response. Describe your results. In which of the models is there a statistically significant association between the predictor and the response? Create some plots to back up your assertions. fits &lt;- lapply(pred, function(x) lm(Boston$crim ~ x)) printCoefmat(do.call(rbind, lapply(fits, function(x) coef(summary(x))[2, ]))) ## Estimate Std. Error t value Pr(&gt;|t|) ## zn -0.0739350 0.0160946 -4.5938 5.506e-06 *** ## indus 0.5097763 0.0510243 9.9908 &lt; 2.2e-16 *** ## chas -1.8927766 1.5061155 -1.2567 0.2094 ## nox 31.2485312 2.9991904 10.4190 &lt; 2.2e-16 *** ## rm -2.6840512 0.5320411 -5.0448 6.347e-07 *** ## age 0.1077862 0.0127364 8.4628 2.855e-16 *** ## dis -1.5509017 0.1683300 -9.2135 &lt; 2.2e-16 *** ## rad 0.6179109 0.0343318 17.9982 &lt; 2.2e-16 *** ## tax 0.0297423 0.0018474 16.0994 &lt; 2.2e-16 *** ## ptratio 1.1519828 0.1693736 6.8014 2.943e-11 *** ## lstat 0.5488048 0.0477610 11.4907 &lt; 2.2e-16 *** ## medv -0.3631599 0.0383902 -9.4597 &lt; 2.2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 There are significant associations for all predictors with the exception of chas when fitting separate linear models. For example, consider the following plot representing the third model plot(Boston$rm, Boston$crim) abline(fits[[5]]) Fit a multiple regression model to predict the response using all of the predictors. Describe your results. For which predictors can we reject the null hypothesis \\(H_0 : \\beta_j = 0\\)? mfit &lt;- lm(crim ~ ., data = Boston) summary(mfit) ## ## Call: ## lm(formula = crim ~ ., data = Boston) ## ## Residuals: ## Min 1Q Median 3Q Max ## -8.534 -2.248 -0.348 1.087 73.923 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 13.7783938 7.0818258 1.946 0.052271 . ## zn 0.0457100 0.0187903 2.433 0.015344 * ## indus -0.0583501 0.0836351 -0.698 0.485709 ## chas -0.8253776 1.1833963 -0.697 0.485841 ## nox -9.9575865 5.2898242 -1.882 0.060370 . ## rm 0.6289107 0.6070924 1.036 0.300738 ## age -0.0008483 0.0179482 -0.047 0.962323 ## dis -1.0122467 0.2824676 -3.584 0.000373 *** ## rad 0.6124653 0.0875358 6.997 8.59e-12 *** ## tax -0.0037756 0.0051723 -0.730 0.465757 ## ptratio -0.3040728 0.1863598 -1.632 0.103393 ## lstat 0.1388006 0.0757213 1.833 0.067398 . ## medv -0.2200564 0.0598240 -3.678 0.000261 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 6.46 on 493 degrees of freedom ## Multiple R-squared: 0.4493, Adjusted R-squared: 0.4359 ## F-statistic: 33.52 on 12 and 493 DF, p-value: &lt; 2.2e-16 There are now only significant associations for zn, dis, rad, black and medv. How do your results from (a) compare to your results from (b)? Create a plot displaying the univariate regression coefficients from (a) on the \\(x\\)-axis, and the multiple regression coefficients from (b) on the \\(y\\)-axis. That is, each predictor is displayed as a single point in the plot. Its coefficient in a simple linear regression model is shown on the x-axis, and its coefficient estimate in the multiple linear regression model is shown on the y-axis. The results from (b) show reduced significance compared to the models fit in (a). plot(sapply(fits, function(x) coef(x)[2]), coef(mfit)[-1], xlab = &quot;Univariate regression&quot;, ylab = &quot;multiple regression&quot;) The estimated coefficients differ (in particular the estimated coefficient for nox is dramatically different) between the two modelling strategies. Is there evidence of non-linear association between any of the predictors and the response? To answer this question, for each predictor X, fit a model of the form \\[ Y = \\beta_0 + \\beta_1X + \\beta_2X^2 + \\beta_3X^3 + \\epsilon \\] pred &lt;- subset(pred, select = -chas) fits &lt;- lapply(names(pred), function(p) { f &lt;- paste0(&quot;crim ~ poly(&quot;, p, &quot;, 3)&quot;) lm(as.formula(f), data = Boston) }) for (fit in fits) printCoefmat(coef(summary(fit))) ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 3.61352 0.37219 9.7088 &lt; 2.2e-16 *** ## poly(zn, 3)1 -38.74984 8.37221 -4.6284 4.698e-06 *** ## poly(zn, 3)2 23.93983 8.37221 2.8594 0.004421 ** ## poly(zn, 3)3 -10.07187 8.37221 -1.2030 0.229539 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 3.6135 0.3300 10.9501 &lt; 2.2e-16 *** ## poly(indus, 3)1 78.5908 7.4231 10.5873 &lt; 2.2e-16 *** ## poly(indus, 3)2 -24.3948 7.4231 -3.2863 0.001086 ** ## poly(indus, 3)3 -54.1298 7.4231 -7.2920 1.196e-12 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 3.61352 0.32157 11.2370 &lt; 2.2e-16 *** ## poly(nox, 3)1 81.37202 7.23361 11.2492 &lt; 2.2e-16 *** ## poly(nox, 3)2 -28.82859 7.23361 -3.9854 7.737e-05 *** ## poly(nox, 3)3 -60.36189 7.23361 -8.3446 6.961e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 3.6135 0.3703 9.7584 &lt; 2.2e-16 *** ## poly(rm, 3)1 -42.3794 8.3297 -5.0878 5.128e-07 *** ## poly(rm, 3)2 26.5768 8.3297 3.1906 0.001509 ** ## poly(rm, 3)3 -5.5103 8.3297 -0.6615 0.508575 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 3.61352 0.34852 10.3683 &lt; 2.2e-16 *** ## poly(age, 3)1 68.18201 7.83970 8.6970 &lt; 2.2e-16 *** ## poly(age, 3)2 37.48447 7.83970 4.7814 2.291e-06 *** ## poly(age, 3)3 21.35321 7.83970 2.7237 0.00668 ** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 3.61352 0.32592 11.0870 &lt; 2.2e-16 *** ## poly(dis, 3)1 -73.38859 7.33148 -10.0101 &lt; 2.2e-16 *** ## poly(dis, 3)2 56.37304 7.33148 7.6892 7.870e-14 *** ## poly(dis, 3)3 -42.62188 7.33148 -5.8135 1.089e-08 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 3.61352 0.29707 12.1639 &lt; 2.2e-16 *** ## poly(rad, 3)1 120.90745 6.68240 18.0934 &lt; 2.2e-16 *** ## poly(rad, 3)2 17.49230 6.68240 2.6177 0.009121 ** ## poly(rad, 3)3 4.69846 6.68240 0.7031 0.482314 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 3.61352 0.30468 11.8599 &lt; 2.2e-16 *** ## poly(tax, 3)1 112.64583 6.85371 16.4358 &lt; 2.2e-16 *** ## poly(tax, 3)2 32.08725 6.85371 4.6817 3.665e-06 *** ## poly(tax, 3)3 -7.99681 6.85371 -1.1668 0.2439 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 3.61352 0.36105 10.0084 &lt; 2.2e-16 *** ## poly(ptratio, 3)1 56.04523 8.12158 6.9008 1.565e-11 *** ## poly(ptratio, 3)2 24.77482 8.12158 3.0505 0.002405 ** ## poly(ptratio, 3)3 -22.27974 8.12158 -2.7433 0.006301 ** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 3.61352 0.33917 10.6540 &lt;2e-16 *** ## poly(lstat, 3)1 88.06967 7.62944 11.5434 &lt;2e-16 *** ## poly(lstat, 3)2 15.88816 7.62944 2.0825 0.0378 * ## poly(lstat, 3)3 -11.57402 7.62944 -1.5170 0.1299 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 3.61352 0.29203 12.374 &lt; 2.2e-16 *** ## poly(medv, 3)1 -75.05761 6.56915 -11.426 &lt; 2.2e-16 *** ## poly(medv, 3)2 88.08621 6.56915 13.409 &lt; 2.2e-16 *** ## poly(medv, 3)3 -48.03343 6.56915 -7.312 1.047e-12 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Yes there is strong evidence for many variables having non-linear associations. In many cases, the addition of a cubic term is significant (indus, nox, age, dis, ptratio and medv). In other cases although the cubic terms is not significant, the squared term is (zn, rm, rad, tax, lstat). In only one case, black is there no evidence for a non-linear relationship. "],["classification.html", "4 Classification 4.1 Conceptual 4.2 Applied", " 4 Classification 4.1 Conceptual 4.1.1 Question 1 Using a little bit of algebra, prove that (4.2) is equivalent to (4.3). In other words, the logistic function representation and logit representation for the logistic regression model are equivalent. We need to show that \\[ p(X) = \\frac{e^{\\beta_0 + \\beta_1X}}{1 + e^{\\beta_0 + \\beta_1X}} \\] is equivalent to \\[ \\frac{p(X)}{1-p(X)} = e^{\\beta_0 + \\beta_1X} \\] Letting \\(x = e^{\\beta_0 + \\beta_1X}\\) \\[\\begin{align} \\frac{P(X)}{1-p(X)} &amp;= \\frac{\\frac{x}{1 + x}} {1 - \\frac{x}{1 + x}} \\\\ &amp;= \\frac{\\frac{x}{1 + x}} {\\frac{1}{1 + x}} \\\\ &amp;= x \\end{align}\\] 4.1.2 Question 2 It was stated in the text that classifying an observation to the class for which (4.12) is largest is equivalent to classifying an observation to the class for which (4.13) is largest. Prove that this is the case. In other words, under the assumption that the observations in the \\(k\\)th class are drawn from a \\(N(\\mu_k,\\sigma^2)\\) distribution, the Bayes’ classifier assigns an observation to the class for which the discriminant function is maximized. 4.12 is \\[ p_k(x) = \\frac{\\pi_k\\frac{1}{\\sqrt{2\\pi\\sigma}} \\exp(-\\frac{1}{2\\sigma^2}(x - \\mu_k)^2)} {\\sum_{l=1}^k \\pi_l\\frac{1}{\\sqrt{2\\pi\\sigma}} \\exp(-\\frac{1}{2\\sigma^2}(x - \\mu_l)^2)} \\] and the discriminant function is \\[ \\delta_k(x) = x.\\frac{\\mu_k}{\\sigma^2} - \\frac{\\mu_k^2}{2\\sigma_2} + \\log(\\pi_k) \\] Since \\(\\sigma^2\\) is constant \\[ p_k(x) = \\frac{\\pi_k \\exp\\left(-\\frac{1}{2\\sigma^2}(x - \\mu_k)^2\\right)} {\\sum_{l=1}^k \\pi_l \\exp\\left(-\\frac{1}{2\\sigma^2}(x - \\mu_l)^2\\right)} \\] Maximizing \\(p_k(x)\\) also maximizes any monotonic function of \\(p_k(X)\\), and therefore, we can consider maximizing \\(\\log(p_K(X))\\) \\[ \\log(p_k(x)) = \\log(\\pi_k) - \\frac{1}{2\\sigma^2}(x - \\mu_k)^2 - \\log\\left(\\sum_{l=1}^k \\pi_l \\exp\\left(-\\frac{1}{2\\sigma^2}(x - \\mu_l)^2\\right)\\right) \\] Remember that we are maximizing over \\(k\\), and since the last term does not vary with \\(k\\) it can be ignored. So we just need to maximize \\[\\begin{align} f &amp;= \\log(\\pi_k) - \\frac{1}{2\\sigma^2} (x^2 - 2x\\mu_k + \\mu_k^2) \\\\ &amp;= \\log(\\pi_k) - \\frac{x^2}{2\\sigma^2} + \\frac{x\\mu_k}{\\sigma^2} - \\frac{\\mu_k^2}{2\\sigma^2} \\\\ \\end{align}\\] Since \\(\\frac{x^2}{2\\sigma^2}\\) is also independent of \\(k\\), we just need to maximize \\[ \\log(\\pi_k) + \\frac{x\\mu_k}{\\sigma^2} - \\frac{\\mu_k^2}{2\\sigma^2} \\] 4.1.3 Question 3 This problem relates to the QDA model, in which the observations within each class are drawn from a normal distribution with a class-specific mean vector and a class specific covariance matrix. We consider the simple case where \\(p = 1\\); i.e. there is only one feature. Suppose that we have \\(K\\) classes, and that if an observation belongs to the \\(k\\)th class then \\(X\\) comes from a one-dimensional normal distribution, \\(X \\sim N(\\mu_k,\\sigma^2)\\). Recall that the density function for the one-dimensional normal distribution is given in (4.16). Prove that in this case, the Bayes classifier is not linear. Argue that it is in fact quadratic. Hint: For this problem, you should follow the arguments laid out in Section 4.4.1, but without making the assumption that \\(\\sigma_1^2 = ... = \\sigma_K^2\\). As above, \\[ p_k(x) = \\frac{\\pi_k\\frac{1}{\\sqrt{2\\pi\\sigma_k}} \\exp(-\\frac{1}{2\\sigma_k^2}(x - \\mu_k)^2)} {\\sum_{l=1}^k \\pi_l\\frac{1}{\\sqrt{2\\pi\\sigma_l}} \\exp(-\\frac{1}{2\\sigma_l^2}(x - \\mu_l)^2)} \\] Now lets derive the Bayes classifier, without assuming \\(\\sigma_1^2 = ... = \\sigma_K^2\\) Maximizing \\(p_k(x)\\) also maximizes any monotonic function of \\(p_k(X)\\), and therefore, we can consider maximizing \\(\\log(p_K(X))\\) \\[ \\log(p_k(x)) = \\log(\\pi_k) + \\log\\left(\\frac{1}{\\sqrt{2\\pi\\sigma_k}}\\right) - \\frac{1}{2\\sigma_k^2}(x - \\mu_k)^2 - \\log\\left(\\sum_{l=1}^k \\frac{1}{\\sqrt{2\\pi\\sigma_l}} \\pi_l \\exp\\left(-\\frac{1}{2\\sigma_l^2}(x - \\mu_l)^2\\right)\\right) \\] Remember that we are maximizing over \\(k\\), and since the last term does not vary with \\(k\\) it can be ignored. So we just need to maximize \\[\\begin{align} f &amp;= \\log(\\pi_k) + \\log\\left(\\frac{1}{\\sqrt{2\\pi\\sigma_k}}\\right) - \\frac{1}{2\\sigma_k^2}(x - \\mu_k)^2 \\\\ &amp;= \\log(\\pi_k) + \\log\\left(\\frac{1}{\\sqrt{2\\pi\\sigma_k}}\\right) - \\frac{x^2}{2\\sigma_k^2} + \\frac{x\\mu_k}{\\sigma_k^2} - \\frac{\\mu_k^2}{2\\sigma_k^2} \\\\ \\end{align}\\] However, unlike in Q2, \\(\\frac{x^2}{2\\sigma_k^2}\\) is not independent of \\(k\\), so we retain the term with \\(x^2\\), hence \\(f\\), the Bayes’ classifier, is a quadratic function of \\(x\\). 4.1.4 Question 4 When the number of features \\(p\\) is large, there tends to be a deterioration in the performance of KNN and other local approaches that perform prediction using only observations that are near the test observation for which a prediction must be made. This phenomenon is known as the curse of dimensionality, and it ties into the fact that non-parametric approaches often perform poorly when \\(p\\) is large. We will now investigate this curse. Suppose that we have a set of observations, each with measurements on \\(p = 1\\) feature, \\(X\\). We assume that \\(X\\) is uniformly (evenly) distributed on \\([0, 1]\\). Associated with each observation is a response value. Suppose that we wish to predict a test observation’s response using only observations that are within 10% of the range of \\(X\\) closest to that test observation. For instance, in order to predict the response for a test observation with \\(X = 0.6\\), we will use observations in the range \\([0.55, 0.65]\\). On average, what fraction of the available observations will we use to make the prediction? For values in \\(0...0.05\\), we use less than 10% of observations (between 5% and 10%, 7.5% on average), similarly with values in \\(0.95...1\\). For values in \\(0.05...0.95\\) we use 10% of available observations. The (weighted) average is then \\(7.5 \\times 0.1 + 10 \\times 0.9 = 9.75\\%\\). Now suppose that we have a set of observations, each with measurements on \\(p = 2\\) features, \\(X_1\\) and \\(X_2\\). We assume that \\((X_1, X_2)\\) are uniformly distributed on \\([0, 1] \\times [0, 1]\\). We wish to predict a test observation’s response using only observations that are within 10% of the range of \\(X_1\\) and within 10% of the range of \\(X_2\\) closest to that test observation. For instance, in order to predict the response for a test observation with \\(X_1 = 0.6\\) and \\(X_2 = 0.35\\), we will use observations in the range \\([0.55, 0.65]\\) for \\(X_1\\) and in the range \\([0.3, 0.4]\\) for \\(X_2\\). On average, what fraction of the available observations will we use to make the prediction? Since we need the observation to be within range for \\(X_1\\) and \\(X_2\\) we square 9.75% = \\(0.0975^2 \\times 100 = 0.95\\%\\) Now suppose that we have a set of observations on \\(p = 100\\) features. Again the observations are uniformly distributed on each feature, and again each feature ranges in value from 0 to 1. We wish to predict a test observation’s response using observations within the 10% of each feature’s range that is closest to that test observation. What fraction of the available observations will we use to make the prediction? Similar to above, we use: \\(0.0975^{100} \\times 100 = 8 \\times 10^{-100}\\%\\), essentially zero. Using your answers to parts (a)–(c), argue that a drawback of KNN when \\(p\\) is large is that there are very few training observations “near” any given test observation. As \\(p\\) increases, the fraction of observations near any given point rapidly approaches zero. For instance, even if you use 50% of the nearest observations for each \\(p\\), with \\(p = 10\\), only \\(0.5^{10} \\times 100 \\approx 0.1\\%\\) points are “near”. Now suppose that we wish to make a prediction for a test observation by creating a \\(p\\)-dimensional hypercube centered around the test observation that contains, on average, 10% of the training observations. For \\(p = 1,2,\\) and \\(100\\), what is the length of each side of the hypercube? Comment on your answer. Note: A hypercube is a generalization of a cube to an arbitrary number of dimensions. When \\(p = 1\\), a hypercube is simply a line segment, when \\(p = 2\\) it is a square, and when \\(p = 100\\) it is a 100-dimensional cube. When \\(p = 1\\), clearly the length is 0.1. When \\(p = 2\\), we need the value \\(l\\) such that \\(l^2 = 0.1\\), so \\(l = \\sqrt{0.1} = 0.32\\). When \\(p = n\\), \\(l = 0.1^{1/n}\\), so in the case of \\(n = 100\\), \\(l = 0.98\\). Therefore, the length of each side of the hypercube rapidly approaches 1 (or 100%) of the range of each \\(p\\). 4.1.5 Question 5 We now examine the differences between LDA and QDA. If the Bayes decision boundary is linear, do we expect LDA or QDA to perform better on the training set? On the test set? QDA, being a more flexible model, will always perform better on the training set, but LDA would be expected to perform better on the test set. If the Bayes decision boundary is non-linear, do we expect LDA or QDA to perform better on the training set? On the test set? QDA, being a more flexible model, will perform better on the training set, and we would hope that extra flexibility translates to a better fit on the test set. In general, as the sample size \\(n\\) increases, do we expect the test prediction accuracy of QDA relative to LDA to improve, decline, or be unchanged? Why? As \\(n\\) increases, we would expect the prediction accuracy of QDA relative to LDA to improve as there is more data to fit to subtle effects in the data. True or False: Even if the Bayes decision boundary for a given problem is linear, we will probably achieve a superior test error rate using QDA rather than LDA because QDA is flexible enough to model a linear decision boundary. Justify your answer. False. QDA can overfit leading to poorer test performance. 4.1.6 Question 6 Suppose we collect data for a group of students in a statistics class with variables \\(X_1 =\\) hours studied, \\(X_2 =\\) undergrad GPA, and \\(Y =\\) receive an A. We fit a logistic regression and produce estimated coefficient, \\(\\hat\\beta_0 = -6\\), \\(\\hat\\beta_1 = 0.05\\), \\(\\hat\\beta_2 = 1\\). Estimate the probability that a student who studies for 40h and has an undergrad GPA of 3.5 gets an A in the class. The logistic model is: \\[ \\log\\left(\\frac{p(X)}{1-p(x)}\\right) = -6 + 0.05X_1 + X_2 \\] or \\[ p(X) = \\frac{e^{-6 + 0.05X_1 + X_2}}{1 + e^{-6 + 0.05X_1 + X_2}} \\] when \\(X_1 = 40\\) and \\(X_2 = 3.5\\), \\(p(X) = 0.38\\) How many hours would the student in part (a) need to study to have a 50% chance of getting an A in the class? We would like to solve for \\(X_1\\) where \\(p(X) = 0.5\\). Taking the first equation above, we need to solve \\(0 = −6 + 0.05X_1 + 3.5\\), so \\(X_1 = 50\\) hours. 4.1.7 Question 7 Suppose that we wish to predict whether a given stock will issue a dividend this year (“Yes” or “No”) based on \\(X\\), last year’s percent profit. We examine a large number of companies and discover that the mean value of \\(X\\) for companies that issued a dividend was \\(\\bar{X} = 10\\), while the mean for those that didn’t was \\(\\bar{X} = 0\\). In addition, the variance of \\(X\\) for these two sets of companies was \\(\\hat{\\sigma}^2 = 36\\). Finally, 80% of companies issued dividends. Assuming that \\(X\\) follows a normal distribution, predict the probability that a company will issue a dividend this year given that its percentage profit was \\(X = 4\\) last year. Hint: Recall that the density function for a normal random variable is \\(f(x) =\\frac{1}{\\sqrt{2\\pi\\sigma^2}}e^{-(x-\\mu)^2/2\\sigma^2}\\). You will need to use Bayes’ theorem. Value \\(v\\) for companies (D) issuing a dividend = \\(v_D \\sim \\mathcal{N}(10, 36)\\). Value \\(v\\) for companies (N) not issuing a dividend = \\(v_N \\sim \\mathcal{N}(0, 36)\\) and \\(p(D) = 0.8\\). We want to find \\(p(D|v)\\) and we can calculate \\(p(v|D)\\) from the Gaussian density function. Note that since \\(e^2\\) is equal between both classes, the term \\(\\frac{1}{\\sqrt{2\\pi\\sigma^2}}\\) cancels. \\[\\begin{align} p(D|v) &amp;= \\frac{p(v|D) p(D)}{p(v|D)p(D) + p(v|N)p(N)} \\\\ &amp;= \\frac{\\pi_D \\frac{1}{\\sqrt{2\\pi\\sigma^2}} e^{-(x-\\mu_D)^2/2\\sigma^2}} {\\pi_D \\frac{1}{\\sqrt{2\\pi\\sigma^2}} e^{-(x-\\mu_D)^2/2\\sigma^2} + \\pi_N \\frac{1}{\\sqrt{2\\pi\\sigma^2}} e^{-(x-\\mu_N)^2/2\\sigma^2}} \\\\ &amp;= \\frac{\\pi_D e^{-(x-\\mu_D)^2/2\\sigma^2}} {\\pi_D e^{-(x-\\mu_D)^2/2\\sigma^2} + \\pi_N e^{-(x-\\mu_N)^2/2\\sigma^2}} \\\\ &amp;= \\frac{0.8 \\times e^{-(4-10)^2/(2 \\times 36)}} {0.8 \\times e^{-(4-10)^2/(2 \\times 36)} + 0.2 \\times e^{-(4-0)^2/(2 \\times 36)}} \\\\ &amp;= \\frac{0.8 e^{-1/2}}{0.8 e^{-1/2} + 0.2 e^{-2/9}} \\end{align}\\] exp(-0.5) * 0.8 / (exp(-0.5) * 0.8 + exp(-2/9) * 0.2) ## [1] 0.7518525 4.1.8 Question 8 Suppose that we take a data set, divide it into equally-sized training and test sets, and then try out two different classification procedures. First we use logistic regression and get an error rate of 20% on the training data and 30% on the test data. Next we use 1-nearest neighbors (i.e. \\(K = 1\\)) and get an average error rate (averaged over both test and training data sets) of 18%. Based on these results, which method should we prefer to use for classification of new observations? Why? For \\(K = 1\\), performance on the training set is perfect and the error rate is zero, implying a test error rate of 36%. Logistic regression outperforms 1-nearest neighbor on the test set and therefore should be preferred. 4.1.9 Question 9 This problem has to do with odds. On average, what fraction of people with an odds of 0.37 of defaulting on their credit card payment will in fact default? Odds is defined as \\(p/(1-p)\\). \\[0.37 = \\frac{p(x)}{1 - p(x)}\\] therefore, \\[p(x) = \\frac{0.37}{1 + 0.37} = 0.27\\] Suppose that an individual has a 16% chance of defaulting on her credit card payment. What are the odds that she will default? \\[0.16 / (1 - 0.16) = 0.19\\] 4.1.10 Question 10 Equation 4.32 derived an expression for \\(\\log(\\frac{Pr(Y=k|X=x)}{Pr(Y=K|X=x)})\\) in the setting where \\(p &gt; 1\\), so that the mean for the \\(k\\)th class, \\(\\mu_k\\), is a \\(p\\)-dimensional vector, and the shared covariance \\(\\Sigma\\) is a \\(p \\times p\\) matrix. However, in the setting with \\(p = 1\\), (4.32) takes a simpler form, since the means \\(\\mu_1, ..., \\mu_k\\) and the variance \\(\\sigma^2\\) are scalars. In this simpler setting, repeat the calculation in (4.32), and provide expressions for \\(a_k\\) and \\(b_{kj}\\) in terms of \\(\\pi_k, \\pi_K, \\mu_k, \\mu_K,\\) and \\(\\sigma^2\\). \\[\\begin{align*} \\log\\left(\\frac{Pr(Y=k|X=x)}{Pr(Y=K|X=x)}\\right) &amp; = \\log\\left(\\frac{\\pi_k f_k(x)}{\\pi_K f_K(x)}\\right) \\\\ &amp; = \\log\\left(\\frac{\\pi_k \\exp(-1/2((x-\\mu_k)/\\sigma)^2)}{\\pi_K \\exp(-1/2((x-\\mu_K)/\\sigma)^2)}\\right) \\\\ &amp; = \\log\\left(\\frac{\\pi_k}{\\pi_K}\\right) - \\frac{1}{2} \\left(\\frac{x-\\mu_k}{\\sigma}\\right)^2 + \\frac{1}{2} \\left(\\frac{x-\\mu_K}{\\sigma}\\right)^2 \\\\ &amp; = \\log\\left(\\frac{\\pi_k}{\\pi_K}\\right) - \\frac{1}{2\\sigma^2} (x-\\mu_k)^2 + \\frac{1}{2\\sigma^2} (x-\\mu_K)^2 \\\\ &amp; = \\log\\left(\\frac{\\pi_k}{\\pi_K}\\right) - \\frac{1}{2\\sigma^2} \\left((x-\\mu_k)^2 - (x-\\mu_K)^2\\right) \\\\ &amp; = \\log\\left(\\frac{\\pi_k}{\\pi_K}\\right) - \\frac{1}{2\\sigma^2} \\left(x^2-2x\\mu_k+\\mu_k^2 - x^2 + 2x\\mu_K - \\mu_K^2\\right) \\\\ &amp; = \\log\\left(\\frac{\\pi_k}{\\pi_K}\\right) - \\frac{1}{2\\sigma^2} \\left(2x(\\mu_K - \\mu_k) + \\mu_k^2 -\\mu_K^2\\right) \\\\ &amp; = \\log\\left(\\frac{\\pi_k}{\\pi_K}\\right) - \\frac{\\mu_k^2 -\\mu_K^2}{2\\sigma^2} + \\frac{x(\\mu_k - \\mu_K)}{\\sigma^2} \\end{align*}\\] Therefore, \\[a_k = \\log\\left(\\frac{\\pi_k}{\\pi_K}\\right) - \\frac{\\mu_k^2 -\\mu_K^2}{2\\sigma^2}\\] and \\[b_k = (\\mu_k - \\mu_K) / \\sigma^2\\] 4.1.11 Question 11 ToDo Work out the detailed forms of \\(a_k\\), \\(b_{kj}\\), and \\(b_{kjl}\\) in (4.33). Your answer should involve \\(\\pi_k\\), \\(\\pi_K\\), \\(\\mu_k\\), \\(\\mu_K\\), \\(\\Sigma_k\\), and \\(\\Sigma_K\\). 4.1.12 Question 12 Suppose that you wish to classify an observation \\(X \\in \\mathbb{R}\\) into apples and oranges. You fit a logistic regression model and find that \\[ \\hat{Pr}(Y=orange|X=x) = \\frac{\\exp(\\hat\\beta_0 + \\hat\\beta_1x)}{1 + \\exp(\\hat\\beta_0 + \\hat\\beta_1x)} \\] Your friend fits a logistic regression model to the same data using the softmax formulation in (4.13), and finds that \\[ \\hat{Pr}(Y=orange|X=x) = \\frac{\\exp(\\hat\\alpha_{orange0} + \\hat\\alpha_{orange1}x)} {\\exp(\\hat\\alpha_{orange0} + \\hat\\alpha_{orange1}x) + \\exp(\\hat\\alpha_{apple0} + \\hat\\alpha_{apple1}x)} \\] What is the log odds of orange versus apple in your model? The log odds is just \\(\\hat\\beta_0 + \\hat\\beta_1x\\) What is the log odds of orange versus apple in your friend’s model? From 4.14, log odds of our friend’s model is: \\[ (\\hat\\alpha_{orange0} - \\hat\\alpha_{apple0}) + (\\hat\\alpha_{orange1} - \\hat\\alpha_{apple1})x \\] Suppose that in your model, \\(\\hat\\beta_0 = 2\\) and \\(\\hat\\beta = −1\\). What are the coefficient estimates in your friend’s model? Be as specific as possible. We can say that in our friend’s model \\(\\hat\\alpha_{orange0} - \\hat\\alpha_{apple0} = 2\\) and \\(\\hat\\alpha_{orange1} - \\hat\\alpha_{apple1} = -1\\). We are unable to know the specific value of each parameter however. Now suppose that you and your friend fit the same two models on a different data set. This time, your friend gets the coefficient estimates \\(\\hat\\alpha_{orange0} = 1.2\\), \\(\\hat\\alpha_{orange1} = −2\\), \\(\\hat\\alpha_{apple0} = 3\\), \\(\\hat\\alpha_{apple1} = 0.6\\). What are the coefficient estimates in your model? The coefficients in our model would be \\(\\hat\\beta_0 = 1.2 - 3 = -1.8\\) and \\(\\hat\\beta_1 = -2 - 0.6 = -2.6\\) Finally, suppose you apply both models from (d) to a data set with 2,000 test observations. What fraction of the time do you expect the predicted class labels from your model to agree with those from your friend’s model? Explain your answer. The models are identical with different parameterization so they should perfectly agree. 4.2 Applied 4.2.1 Question 13 This question should be answered using the Weekly data set, which is part of the ISLR2 package. This data is similar in nature to the Smarket data from this chapter’s lab, except that it contains 1,089 weekly returns for 21 years, from the beginning of 1990 to the end of 2010. Produce some numerical and graphical summaries of the Weekly data. Do there appear to be any patterns? library(MASS) library(class) library(tidyverse) library(corrplot) library(ISLR2) library(e1071) summary(Weekly) ## Year Lag1 Lag2 Lag3 ## Min. :1990 Min. :-18.1950 Min. :-18.1950 Min. :-18.1950 ## 1st Qu.:1995 1st Qu.: -1.1540 1st Qu.: -1.1540 1st Qu.: -1.1580 ## Median :2000 Median : 0.2410 Median : 0.2410 Median : 0.2410 ## Mean :2000 Mean : 0.1506 Mean : 0.1511 Mean : 0.1472 ## 3rd Qu.:2005 3rd Qu.: 1.4050 3rd Qu.: 1.4090 3rd Qu.: 1.4090 ## Max. :2010 Max. : 12.0260 Max. : 12.0260 Max. : 12.0260 ## Lag4 Lag5 Volume Today ## Min. :-18.1950 Min. :-18.1950 Min. :0.08747 Min. :-18.1950 ## 1st Qu.: -1.1580 1st Qu.: -1.1660 1st Qu.:0.33202 1st Qu.: -1.1540 ## Median : 0.2380 Median : 0.2340 Median :1.00268 Median : 0.2410 ## Mean : 0.1458 Mean : 0.1399 Mean :1.57462 Mean : 0.1499 ## 3rd Qu.: 1.4090 3rd Qu.: 1.4050 3rd Qu.:2.05373 3rd Qu.: 1.4050 ## Max. : 12.0260 Max. : 12.0260 Max. :9.32821 Max. : 12.0260 ## Direction ## Down:484 ## Up :605 ## ## ## ## corrplot(cor(Weekly[, -9]), type = &quot;lower&quot;, diag = FALSE, method = &quot;ellipse&quot;) Volume is strongly positively correlated with Year. Other correlations are week, but Lag1 is negatively correlated with Lag2 but positively correlated with Lag3. Use the full data set to perform a logistic regression with Direction as the response and the five lag variables plus Volume as predictors. Use the summary function to print the results. Do any of the predictors appear to be statistically significant? If so, which ones? fit &lt;- glm( Direction ~ Lag1 + Lag2 + Lag3 + Lag4 + Lag5 + Volume, data = Weekly, family = binomial ) summary(fit) ## ## Call: ## glm(formula = Direction ~ Lag1 + Lag2 + Lag3 + Lag4 + Lag5 + ## Volume, family = binomial, data = Weekly) ## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -1.6949 -1.2565 0.9913 1.0849 1.4579 ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) 0.26686 0.08593 3.106 0.0019 ** ## Lag1 -0.04127 0.02641 -1.563 0.1181 ## Lag2 0.05844 0.02686 2.175 0.0296 * ## Lag3 -0.01606 0.02666 -0.602 0.5469 ## Lag4 -0.02779 0.02646 -1.050 0.2937 ## Lag5 -0.01447 0.02638 -0.549 0.5833 ## Volume -0.02274 0.03690 -0.616 0.5377 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for binomial family taken to be 1) ## ## Null deviance: 1496.2 on 1088 degrees of freedom ## Residual deviance: 1486.4 on 1082 degrees of freedom ## AIC: 1500.4 ## ## Number of Fisher Scoring iterations: 4 Lag2 is significant. Compute the confusion matrix and overall fraction of correct predictions. Explain what the confusion matrix is telling you about the types of mistakes made by logistic regression. contrasts(Weekly$Direction) ## Up ## Down 0 ## Up 1 pred &lt;- predict(fit, type = &quot;response&quot;) &gt; 0.5 (t &lt;- table(ifelse(pred, &quot;Up (pred)&quot;, &quot;Down (pred)&quot;), Weekly$Direction)) ## ## Down Up ## Down (pred) 54 48 ## Up (pred) 430 557 sum(diag(t)) / sum(t) ## [1] 0.5610652 The overall fraction of correct predictions is 0.56. Although logistic regression correctly predicts upwards movements well, it incorrectly predicts most downwards movements as up. Now fit the logistic regression model using a training data period from 1990 to 2008, with Lag2 as the only predictor. Compute the confusion matrix and the overall fraction of correct predictions for the held out data (that is, the data from 2009 and 2010). train &lt;- Weekly$Year &lt; 2009 fit &lt;- glm(Direction ~ Lag2, data = Weekly[train, ], family = binomial) pred &lt;- predict(fit, Weekly[!train, ], type = &quot;response&quot;) &gt; 0.5 (t &lt;- table(ifelse(pred, &quot;Up (pred)&quot;, &quot;Down (pred)&quot;), Weekly[!train, ]$Direction)) ## ## Down Up ## Down (pred) 9 5 ## Up (pred) 34 56 sum(diag(t)) / sum(t) ## [1] 0.625 Repeat (d) using LDA. fit &lt;- lda(Direction ~ Lag2, data = Weekly[train, ]) pred &lt;- predict(fit, Weekly[!train, ], type = &quot;response&quot;)$class (t &lt;- table(pred, Weekly[!train, ]$Direction)) ## ## pred Down Up ## Down 9 5 ## Up 34 56 sum(diag(t)) / sum(t) ## [1] 0.625 Repeat (d) using QDA. fit &lt;- qda(Direction ~ Lag2, data = Weekly[train, ]) pred &lt;- predict(fit, Weekly[!train, ], type = &quot;response&quot;)$class (t &lt;- table(pred, Weekly[!train, ]$Direction)) ## ## pred Down Up ## Down 0 0 ## Up 43 61 sum(diag(t)) / sum(t) ## [1] 0.5865385 Repeat (d) using KNN with \\(K = 1\\). fit &lt;- knn( Weekly[train, &quot;Lag2&quot;, drop = FALSE], Weekly[!train, &quot;Lag2&quot;, drop = FALSE], Weekly$Direction[train] ) (t &lt;- table(fit, Weekly[!train, ]$Direction)) ## ## fit Down Up ## Down 21 29 ## Up 22 32 sum(diag(t)) / sum(t) ## [1] 0.5096154 Repeat (d) using naive Bayes. fit &lt;- naiveBayes(Direction ~ Lag2, data = Smarket, subset = train) pred &lt;- predict(fit, Weekly[!train, ], type = &quot;class&quot;) (t &lt;- table(pred, Weekly[!train, ]$Direction)) ## ## pred Down Up ## Down 27 29 ## Up 16 32 sum(diag(t)) / sum(t) ## [1] 0.5673077 Which of these methods appears to provide the best results on this data? Logistic regression and LDA are the best performing. Experiment with different combinations of predictors, including possible transformations and interactions, for each of the methods. Report the variables, method, and associated confusion matrix that appears to provide the best results on the held out data. Note that you should also experiment with values for \\(K\\) in the KNN classifier. fit &lt;- glm(Direction ~ Lag1, data = Weekly[train, ], family = binomial) pred &lt;- predict(fit, Weekly[!train, ], type = &quot;response&quot;) &gt; 0.5 mean(ifelse(pred, &quot;Up&quot;, &quot;Down&quot;) == Weekly[!train, ]$Direction) ## [1] 0.5673077 fit &lt;- glm(Direction ~ Lag3, data = Weekly[train, ], family = binomial) pred &lt;- predict(fit, Weekly[!train, ], type = &quot;response&quot;) &gt; 0.5 mean(ifelse(pred, &quot;Up&quot;, &quot;Down&quot;) == Weekly[!train, ]$Direction) ## [1] 0.5865385 fit &lt;- glm(Direction ~Lag4, data = Weekly[train, ], family = binomial) pred &lt;- predict(fit, Weekly[!train, ], type = &quot;response&quot;) &gt; 0.5 mean(ifelse(pred, &quot;Up&quot;, &quot;Down&quot;) == Weekly[!train, ]$Direction) ## [1] 0.5865385 fit &lt;- glm(Direction ~ Lag1 + Lag2 + Lag3 + Lag4, data = Weekly[train, ], family = binomial) pred &lt;- predict(fit, Weekly[!train, ], type = &quot;response&quot;) &gt; 0.5 mean(ifelse(pred, &quot;Up&quot;, &quot;Down&quot;) == Weekly[!train, ]$Direction) ## [1] 0.5865385 fit &lt;- glm(Direction ~ Lag1 * Lag2 * Lag3 * Lag4, data = Weekly[train, ], family = binomial) pred &lt;- predict(fit, Weekly[!train, ], type = &quot;response&quot;) &gt; 0.5 mean(ifelse(pred, &quot;Up&quot;, &quot;Down&quot;) == Weekly[!train, ]$Direction) ## [1] 0.5961538 fit &lt;- lda(Direction ~ Lag1 + Lag2 + Lag3 + Lag4,data = Weekly[train, ]) pred &lt;- predict(fit, Weekly[!train, ], type = &quot;response&quot;)$class mean(pred == Weekly[!train, ]$Direction) ## [1] 0.5769231 fit &lt;- qda(Direction ~ Lag1 + Lag2 + Lag3 + Lag4, data = Weekly[train, ]) pred &lt;- predict(fit, Weekly[!train, ], type = &quot;response&quot;)$class mean(pred == Weekly[!train, ]$Direction) ## [1] 0.5192308 fit &lt;- naiveBayes(Direction ~ Lag1 + Lag2 + Lag3 + Lag4, data = Weekly[train, ]) pred &lt;- predict(fit, Weekly[!train, ], type = &quot;class&quot;) mean(pred == Weekly[!train, ]$Direction) ## [1] 0.5096154 set.seed(1) res &lt;- sapply(1:30, function(k) { fit &lt;- knn( Weekly[train, 2:4, drop = FALSE], Weekly[!train, 2:4, drop = FALSE], Weekly$Direction[train], k = k ) mean(fit == Weekly[!train, ]$Direction) }) plot(1:30, res, type = &quot;o&quot;, xlab = &quot;k&quot;, ylab = &quot;Fraction correct&quot;) (k &lt;- which.max(res)) ## [1] 26 fit &lt;- knn( Weekly[train, 2:4, drop = FALSE], Weekly[!train, 2:4, drop = FALSE], Weekly$Direction[train], k = k ) table(fit, Weekly[!train, ]$Direction) ## ## fit Down Up ## Down 23 18 ## Up 20 43 mean(fit == Weekly[!train, ]$Direction) ## [1] 0.6346154 KNN using the first 3 Lag variables performs marginally better than logistic regression with Lag2 if we tune \\(k\\) to be \\(k = 26\\). 4.2.2 Question 14 In this problem, you will develop a model to predict whether a given car gets high or low gas mileage based on the Auto data set. Create a binary variable, mpg01, that contains a 1 if mpg contains a value above its median, and a 0 if mpg contains a value below its median. You can compute the median using the median() function. Note you may find it helpful to use the data.frame() function to create a single data set containing both mpg01 and the other Auto variables. x &lt;- cbind(Auto[, -1], data.frame(&quot;mpg01&quot; = Auto$mpg &gt; median(Auto$mpg))) Explore the data graphically in order to investigate the association between mpg01 and the other features. Which of the other features seem most likely to be useful in predicting mpg01? Scatterplots and boxplots may be useful tools to answer this question. Describe your findings. par(mfrow = c(2, 4)) for (i in 1:7) hist(x[, i], breaks = 20, main = colnames(x)[i]) par(mfrow = c(2, 4)) for (i in 1:7) boxplot(x[, i] ~ x$mpg01, main = colnames(x)[i]) pairs(x[, 1:7]) Most variables show an association with mpg01 category, and several variables are colinear. Split the data into a training set and a test set. set.seed(1) train &lt;- sample(seq_len(nrow(x)), nrow(x) * 2/3) Perform LDA on the training data in order to predict mpg01 using the variables that seemed most associated with mpg01 in (b). What is the test error of the model obtained? sort(sapply(1:7, function(i) { setNames(abs(t.test(x[, i] ~ x$mpg01)$statistic), colnames(x)[i]) })) ## acceleration year origin horsepower displacement weight ## 7.302430 9.403221 11.824099 17.681939 22.632004 22.932777 ## cylinders ## 23.035328 fit &lt;- lda(mpg01 ~ cylinders + weight + displacement, data = x[train, ]) pred &lt;- predict(fit, x[-train, ], type = &quot;response&quot;)$class mean(pred != x[-train, ]$mpg01) ## [1] 0.1068702 Perform QDA on the training data in order to predict mpg01 using the variables that seemed most associated with mpg01 in (b). What is the test error of the model obtained? fit &lt;- qda(mpg01 ~ cylinders + weight + displacement, data = x[train, ]) pred &lt;- predict(fit, x[-train, ], type = &quot;response&quot;)$class mean(pred != x[-train, ]$mpg01) ## [1] 0.09923664 Perform logistic regression on the training data in order to predict mpg01 using the variables that seemed most associated with mpg01 in (b). What is the test error of the model obtained? fit &lt;- glm(mpg01 ~ cylinders + weight + displacement, data = x[train, ], family = binomial) pred &lt;- predict(fit, x[-train, ], type = &quot;response&quot;) &gt; 0.5 mean(pred != x[-train, ]$mpg01) ## [1] 0.1145038 Perform naive Bayes on the training data in order to predict mpg01 using the variables that seemed most associated with mpg01 in (b). What is the test error of the model obtained? fit &lt;- naiveBayes(mpg01 ~ cylinders + weight + displacement, data = x[train, ]) pred &lt;- predict(fit, x[-train, ], type = &quot;class&quot;) mean(pred != x[-train, ]$mpg01) ## [1] 0.09923664 Perform KNN on the training data, with several values of \\(K\\), in order to predict mpg01. Use only the variables that seemed most associated with mpg01 in (b). What test errors do you obtain? Which value of \\(K\\) seems to perform the best on this data set? res &lt;- sapply(1:50, function(k) { fit &lt;- knn(x[train, c(1, 4, 2)], x[-train, c(1, 4, 2)], x$mpg01[train], k = k) mean(fit != x[-train, ]$mpg01) }) names(res) &lt;- 1:50 plot(res, type = &quot;o&quot;) res[which.min(res)] ## 3 ## 0.1068702 For the models tested here, \\(k = 32\\) appears to perform best. QDA has a lower error rate overall, performing slightly better than LDA. 4.2.3 Question 15 This problem involves writing functions. Write a function, Power(), that prints out the result of raising 2 to the 3rd power. In other words, your function should compute \\(2^3\\) and print out the results. Hint: Recall that x^a raises x to the power a. Use the print() function to output the result. Power &lt;- function() print(2^3) Create a new function, Power2(), that allows you to pass any two numbers, x and a, and prints out the value of x^a. You can do this by beginning your function with the line &gt; Power2=function(x,a) { You should be able to call your function by entering, for instance, &gt; Power2(3, 8) on the command line. This should output the value of \\(3^8\\), namely, 6,561. Power2 &lt;- function(x, a) print(x^a) Using the Power2() function that you just wrote, compute \\(10^3\\), \\(8^{17}\\), and \\(131^3\\). c(Power2(10, 3), Power2(8, 17), Power2(131, 3)) ## [1] 1000 ## [1] 2.2518e+15 ## [1] 2248091 ## [1] 1.000000e+03 2.251800e+15 2.248091e+06 Now create a new function, Power3(), that actually returns the result x^a as an R object, rather than simply printing it to the screen. That is, if you store the value x^a in an object called result within your function, then you can simply return() this result, using the following line: &gt; return(result) The line above should be the last line in your function, before the } symbol. Power3 &lt;- function(x, a) { result &lt;- x^a return(result) } Now using the Power3() function, create a plot of \\(f(x) = x^2\\). The \\(x\\)-axis should display a range of integers from 1 to 10, and the \\(y\\)-axis should display \\(x^2\\). Label the axes appropriately, and use an appropriate title for the figure. Consider displaying either the \\(x\\)-axis, the \\(y\\)-axis, or both on the log-scale. You can do this by using log = \"x\", log = \"y\", or log = \"xy\" as arguments to the plot() function. plot(1:10, Power3(1:10, 2), xlab = &quot;x&quot;, ylab = expression(paste(&quot;x&quot;^&quot;2&quot;)), log = &quot;y&quot; ) Create a function, PlotPower(), that allows you to create a plot of x against x^a for a fixed a and for a range of values of x. For instance, if you call &gt; PlotPower(1:10, 3) then a plot should be created with an \\(x\\)-axis taking on values \\(1,2,...,10\\), and a \\(y\\)-axis taking on values \\(1^3,2^3,...,10^3\\). PlotPower &lt;- function(x, a, log = &quot;y&quot;) { plot(x, Power3(x, a), xlab = &quot;x&quot;, ylab = substitute(&quot;x&quot;^a, list(a = a)), log = log ) } PlotPower(1:10, 3) 4.2.4 Question 13 Using the Boston data set, fit classification models in order to predict whether a given census tract has a crime rate above or below the median. Explore logistic regression, LDA, naive Bayes and KNN models using various sub-sets of the predictors. Describe your findings. Hint: You will have to create the response variable yourself, using the variables that are contained in the Boston data set. x &lt;- cbind( ISLR2::Boston[, -1], data.frame(&quot;highcrim&quot; = Boston$crim &gt; median(Boston$crim)) ) set.seed(1) train &lt;- sample(seq_len(nrow(x)), nrow(x) * 2/3) We can find the most associated variables by performing wilcox tests. ord &lt;- order(sapply(1:12, function(i) { p &lt;- wilcox.test(as.numeric(x[train, i]) ~ x[train, ]$highcrim)$p.value setNames(log10(p), colnames(x)[i]) })) ord &lt;- names(x)[ord] ord ## [1] &quot;nox&quot; &quot;dis&quot; &quot;indus&quot; &quot;tax&quot; &quot;age&quot; &quot;rad&quot; &quot;zn&quot; ## [8] &quot;lstat&quot; &quot;medv&quot; &quot;ptratio&quot; &quot;rm&quot; &quot;chas&quot; Variables nox (nitrogen oxides concentration) followed by dis (distance to employment center) appear to be most associated with high crime. Let’s reorder columns by those most associated with highcrim (in the training data) x &lt;- x[, c(ord, &quot;highcrim&quot;)] Let’s look at univariate associations with highcrim (in the training data) x[train, ] |&gt; pivot_longer(!highcrim) |&gt; mutate(name = factor(name, levels = ord)) |&gt; ggplot(aes(highcrim, value)) + geom_boxplot() + facet_wrap(~name, scale = &quot;free&quot;) Fit lda, logistic regression, naive Bayes and KNN models (with k = 1..50) for a set of specific predictors and return the error rate. We fit models using increasing numbers of predictors: column 1, then columns 1 and 2 etc. fit_models &lt;- function(cols, k_vals = 1:50) { dat_train &lt;- x[train, cols, drop = FALSE] dat_test &lt;- x[-train, cols, drop = FALSE] fit &lt;- lda(x$highcrim[train] ~ ., data = dat_train) pred &lt;- predict(fit, dat_test, type = &quot;response&quot;)$class lda_err &lt;- mean(pred != x$highcrim[-train]) fit &lt;- glm(x$highcrim[train] ~ ., data = dat_train, family = binomial) pred &lt;- predict(fit, dat_test, type = &quot;response&quot;) &gt; 0.5 logreg_err &lt;- mean(pred != x$highcrim[-train]) fit &lt;- naiveBayes(x$highcrim[train] ~ ., data = dat_train) pred &lt;- predict(fit, dat_test, type = &quot;class&quot;) nb_err &lt;- mean(pred != x$highcrim[-train]) res &lt;- sapply(k_vals, function(k) { fit &lt;- knn(dat_train, dat_test, x$highcrim[train], k = k) mean(fit != x$highcrim[-train]) }) knn_err &lt;- min(res) c(&quot;LDA&quot; = lda_err, &quot;LR&quot; = logreg_err, &quot;NB&quot; = nb_err, &quot;KNN&quot; = knn_err) } res &lt;- sapply(1:12, function(max) fit_models(1:max)) res &lt;- as_tibble(t(res)) res$n_var &lt;- 1:12 pivot_longer(res, cols = !n_var) |&gt; ggplot(aes(n_var, value, col = name)) + geom_line() + xlab(&quot;Number of predictors&quot;) + ylab(&quot;Error rate&quot;) KNN appears to perform better (if we tune \\(k\\)) for all numbers of predictors. fit &lt;- knn( x[train, &quot;nox&quot;, drop = FALSE], x[-train, &quot;nox&quot;, drop = FALSE], x$highcrim[train], k = 1 ) table(fit, x[-train, ]$highcrim) ## ## fit FALSE TRUE ## FALSE 78 2 ## TRUE 3 86 mean(fit != x[-train, ]$highcrim) * 100 ## [1] 2.95858 Surprisingly, the best model (with an error rate of &lt;5%) uses \\(k = 1\\) and assigns crime rate categories based on the town with the single most similar nitrogen oxide concentration (nox). This might be, for example, because nearby towns have similar crime rates, and we can obtain good predictions by predicting crime rate based on a nearby town. But what if we only consider \\(k = 20\\). res &lt;- sapply(1:12, function(max) fit_models(1:max, k_vals = 20)) res &lt;- as_tibble(t(res)) res$n_var &lt;- 1:12 pivot_longer(res, cols = !n_var) |&gt; ggplot(aes(n_var, value, col = name)) + geom_line() + xlab(&quot;Number of predictors&quot;) + ylab(&quot;Error rate&quot;) KNN still performs best with a single predictor (nox), but logistic regression with 12 predictors also performs well and has an error rate of ~12%. vars &lt;- names(x)[1:12] dat_train &lt;- x[train, vars] dat_test &lt;- x[-train, vars] fit &lt;- glm(x$highcrim[train] ~ ., data = dat_train, family = binomial) pred &lt;- predict(fit, dat_test, type = &quot;response&quot;) &gt; 0.5 table(pred, x[-train, ]$highcrim) ## ## pred FALSE TRUE ## FALSE 70 9 ## TRUE 11 79 mean(pred != x$highcrim[-train]) * 100 ## [1] 11.83432 summary(fit) ## ## Call: ## glm(formula = x$highcrim[train] ~ ., family = binomial, data = dat_train) ## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -2.3277 -0.1603 -0.0010 0.0015 3.5339 ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) -44.525356 7.935621 -5.611 2.01e-08 *** ## nox 55.062428 10.281556 5.355 8.53e-08 *** ## dis 1.080847 0.304084 3.554 0.000379 *** ## indus -0.067493 0.058547 -1.153 0.248997 ## tax -0.005336 0.003138 -1.700 0.089060 . ## age 0.020965 0.014190 1.477 0.139556 ## rad 0.678196 0.192193 3.529 0.000418 *** ## zn -0.099558 0.045914 -2.168 0.030134 * ## lstat 0.134035 0.058623 2.286 0.022231 * ## medv 0.213114 0.088922 2.397 0.016547 * ## ptratio 0.294396 0.155285 1.896 0.057981 . ## rm -0.518115 0.896423 -0.578 0.563278 ## chas 0.139557 0.798632 0.175 0.861280 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for binomial family taken to be 1) ## ## Null deviance: 467.04 on 336 degrees of freedom ## Residual deviance: 135.80 on 324 degrees of freedom ## AIC: 161.8 ## ## Number of Fisher Scoring iterations: 9 "],["resampling-methods.html", "5 Resampling Methods 5.1 Conceptual 5.2 Applied", " 5 Resampling Methods 5.1 Conceptual 5.1.1 Question 1 Using basic statistical properties of the variance, as well as single- variable calculus, derive (5.6). In other words, prove that \\(\\alpha\\) given by (5.6) does indeed minimize \\(Var(\\alpha X + (1 − \\alpha)Y)\\). Equation 5.6 is: \\[ \\alpha = \\frac{\\sigma^2_Y - \\sigma_{XY}}{\\sigma^2_X + \\sigma^2_Y - 2\\sigma_{XY}} \\] Remember that: \\[ Var(aX) = a^2Var(X), \\\\ \\mathrm{Var}(X + Y) = \\mathrm{Var}(X) + \\mathrm{Var}(Y) + \\mathrm{Cov}(X,Y), \\\\ \\mathrm{Cov}(aX, bY) = ab\\mathrm{Cov}(X, Y) \\] If we define \\(\\sigma^2_X = \\mathrm{Var}(X)\\), \\(\\sigma^2_Y = \\mathrm{Var}(Y)\\) and \\(\\sigma_{XY} = \\mathrm{Cov}(X, Y)\\) \\[\\begin{align} Var(\\alpha X + (1 - \\alpha)Y) &amp;= \\alpha^2\\sigma^2_X + (1-\\alpha)^2\\sigma^2_Y + 2\\alpha(1 - \\alpha)\\sigma_{XY} \\\\ &amp;= \\alpha^2\\sigma^2_X + \\sigma^2_Y - 2\\alpha\\sigma^2_Y + \\alpha^2\\sigma^2_Y + 2\\alpha\\sigma_{XY} - 2\\alpha^2\\sigma_{XY} \\end{align}\\] Now we want to find when the rate of change of this function is 0 with respect to \\(\\alpha\\), so we compute the partial derivative, set to 0 and solve. \\[ \\frac{\\partial}{\\partial{\\alpha}} = 2\\alpha\\sigma^2_X - 2\\sigma^2_Y + 2\\alpha\\sigma^2_Y + 2\\sigma_{XY} - 4\\alpha\\sigma_{XY} = 0 \\] Moving \\(\\alpha\\) terms to the same side: \\[ \\alpha\\sigma^2_X + \\alpha\\sigma^2_Y - 2\\alpha\\sigma_{XY} = \\sigma^2_Y - \\sigma_{XY} \\] \\[ \\alpha = \\frac{\\sigma^2_Y - \\sigma_{XY}}{\\sigma^2_X + \\sigma^2_Y - 2\\sigma_{XY}} \\] We should also show that this is a minimum, so that the second partial derivative wrt \\(\\alpha\\) is \\(&gt;= 0\\). \\[\\begin{align} \\frac{\\partial^2}{\\partial{\\alpha^2}} &amp;= 2\\sigma^2_X + 2\\sigma^2_Y - 4\\sigma_{XY} \\\\ &amp;= 2(\\sigma^2_X + \\sigma^2_Y - 2\\sigma_{XY}) \\\\ &amp;= 2\\mathrm{Var}(X - Y) \\end{align}\\] Since variance is positive, then this must be positive. 5.1.2 Question 2 We will now derive the probability that a given observation is part of a bootstrap sample. Suppose that we obtain a bootstrap sample from a set of n observations. What is the probability that the first bootstrap observation is not the \\(j\\)th observation from the original sample? Justify your answer. This is 1 - probability that it is the \\(j\\)th = \\(1 - 1/n\\). What is the probability that the second bootstrap observation is not the \\(j\\)th observation from the original sample? Since each bootstrap observation is a random sample, this probability is the same (\\(1 - 1/n\\)). Argue that the probability that the \\(j\\)th observation is not in the bootstrap sample is \\((1 − 1/n)^n\\). For the \\(j\\)th observation to not be in the sample, it would have to not be picked for each of \\(n\\) positions, so not picked for \\(1, 2, ..., n\\), thus the probability is \\((1 - 1/n)^n\\) When \\(n = 5\\), what is the probability that the \\(j\\)th observation is in the bootstrap sample? n &lt;- 5 1 - (1 - 1/n)^n ## [1] 0.67232 \\(p = 0.67\\) When \\(n = 100\\), what is the probability that the \\(j\\)th observation is in the bootstrap sample? n &lt;- 100 1 - (1 - 1/n)^n ## [1] 0.6339677 \\(p = 0.64\\) When \\(n = 10,000\\), what is the probability that the \\(j\\)th observation is in the bootstrap sample? n &lt;- 100000 1 - (1 - 1/n)^n ## [1] 0.6321224 \\(p = 0.63\\) Create a plot that displays, for each integer value of \\(n\\) from 1 to 100,000, the probability that the \\(j\\)th observation is in the bootstrap sample. Comment on what you observe. x &lt;- sapply(1:100000, function(n) 1 - (1 - 1/n)^n) plot(x, log = &quot;x&quot;, type = &quot;o&quot;) The probability rapidly approaches 0.63 with increasing \\(n\\). Note that \\[e^x = \\lim_{x \\to \\inf} \\left(1 + \\frac{x}{n}\\right)^n,\\] so with \\(x = -1\\), we can see that our limit is \\(1 - e^{-1} = 1 - 1/e\\). We will now investigate numerically the probability that a bootstrap sample of size \\(n = 100\\) contains the \\(j\\)th observation. Here \\(j = 4\\). We repeatedly create bootstrap samples, and each time we record whether or not the fourth observation is contained in the bootstrap sample. &gt; store &lt;- rep (NA, 10000) &gt; for (i in 1:10000) { store[i] &lt;- sum(sample(1:100, rep = TRUE) == 4) &gt; 0 } &gt; mean(store) Comment on the results obtained. store &lt;- replicate(10000, sum(sample(1:100, replace = TRUE) == 4) &gt; 0) mean(store) ## [1] 0.6319 The probability of including \\(4\\) when resampling numbers \\(1...100\\) is close to \\(1 - (1 - 1/100)^{100}\\). 5.1.3 Question 3 We now review \\(k\\)-fold cross-validation. Explain how \\(k\\)-fold cross-validation is implemented. We divided our data into (approximately equal) \\(k\\) subsets, and then generate predictions for each \\(k\\)th set, training on the exclusive \\(k\\) sets combined. What are the advantages and disadvantages of \\(k\\)-fold cross-validation relative to: The validation set approach? LOOCV? When using a validation set, we can only train on a small portion of the data as we must reserve the rest for validation. As a result it can overestimate the test error rate (assuming we then train using the complete data for future prediction). It is also sensitive to which observations are including in train vs. test. It is, however, low cost in terms of processing time (as we only have to fit one model). When using LOOCV, we can train on \\(n-1\\) observations, however, the trained models we generate each differ only by the inclusion (and exclusion) of a single observation. As a result, LOOCV can have high variance (the models fit will be similar, and might be quite different to what we would obtain with a different data set). LOOCV is also costly in terms of processing time. 5.1.4 Question 4 Suppose that we use some statistical learning method to make a prediction for the response \\(Y\\) for a particular value of the predictor \\(X\\). Carefully describe how we might estimate the standard deviation of our prediction. We could address this with bootstrapping. Our procedure would be to (jointly) resample \\(Y\\) and \\(X\\) variables and fit our model many times. For each model we could obtain a summary of our prediction and calculate the standard deviation over bootstrapped samples. 5.2 Applied 5.2.1 Question 5 In Chapter 4, we used logistic regression to predict the probability of default using income and balance on the Default data set. We will now estimate the test error of this logistic regression model using the validation set approach. Do not forget to set a random seed before beginning your analysis. Fit a logistic regression model that uses income and balance to predict default. library(ISLR2) set.seed(42) fit &lt;- glm(default ~ income + balance, data = Default, family = &quot;binomial&quot;) Using the validation set approach, estimate the test error of this model. In order to do this, you must perform the following steps: Split the sample set into a training set and a validation set. Fit a multiple logistic regression model using only the training observations. Obtain a prediction of default status for each individual in the validation set by computing the posterior probability of default for that individual, and classifying the individual to the default category if the posterior probability is greater than 0.5. Compute the validation set error, which is the fraction of the observations in the validation set that are misclassified. train &lt;- sample(nrow(Default), nrow(Default) / 2) fit &lt;- glm(default ~ income + balance, data = Default, family = &quot;binomial&quot;, subset = train) pred &lt;- ifelse(predict(fit, newdata = Default[-train, ], type = &quot;response&quot;) &gt; 0.5, &quot;Yes&quot;, &quot;No&quot;) table(pred, Default$default[-train]) ## ## pred No Yes ## No 4817 110 ## Yes 20 53 mean(pred != Default$default[-train]) ## [1] 0.026 Repeat the process in (b) three times, using three different splits of the observations into a training set and a validation set. Comment on the results obtained. replicate(3, { train &lt;- sample(nrow(Default), nrow(Default) / 2) fit &lt;- glm(default ~ income + balance, data = Default, family = &quot;binomial&quot;, subset = train) pred &lt;- ifelse(predict(fit, newdata = Default[-train, ], type = &quot;response&quot;) &gt; 0.5, &quot;Yes&quot;, &quot;No&quot;) mean(pred != Default$default[-train]) }) ## [1] 0.0260 0.0294 0.0258 The results obtained are variable and depend on the samples allocated to training vs. test. Now consider a logistic regression model that predicts the probability of default using income, balance, and a dummy variable for student. Estimate the test error for this model using the validation set approach. Comment on whether or not including a dummy variable for student leads to a reduction in the test error rate. replicate(3, { train &lt;- sample(nrow(Default), nrow(Default) / 2) fit &lt;- glm(default ~ income + balance + student, data = Default, family = &quot;binomial&quot;, subset = train) pred &lt;- ifelse(predict(fit, newdata = Default[-train, ], type = &quot;response&quot;) &gt; 0.5, &quot;Yes&quot;, &quot;No&quot;) mean(pred != Default$default[-train]) }) ## [1] 0.0278 0.0256 0.0250 Including student does not seem to make a substantial improvement to the test error. 5.2.2 Question 6 We continue to consider the use of a logistic regression model to predict the probability of default using income and balance on the Default data set. In particular, we will now compute estimates for the standard errors of the income and balance logistic regression coefficients in two different ways: (1) using the bootstrap, and (2) using the standard formula for computing the standard errors in the glm() function. Do not forget to set a random seed before beginning your analysis. Using the summary() and glm() functions, determine the estimated standard errors for the coefficients associated with income and balance in a multiple logistic regression model that uses both predictors. fit &lt;- glm(default ~ income + balance, data = Default, family = &quot;binomial&quot;) summary(fit) ## ## Call: ## glm(formula = default ~ income + balance, family = &quot;binomial&quot;, ## data = Default) ## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -2.4725 -0.1444 -0.0574 -0.0211 3.7245 ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) -1.154e+01 4.348e-01 -26.545 &lt; 2e-16 *** ## income 2.081e-05 4.985e-06 4.174 2.99e-05 *** ## balance 5.647e-03 2.274e-04 24.836 &lt; 2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for binomial family taken to be 1) ## ## Null deviance: 2920.6 on 9999 degrees of freedom ## Residual deviance: 1579.0 on 9997 degrees of freedom ## AIC: 1585 ## ## Number of Fisher Scoring iterations: 8 The standard errors obtained by bootstrapping are \\(\\beta_1\\) = 5.0e-6 and \\(\\beta_2\\) = 2.3e-4. Write a function, boot.fn(), that takes as input the Default data set as well as an index of the observations, and that outputs the coefficient estimates for income and balance in the multiple logistic regression model. boot.fn &lt;- function(x, i) { fit &lt;- glm(default ~ income + balance, data = Default[i, ], family = &quot;binomial&quot;) coef(fit)[-1] } Use the boot() function together with your boot.fn() function to estimate the standard errors of the logistic regression coefficients for income and balance. library(boot) set.seed(42) boot(Default, boot.fn, R = 1000) ## ## ORDINARY NONPARAMETRIC BOOTSTRAP ## ## ## Call: ## boot(data = Default, statistic = boot.fn, R = 1000) ## ## ## Bootstrap Statistics : ## original bias std. error ## t1* 2.080898e-05 2.737444e-08 5.073444e-06 ## t2* 5.647103e-03 1.176249e-05 2.299133e-04 Comment on the estimated standard errors obtained using the glm() function and using your bootstrap function. The standard errors obtained by bootstrapping are similar to those estimated by glm. 5.2.3 Question 7 In Sections 5.3.2 and 5.3.3, we saw that the cv.glm() function can be used in order to compute the LOOCV test error estimate. Alternatively, one could compute those quantities using just the glm() and predict.glm() functions, and a for loop. You will now take this approach in order to compute the LOOCV error for a simple logistic regression model on the Weekly data set. Recall that in the context of classification problems, the LOOCV error is given in (5.4). Fit a logistic regression model that predicts Direction using Lag1 and Lag2. fit &lt;- glm(Direction ~ Lag1 + Lag2, data = Weekly, family = &quot;binomial&quot;) Fit a logistic regression model that predicts Direction using Lag1 and Lag2 using all but the first observation. fit &lt;- glm(Direction ~ Lag1 + Lag2, data = Weekly[-1, ], family = &quot;binomial&quot;) Use the model from (b) to predict the direction of the first observation. You can do this by predicting that the first observation will go up if \\(P(\\)Direction=\"Up\" | Lag1 , Lag2\\() &gt; 0.5\\). Was this observation correctly classified? predict(fit, newdata = Weekly[1, , drop = FALSE], type = &quot;response&quot;) &gt; 0.5 ## 1 ## TRUE Yes the observation was correctly classified. Write a for loop from \\(i = 1\\) to \\(i = n\\), where \\(n\\) is the number of observations in the data set, that performs each of the following steps: Fit a logistic regression model using all but the \\(i\\)th observation to predict Direction using Lag1 and Lag2 . Compute the posterior probability of the market moving up for the \\(i\\)th observation. Use the posterior probability for the \\(i\\)th observation in order to predict whether or not the market moves up. Determine whether or not an error was made in predicting the direction for the \\(i\\)th observation. If an error was made, then indicate this as a 1, and otherwise indicate it as a 0. error &lt;- numeric(nrow(Weekly)) for (i in 1:nrow(Weekly)) { fit &lt;- glm(Direction ~ Lag1 + Lag2, data = Weekly[-i, ], family = &quot;binomial&quot;) p &lt;- predict(fit, newdata = Weekly[i, , drop = FALSE], type = &quot;response&quot;) &gt; 0.5 error[i] &lt;- ifelse(p, &quot;Down&quot;, &quot;Up&quot;) == Weekly$Direction[i] } Take the average of the \\(n\\) numbers obtained in (d) in order to obtain the LOOCV estimate for the test error. Comment on the results. mean(error) ## [1] 0.4499541 The LOOCV test error rate is 45% which implies that our predictions are marginally more often correct than not. 5.2.4 Question 8 We will now perform cross-validation on a simulated data set. Generate a simulated data set as follows: &gt; set.seed(1) &gt; x &lt;- rnorm(100) &gt; y &lt;- x - 2 *x^2 + rnorm(100) In this data set, what is \\(n\\) and what is \\(p\\)? Write out the model used to generate the data in equation form. set.seed(1) x &lt;- rnorm(100) y &lt;- x - 2 * x^2 + rnorm(100) \\(n\\) is 100 and \\(p\\) is 1 (there are 100 observations and \\(y\\) is predicted with a single variable \\(x\\)). The model equation is: \\[y = -2x^2 + x + \\epsilon\\]. Create a scatterplot of \\(X\\) against \\(Y\\). Comment on what you find. plot(x, y) \\(y\\) has a (negative) quadratic relationship with \\(x\\). Set a random seed, and then compute the LOOCV errors that result from fitting the following four models using least squares: \\(Y = \\beta_0 + \\beta_1 X + \\epsilon\\) \\(Y = \\beta_0 + \\beta_1 X + \\beta_2 X^2 + \\epsilon\\) \\(Y = \\beta_0 + \\beta_1 X + \\beta_2 X^2 + \\beta_3 X^3 + \\epsilon\\) \\(Y = \\beta_0 + \\beta_1 X + \\beta_2 X^2 + \\beta_3 X^3 + \\beta_4 X^4 + \\epsilon\\). Note you may find it helpful to use the data.frame() function to create a single data set containing both \\(X\\) and \\(Y\\). library(boot) set.seed(42) dat &lt;- data.frame(x, y) sapply(1:4, function(i) cv.glm(dat, glm(y ~ poly(x, i)))$delta[1]) ## [1] 7.2881616 0.9374236 0.9566218 0.9539049 Repeat (c) using another random seed, and report your results. Are your results the same as what you got in (c)? Why? set.seed(43) dat &lt;- data.frame(x, y) sapply(1:4, function(i) cv.glm(dat, glm(y ~ poly(x, i)))$delta[1]) ## [1] 7.2881616 0.9374236 0.9566218 0.9539049 The results are the same because we are using LOOCV. When doing this, the model is fit leaving each one of the observations out in turn, and thus there is no stochasticity involved. Which of the models in (c) had the smallest LOOCV error? Is this what you expected? Explain your answer. The second model had the smallest LOOCV. This what would be expected since the model to generate the data was quadratic and we are measuring the test (rather than training) error rate to evaluate performance. Comment on the statistical significance of the coefficient estimates that results from fitting each of the models in (c) using least squares. Do these results agree with the conclusions drawn based on the cross-validation results? for (i in 1:4) printCoefmat(coef(summary(glm(y ~ poly(x, i), data = dat)))) ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -1.55002 0.26001 -5.9613 3.954e-08 *** ## poly(x, i) 6.18883 2.60014 2.3802 0.01924 * ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -1.550023 0.095803 -16.1792 &lt; 2.2e-16 *** ## poly(x, i)1 6.188826 0.958032 6.4599 4.185e-09 *** ## poly(x, i)2 -23.948305 0.958032 -24.9974 &lt; 2.2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -1.550023 0.096263 -16.1019 &lt; 2.2e-16 *** ## poly(x, i)1 6.188826 0.962632 6.4291 4.972e-09 *** ## poly(x, i)2 -23.948305 0.962632 -24.8779 &lt; 2.2e-16 *** ## poly(x, i)3 0.264106 0.962632 0.2744 0.7844 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -1.550023 0.095905 -16.1620 &lt; 2.2e-16 *** ## poly(x, i)1 6.188826 0.959051 6.4531 4.591e-09 *** ## poly(x, i)2 -23.948305 0.959051 -24.9708 &lt; 2.2e-16 *** ## poly(x, i)3 0.264106 0.959051 0.2754 0.7836 ## poly(x, i)4 1.257095 0.959051 1.3108 0.1931 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 We can see that the coefficients in the first model are not highly significant, but all terms (\\(\\beta_0, \\beta_1\\) and \\(\\beta_2\\)) are in the quadratic model. After this, subsequent \\(\\beta_n\\) terms are not significant. Therefore, these results agree with those from cross-validation. 5.2.5 Question 9 We will now consider the Boston housing data set, from the ISLR2 library. Based on this data set, provide an estimate for the population mean of medv. Call this estimate \\(\\hat\\mu\\). (mu &lt;- mean(Boston$medv)) ## [1] 22.53281 Provide an estimate of the standard error of \\(\\hat\\mu\\). Interpret this result. Hint: We can compute the standard error of the sample mean by dividing the sample standard deviation by the square root of the number of observations. sd(Boston$medv) / sqrt(length(Boston$medv)) ## [1] 0.4088611 Now estimate the standard error of \\(\\hat\\mu\\) using the bootstrap. How does this compare to your answer from (b)? set.seed(42) (bs &lt;- boot(Boston$medv, function(v, i) mean(v[i]), 10000)) ## ## ORDINARY NONPARAMETRIC BOOTSTRAP ## ## ## Call: ## boot(data = Boston$medv, statistic = function(v, i) mean(v[i]), ## R = 10000) ## ## ## Bootstrap Statistics : ## original bias std. error ## t1* 22.53281 0.002175751 0.4029139 The standard error using the bootstrap (0.403) is very close to that obtained from the formula above (0.409). Based on your bootstrap estimate from (c), provide a 95% confidence interval for the mean of medv. Compare it to the results obtained using t.test(Boston$medv). Hint: You can approximate a 95% confidence interval using the formula \\([\\hat\\mu − 2SE(\\hat\\mu), \\hat\\mu + 2SE(\\hat\\mu)].\\) se &lt;- sd(bs$t) c(mu - 2*se, mu + 2*se) ## [1] 21.72698 23.33863 Based on this data set, provide an estimate, \\(\\hat\\mu_{med}\\), for the median value of medv in the population. median(Boston$medv) ## [1] 21.2 We now would like to estimate the standard error of \\(\\hat\\mu_{med}\\). Unfortunately, there is no simple formula for computing the standard error of the median. Instead, estimate the standard error of the median using the bootstrap. Comment on your findings. set.seed(42) boot(Boston$medv, function(v, i) median(v[i]), 10000) ## ## ORDINARY NONPARAMETRIC BOOTSTRAP ## ## ## Call: ## boot(data = Boston$medv, statistic = function(v, i) median(v[i]), ## R = 10000) ## ## ## Bootstrap Statistics : ## original bias std. error ## t1* 21.2 -0.01331 0.3744634 The estimated standard error of the median is 0.374. This is lower than the standard error of the mean. Based on this data set, provide an estimate for the tenth percentile of medv in Boston census tracts. Call this quantity \\(\\hat\\mu_{0.1}\\). (You can use the quantile() function.) quantile(Boston$medv, 0.1) ## 10% ## 12.75 Use the bootstrap to estimate the standard error of \\(\\hat\\mu_{0.1}\\). Comment on your findings. set.seed(42) boot(Boston$medv, function(v, i) quantile(v[i], 0.1), 10000) ## ## ORDINARY NONPARAMETRIC BOOTSTRAP ## ## ## Call: ## boot(data = Boston$medv, statistic = function(v, i) quantile(v[i], ## 0.1), R = 10000) ## ## ## Bootstrap Statistics : ## original bias std. error ## t1* 12.75 0.013405 0.497298 We get a standard error of ~0.5. This is higher than the standard error of the median. Nevertheless the standard error is quite small, thus we can be fairly confidence about the value of the 10th percentile. "],["linear-model-selection-and-regularization.html", "6 Linear Model Selection and Regularization 6.1 Conceptual 6.2 Applied", " 6 Linear Model Selection and Regularization 6.1 Conceptual 6.1.1 Question 1 We perform best subset, forward stepwise, and backward stepwise selection on a single data set. For each approach, we obtain \\(p + 1\\) models, containing \\(0, 1, 2, ..., p\\) predictors. Explain your answers: Which of the three models with \\(k\\) predictors has the smallest training RSS? Best subset considers the most models (all possible combinations of \\(p\\) predictors are considered), therefore this will give the smallest training RSS (it will at least consider all possibilities covered by forward and backward stepwise selection). However, all three approaches are expected to give similar if not identical results in practice. Which of the three models with \\(k\\) predictors has the smallest test RSS? We cannot tell which model will perform best on the test RSS. The answer will depend on the tradeoff between fitting to the data and overfitting. True or False: The predictors in the \\(k\\)-variable model identified by forward stepwise are a subset of the predictors in the (\\(k+1\\))-variable model identified by forward stepwise selection. True. Forward stepwise selection retains all features identified in previous models as \\(k\\) is increased. The predictors in the \\(k\\)-variable model identified by backward stepwise are a subset of the predictors in the \\((k+1)\\)-variable model identified by backward stepwise selection. True. Backward stepwise selection removes features one by one as \\(k\\) is decreased. The predictors in the \\(k\\)-variable model identified by backward stepwise are a subset of the predictors in the \\((k+1)\\)-variable model identified by forward stepwise selection. False. Forward and backward stepwise selection can identify different combinations of variables due to differing algorithms. The predictors in the \\(k\\)-variable model identified by forward stepwise are a subset of the predictors in the \\((k+1)\\)-variable model identified by backward stepwise selection. False. Forward and backward stepwise selection can identify different combinations of variables due to differing algorithms. The predictors in the \\(k\\)-variable model identified by best subset are a subset of the predictors in the \\((k+1)\\)-variable model identified by best subset selection. False. Best subset selection can identify different combinations of variables for each \\(k\\) by considering all possible models. 6.1.2 Question 2 For parts (a) through (c), indicate which of i. through iv. is correct. Justify your answer. The lasso, relative to least squares, is: More flexible and hence will give improved prediction accuracy when its increase in bias is less than its decrease in variance. More flexible and hence will give improved prediction accuracy when its increase in variance is less than its decrease in bias. Less flexible and hence will give improved prediction accuracy when its increase in bias is less than its decrease in variance. Less flexible and hence will give improved prediction accuracy when its increase in variance is less than its decrease in bias. By using shrinkage, lasso can reduce the number of predictors so is less flexible. As a result, it will lead to an increase in bias by approximating the true relationship. We hope that this increase is small but that we dramatically reduce variance (i.e. the difference we would see in the model fit between different sets of training data). Repeat (a) for ridge regression relative to least squares. The same is true of ridge regression—shrinkage results in a less flexible model and can reduce variance. Repeat (a) for non-linear methods relative to least squares. Non-linear methods can be more flexible. They can perform better as long as they don’t substantially increase variance. 6.1.3 Question 3 Suppose we estimate the regression coefficients in a linear regression model by minimizing: \\[ \\sum_{i=1}^n\\left(y_i - \\beta_0 - \\sum_{j=1}^p\\beta_jx_{ij}\\right)^2 \\textrm{subject to} \\sum_{j=1}^p|\\beta_j| \\le s \\] for a particular value of \\(s\\). For parts (a) through (e), indicate which of i. through v. is correct. Justify your answer. As we increase \\(s\\) from 0, the training RSS will: Increase initially, and then eventually start decreasing in an inverted U shape. Decrease initially, and then eventually start increasing in a U shape. Steadily increase. Steadily decrease. Remain constant. As \\(s\\) increases, the model becomes more flexible (the sum of absolute coefficients can be higher). With more flexible models, training RSS will always decrease. Repeat (a) for test RSS. With more flexible models, test RSS will decrease (as the fit improves) and will then increase due to overfitting (high variance). Repeat (a) for variance. As \\(s\\) increases, the model becomes more flexible so variance will increase. Repeat (a) for (squared) bias. As \\(s\\) increases, the model becomes more flexible so bias will decrease. Repeat (a) for the irreducible error. The irreducible error is unchanged. 6.1.4 Question 4 Suppose we estimate the regression coefficients in a linear regression model by minimizing \\[ \\sum_{i=1}^n \\left(y_i - \\beta_0 - \\sum_{j=1}^p\\beta_jx_{ij}\\right)^2 + \\lambda\\sum_{j=1}^p\\beta_j^2 \\] for a particular value of \\(\\lambda\\). For parts (a) through (e), indicate which of i. through v. is correct. Justify your answer. As we increase \\(\\lambda\\) from 0, the training RSS will: Increase initially, and then eventually start decreasing in an inverted U shape. Decrease initially, and then eventually start increasing in a U shape. Steadily increase. Steadily decrease. Remain constant. As \\(\\lambda\\) is increased, more weight is placed on the sum of squared coefficients and so the model becomes less flexible. As a result, training RSS must increase. Repeat (a) for test RSS. As \\(\\lambda\\) increases, flexibility decreases so test RSS will decrease (variance decreases) but will then increase (as bias increases). Repeat (a) for variance. Steadily decrease. Repeat (a) for (squared) bias. Steadily increase. Repeat (a) for the irreducible error. The irreducible error is unchanged. 6.1.5 Question 5 It is well-known that ridge regression tends to give similar coefficient values to correlated variables, whereas the lasso may give quite different coefficient values to correlated variables. We will now explore this property in a very simple setting. Suppose that \\(n = 2, p = 2, x_{11} = x_{12}, x_{21} = x_{22}\\). Furthermore, suppose that \\(y_1 + y_2 =0\\) and \\(x_{11} + x_{21} = 0\\) and \\(x_{12} + x_{22} = 0\\), so that the estimate for the intercept in a least squares, ridge regression, or lasso model is zero: \\(\\hat{\\beta}_0 = 0\\). Write out the ridge regression optimization problem in this setting. We are trying to minimize: \\[ \\sum_{i=1}^n \\left(y_i - \\beta_0 - \\sum_{j=1}^p\\beta_jx_{ij}\\right)^2 + \\lambda\\sum_{j=1}^p\\beta_j^2 \\] We can ignore \\(\\beta_0\\) and can expand the sums since there’s only two terms. Additionally, we can define \\(x_1 = x_{11} = x_{12}\\) and \\(x_2 = x_{21} = x_{22}\\). We then need to minimize \\[\\begin{align} f = &amp; (y_1 - \\beta_1x_1 - \\beta_2x_1)^2 + (y_2 - \\beta_1x_2 - \\beta_2x_2)^2 + \\lambda\\beta_1^2 + \\lambda\\beta_2^2 \\\\ f = &amp; y_1^2 - 2y_1\\beta_1x_1 - 2y_1\\beta_2x_1 + \\beta_1^2x_1^2 + 2\\beta_1\\beta_2x_1^2 + \\beta_2^2x_1^2 + \\\\ &amp; y_2^2 - 2y_2\\beta_1x_2 - 2y_2\\beta_2x_2 + \\beta_1^2x_2^2 + 2\\beta_1\\beta_2x_2^2 + \\beta_2^2x_2^2 + \\\\ &amp; \\lambda\\beta_1^2 + \\lambda\\beta_2^2 \\\\ \\end{align}\\] Argue that in this setting, the ridge coefficient estimates satisfy \\(\\hat{\\beta}_1 = \\hat{\\beta}_2\\) We can find when the above is minimized with respect to each of \\(\\beta_1\\) and \\(\\beta_2\\) by partial differentiation. \\[ \\frac{\\partial}{\\partial{\\beta_1}} = - 2y_1x_1 + 2\\beta_1x_1^2 + 2\\beta_2x_1^2 - 2y_2x_2 + 2\\beta_1x_2^2 + 2\\beta_2x_2^2 + 2\\lambda\\beta_1 \\] \\[ \\frac{\\partial}{\\partial{\\beta_2}} = - 2y_1x_1 + 2\\beta_1x_1^2 + 2\\beta_2x_1^2 - 2y_2x_2 + 2\\beta_1x_2^2 + 2\\beta_2x_2^2 + 2\\lambda\\beta_2 \\] A minimum can be found when these are set to 0. \\[ \\lambda\\beta_1 = y_1x_1 + y_2x_2 - \\beta_1x_1^2 - \\beta_2x_1^2 - \\beta_1x_2^2 - \\beta_2x_2^2 \\\\ \\lambda\\beta_2 = y_1x_1 + y_2x_2 - \\beta_1x_1^2 - \\beta_2x_1^2 - \\beta_1x_2^2 - \\beta_2x_2^2 \\] Therefore \\(\\lambda\\beta_1 = \\lambda\\beta_2\\) Write out the lasso optimization problem in this setting. We are trying to minimize: \\[ \\sum_{i=1}^n \\left(y_i - \\beta_0 - \\sum_{j=1}^p\\beta_jx_{ij}\\right)^2 + \\lambda\\sum_{j=1}^p |\\beta_j| \\] As above, we simplify to \\[ (y_1 - \\beta_1x_1 - \\beta_2x_1)^2 + (y_2 - \\beta_1x_2 - \\beta_2x_2)^2 + \\lambda|\\beta_1| + \\lambda|\\beta_2| \\] Argue that in this setting, the lasso coefficients \\(\\hat{\\beta}_1\\) and \\(\\hat{\\beta}_2\\) are not unique—in other words, there are many possible solutions to the optimization problem in (c). Describe these solutions. \\[ \\frac{\\partial}{\\partial{\\beta_1}} = - 2y_1x_1 + 2\\beta_1x_1^2 + 2\\beta_2x_1^2 - 2y_2x_2 + 2\\beta_1x_2^2 + 2\\beta_2x_2^2 + \\lambda\\frac{\\beta_1}{|\\beta_1|} \\] \\[ \\frac{\\partial}{\\partial{\\beta_2}} = - 2y_1x_1 + 2\\beta_1x_1^2 + 2\\beta_2x_1^2 - 2y_2x_2 + 2\\beta_1x_2^2 + 2\\beta_2x_2^2 + \\lambda\\frac{\\beta_2}{|\\beta_2|} \\] By the above logic \\(\\frac{|\\beta_1|}{\\beta_1} = \\frac{|\\beta_2|}{\\beta_2}\\) What does this imply though??? ToDo 6.1.6 Question 6 We will now explore (6.12) and (6.13) further. Consider (6.12) with \\(p = 1\\). For some choice of \\(y_1\\) and \\(\\lambda &gt; 0\\), plot (6.12) as a function of \\(\\beta_1\\). Your plot should confirm that (6.12) is solved by (6.14). Equation 6.12 is: \\[ \\sum_{j=1}^p(y_j - \\beta_j)^2 + \\lambda\\sum_{j=1}^p\\beta_j^2 \\] Equation 6.14 is: \\[ \\hat{\\beta}_j^R = y_j/(1 + \\lambda) \\] where \\(\\hat{\\beta}_j^R\\) is the ridge regression estimate. lambda &lt;- 0.7 y &lt;- 1.4 fn &lt;- function(beta) { (y - beta)^2 + lambda*beta^2 } plot(seq(0, 2, 0.01), fn(seq(0, 2, 0.01)), type = &quot;l&quot;, xlab = &quot;beta&quot;, ylab = &quot;6.12&quot;) abline(v = y/(1 + lambda), lty = 2) Consider (6.13) with \\(p = 1\\). For some choice of \\(y_1\\) and \\(\\lambda &gt; 0\\), plot (6.13) as a function of \\(\\beta_1\\). Your plot should confirm that (6.13) is solved by (6.15). Equation 6.13 is: \\[ \\sum_{j=1}^p(y_j - \\beta_j)^2 + \\lambda\\sum_{j=1}^p|\\beta_j| \\] Equation 6.15 is: \\[ \\hat{\\beta}_j^L = \\begin{cases} y_j - \\lambda/2 &amp;\\mbox{if } y_j &gt; \\lambda/2; \\\\ y_j + \\lambda/2 &amp;\\mbox{if } y_j &lt; -\\lambda/2; \\\\ 0 &amp;\\mbox{if } |y_j| \\le \\lambda/2; \\end{cases} \\] For \\(\\lambda = 0.7\\) and \\(y = 1.4\\), the top case applies. lambda &lt;- 0.7 y &lt;- 1.4 fn &lt;- function(beta) { (y - beta)^2 + lambda*abs(beta) } plot(seq(0, 2, 0.01), fn(seq(0, 2, 0.01)), type = &quot;l&quot;, xlab = &quot;beta&quot;, ylab = &quot;6.12&quot;) abline(v = y - lambda/2, lty = 2) 6.1.7 Question 7 We will now derive the Bayesian connection to the lasso and ridge regression discussed in Section 6.2.2. Suppose that \\(y_i = \\beta_0 + \\sum_{j=1}^p x_{ij}\\beta_j + \\epsilon_i\\) where \\(\\epsilon_1, ..., \\epsilon_n\\) are independent and identically distributed from a \\(N(0, \\sigma^2)\\) distribution. Write out the likelihood for the data. \\[\\begin{align*} \\mathcal{L} &amp;= \\prod_i^n \\mathcal{N}(0, \\sigma^2) \\\\ &amp;= \\prod_i^n \\frac{1}{\\sqrt{2\\pi\\sigma}}\\exp\\left(-\\frac{\\epsilon_i^2}{2\\sigma^2}\\right) \\\\ &amp;= \\left(\\frac{1}{\\sqrt{2\\pi\\sigma}}\\right)^n \\exp\\left(-\\frac{1}{2\\sigma^2} \\sum_i^n \\epsilon_i^2\\right) \\end{align*}\\] Assume the following prior for \\(\\beta\\): \\(\\beta_1, ..., \\beta_p\\) are independent and identically distributed according to a double-exponential distribution with mean 0 and common scale parameter b: i.e. \\(p(\\beta) = \\frac{1}{2b}\\exp(-|\\beta|/b)\\). Write out the posterior for \\(\\beta\\) in this setting. The posterior can be calculated by multiplying the prior and likelihood (up to a proportionality constant). \\[\\begin{align*} p(\\beta|X,Y) &amp;\\propto \\left(\\frac{1}{\\sqrt{2\\pi\\sigma}}\\right)^n \\exp\\left(-\\frac{1}{2\\sigma^2} \\sum_i^n \\epsilon_i^2\\right) \\prod_j^p\\frac{1}{2b}\\exp\\left(-\\frac{|\\beta_j|}{b}\\right) \\\\ &amp;\\propto \\frac{1}{2b} \\left(\\frac{1}{\\sqrt{2\\pi\\sigma}}\\right)^n \\exp\\left(-\\frac{1}{2\\sigma^2} \\sum_i^n \\epsilon_i^2 -\\sum_j^p\\frac{|\\beta_j|}{b}\\right) \\end{align*}\\] Argue that the lasso estimate is the mode for \\(\\beta\\) under this posterior distribution. Let us find the maximum of the posterior distribution (the mode). Maximizing the posterior probability is equivalent to maximizing its log which is: \\[ \\log(p(\\beta|X,Y)) \\propto \\log\\left[ \\frac{1}{2b} \\left(\\frac{1}{\\sqrt{2\\pi\\sigma}}\\right)^n \\right ] - \\left(\\frac{1}{2\\sigma^2} \\sum_i^n \\epsilon_i^2 + \\sum_j^p\\frac{|\\beta_j|}{b}\\right) \\] Since, the first term is independent of \\(\\beta\\), our solution will be when we minimize the second term. \\[\\begin{align*} \\DeclareMathOperator*{\\argmin}{arg\\,min} % Jan Hlavacek \\argmin_\\beta \\left(\\frac{1}{2\\sigma^2} \\sum_i^n \\epsilon_i^2 + \\sum_j^p\\frac{|\\beta|}{b}\\right) &amp;= \\argmin_\\beta \\left(\\frac{1}{2\\sigma^2} \\right ) \\left( \\sum_i^n \\epsilon_i^2 +\\frac{2\\sigma^2}{b}\\sum_j^p|\\beta_j|\\right) \\\\ &amp;= \\argmin_\\beta \\left( \\sum_i^n \\epsilon_i^2 +\\frac{2\\sigma^2}{b}\\sum_j^p|\\beta_j|\\right) \\end{align*}\\] Note, that \\(RSS = \\sum_i^n \\epsilon_i^2\\) and if we set \\(\\lambda = \\frac{2\\sigma^2}{b}\\), the mode corresponds to lasso optimization. \\[ \\argmin_\\beta RSS + \\lambda\\sum_j^p|\\beta_j| \\] Now assume the following prior for \\(\\beta\\): \\(\\beta_1, ..., \\beta_p\\) are independent and identically distributed according to a normal distribution with mean zero and variance \\(c\\). Write out the posterior for \\(\\beta\\) in this setting. The posterior is now: \\[\\begin{align*} p(\\beta|X,Y) &amp;\\propto \\left(\\frac{1}{\\sqrt{2\\pi\\sigma}}\\right)^n \\exp\\left(-\\frac{1}{2\\sigma^2} \\sum_i^n \\epsilon_i^2\\right) \\prod_j^p\\frac{1}{\\sqrt{2\\pi c}}\\exp\\left(-\\frac{\\beta_j^2}{2c}\\right) \\\\ &amp;\\propto \\left(\\frac{1}{\\sqrt{2\\pi\\sigma}}\\right)^n \\left(\\frac{1}{\\sqrt{2\\pi c}}\\right)^p \\exp\\left(-\\frac{1}{2\\sigma^2} \\sum_i^n \\epsilon_i^2 - \\frac{1}{2c}\\sum_j^p\\beta_j^2\\right) \\end{align*}\\] Argue that the ridge regression estimate is both the mode and the mean for \\(\\beta\\) under this posterior distribution. To show that the ridge estimate is the mode we can again find the maximum by maximizing the log of the posterior. The log is \\[ \\log{p(\\beta|X,Y)} \\propto \\log{\\left[\\left(\\frac{1}{\\sqrt{2\\pi\\sigma}}\\right)^n \\left(\\frac{1}{\\sqrt{2\\pi c}}\\right)^p \\right ]} - \\left(\\frac{1}{2\\sigma^2} \\sum_i^n \\epsilon_i^2 + \\frac{1}{2c}\\sum_j^p\\beta_j^2 \\right) \\] We can maximize (wrt \\(\\beta\\)) by ignoring the first term and minimizing the second term. i.e. we minimize: \\[ \\argmin_\\beta \\left( \\frac{1}{2\\sigma^2} \\sum_i^n \\epsilon_i^2 + \\frac{1}{2c}\\sum_j^p\\beta_j^2 \\right)\\\\ = \\argmin_\\beta \\left( \\frac{1}{2\\sigma^2} \\left( \\sum_i^n \\epsilon_i^2 + \\frac{\\sigma^2}{c}\\sum_j^p\\beta_j^2 \\right) \\right) \\] As above, if \\(RSS = \\sum_i^n \\epsilon_i^2\\) and if we set \\(\\lambda = \\frac{\\sigma^2}{c}\\), we can see that the mode corresponds to ridge optimization. 6.2 Applied 6.2.1 Question 8 In this exercise, we will generate simulated data, and will then use this data to perform best subset selection. Use the rnorm() function to generate a predictor \\(X\\) of length \\(n = 100\\), as well as a noise vector \\(\\epsilon\\) of length \\(n = 100\\). library(ISLR2) library(glmnet) library(leaps) library(pls) set.seed(42) x &lt;- rnorm(100) ep &lt;- rnorm(100) Generate a response vector \\(Y\\) of length \\(n = 100\\) according to the model \\[Y = \\beta_0 + \\beta_1X + \\beta_2X^2 + \\beta_3X^3 + \\epsilon,\\] where \\(\\beta_0, \\beta_1, \\beta_2,\\) and \\(\\beta_3\\) are constants of your choice. y &lt;- 2 + 3*x - 2*x^2 + 0.5*x^3 + ep Use the regsubsets() function to perform best subset selection in order to choose the best model containing the predictors \\(X, X^2, ..., X^{10}\\). What is the best model obtained according to \\(C_p\\), BIC, and adjusted \\(R^2\\)? Show some plots to provide evidence for your answer, and report the coefficients of the best model obtained. Note you will need to use the data.frame() function to create a single data set containing both \\(X\\) and \\(Y\\). dat &lt;- data.frame(x, y) summary(regsubsets(y ~ poly(x, 10, raw = TRUE), data = dat)) ## Subset selection object ## Call: regsubsets.formula(y ~ poly(x, 10, raw = TRUE), data = dat) ## 10 Variables (and intercept) ## Forced in Forced out ## poly(x, 10, raw = TRUE)1 FALSE FALSE ## poly(x, 10, raw = TRUE)2 FALSE FALSE ## poly(x, 10, raw = TRUE)3 FALSE FALSE ## poly(x, 10, raw = TRUE)4 FALSE FALSE ## poly(x, 10, raw = TRUE)5 FALSE FALSE ## poly(x, 10, raw = TRUE)6 FALSE FALSE ## poly(x, 10, raw = TRUE)7 FALSE FALSE ## poly(x, 10, raw = TRUE)8 FALSE FALSE ## poly(x, 10, raw = TRUE)9 FALSE FALSE ## poly(x, 10, raw = TRUE)10 FALSE FALSE ## 1 subsets of each size up to 8 ## Selection Algorithm: exhaustive ## poly(x, 10, raw = TRUE)1 poly(x, 10, raw = TRUE)2 ## 1 ( 1 ) &quot; &quot; &quot; &quot; ## 2 ( 1 ) &quot;*&quot; &quot;*&quot; ## 3 ( 1 ) &quot;*&quot; &quot;*&quot; ## 4 ( 1 ) &quot;*&quot; &quot;*&quot; ## 5 ( 1 ) &quot;*&quot; &quot;*&quot; ## 6 ( 1 ) &quot;*&quot; &quot;*&quot; ## 7 ( 1 ) &quot;*&quot; &quot;*&quot; ## 8 ( 1 ) &quot;*&quot; &quot;*&quot; ## poly(x, 10, raw = TRUE)3 poly(x, 10, raw = TRUE)4 ## 1 ( 1 ) &quot;*&quot; &quot; &quot; ## 2 ( 1 ) &quot; &quot; &quot; &quot; ## 3 ( 1 ) &quot;*&quot; &quot; &quot; ## 4 ( 1 ) &quot;*&quot; &quot; &quot; ## 5 ( 1 ) &quot; &quot; &quot; &quot; ## 6 ( 1 ) &quot; &quot; &quot;*&quot; ## 7 ( 1 ) &quot; &quot; &quot; &quot; ## 8 ( 1 ) &quot;*&quot; &quot; &quot; ## poly(x, 10, raw = TRUE)5 poly(x, 10, raw = TRUE)6 ## 1 ( 1 ) &quot; &quot; &quot; &quot; ## 2 ( 1 ) &quot; &quot; &quot; &quot; ## 3 ( 1 ) &quot; &quot; &quot; &quot; ## 4 ( 1 ) &quot;*&quot; &quot; &quot; ## 5 ( 1 ) &quot;*&quot; &quot; &quot; ## 6 ( 1 ) &quot;*&quot; &quot; &quot; ## 7 ( 1 ) &quot;*&quot; &quot;*&quot; ## 8 ( 1 ) &quot;*&quot; &quot;*&quot; ## poly(x, 10, raw = TRUE)7 poly(x, 10, raw = TRUE)8 ## 1 ( 1 ) &quot; &quot; &quot; &quot; ## 2 ( 1 ) &quot; &quot; &quot; &quot; ## 3 ( 1 ) &quot; &quot; &quot; &quot; ## 4 ( 1 ) &quot; &quot; &quot; &quot; ## 5 ( 1 ) &quot;*&quot; &quot; &quot; ## 6 ( 1 ) &quot;*&quot; &quot; &quot; ## 7 ( 1 ) &quot;*&quot; &quot;*&quot; ## 8 ( 1 ) &quot;*&quot; &quot;*&quot; ## poly(x, 10, raw = TRUE)9 poly(x, 10, raw = TRUE)10 ## 1 ( 1 ) &quot; &quot; &quot; &quot; ## 2 ( 1 ) &quot; &quot; &quot; &quot; ## 3 ( 1 ) &quot; &quot; &quot; &quot; ## 4 ( 1 ) &quot; &quot; &quot; &quot; ## 5 ( 1 ) &quot;*&quot; &quot; &quot; ## 6 ( 1 ) &quot;*&quot; &quot; &quot; ## 7 ( 1 ) &quot;*&quot; &quot; &quot; ## 8 ( 1 ) &quot;*&quot; &quot; &quot; Repeat (c), using forward stepwise selection and also using backwards stepwise selection. How does your answer compare to the results in (c)? summary(regsubsets(y ~ poly(x, 10, raw = TRUE), data = dat, method = &quot;forward&quot;)) ## Subset selection object ## Call: regsubsets.formula(y ~ poly(x, 10, raw = TRUE), data = dat, method = &quot;forward&quot;) ## 10 Variables (and intercept) ## Forced in Forced out ## poly(x, 10, raw = TRUE)1 FALSE FALSE ## poly(x, 10, raw = TRUE)2 FALSE FALSE ## poly(x, 10, raw = TRUE)3 FALSE FALSE ## poly(x, 10, raw = TRUE)4 FALSE FALSE ## poly(x, 10, raw = TRUE)5 FALSE FALSE ## poly(x, 10, raw = TRUE)6 FALSE FALSE ## poly(x, 10, raw = TRUE)7 FALSE FALSE ## poly(x, 10, raw = TRUE)8 FALSE FALSE ## poly(x, 10, raw = TRUE)9 FALSE FALSE ## poly(x, 10, raw = TRUE)10 FALSE FALSE ## 1 subsets of each size up to 8 ## Selection Algorithm: forward ## poly(x, 10, raw = TRUE)1 poly(x, 10, raw = TRUE)2 ## 1 ( 1 ) &quot; &quot; &quot; &quot; ## 2 ( 1 ) &quot; &quot; &quot;*&quot; ## 3 ( 1 ) &quot;*&quot; &quot;*&quot; ## 4 ( 1 ) &quot;*&quot; &quot;*&quot; ## 5 ( 1 ) &quot;*&quot; &quot;*&quot; ## 6 ( 1 ) &quot;*&quot; &quot;*&quot; ## 7 ( 1 ) &quot;*&quot; &quot;*&quot; ## 8 ( 1 ) &quot;*&quot; &quot;*&quot; ## poly(x, 10, raw = TRUE)3 poly(x, 10, raw = TRUE)4 ## 1 ( 1 ) &quot;*&quot; &quot; &quot; ## 2 ( 1 ) &quot;*&quot; &quot; &quot; ## 3 ( 1 ) &quot;*&quot; &quot; &quot; ## 4 ( 1 ) &quot;*&quot; &quot; &quot; ## 5 ( 1 ) &quot;*&quot; &quot; &quot; ## 6 ( 1 ) &quot;*&quot; &quot; &quot; ## 7 ( 1 ) &quot;*&quot; &quot;*&quot; ## 8 ( 1 ) &quot;*&quot; &quot;*&quot; ## poly(x, 10, raw = TRUE)5 poly(x, 10, raw = TRUE)6 ## 1 ( 1 ) &quot; &quot; &quot; &quot; ## 2 ( 1 ) &quot; &quot; &quot; &quot; ## 3 ( 1 ) &quot; &quot; &quot; &quot; ## 4 ( 1 ) &quot;*&quot; &quot; &quot; ## 5 ( 1 ) &quot;*&quot; &quot; &quot; ## 6 ( 1 ) &quot;*&quot; &quot; &quot; ## 7 ( 1 ) &quot;*&quot; &quot; &quot; ## 8 ( 1 ) &quot;*&quot; &quot; &quot; ## poly(x, 10, raw = TRUE)7 poly(x, 10, raw = TRUE)8 ## 1 ( 1 ) &quot; &quot; &quot; &quot; ## 2 ( 1 ) &quot; &quot; &quot; &quot; ## 3 ( 1 ) &quot; &quot; &quot; &quot; ## 4 ( 1 ) &quot; &quot; &quot; &quot; ## 5 ( 1 ) &quot; &quot; &quot; &quot; ## 6 ( 1 ) &quot;*&quot; &quot; &quot; ## 7 ( 1 ) &quot;*&quot; &quot; &quot; ## 8 ( 1 ) &quot;*&quot; &quot; &quot; ## poly(x, 10, raw = TRUE)9 poly(x, 10, raw = TRUE)10 ## 1 ( 1 ) &quot; &quot; &quot; &quot; ## 2 ( 1 ) &quot; &quot; &quot; &quot; ## 3 ( 1 ) &quot; &quot; &quot; &quot; ## 4 ( 1 ) &quot; &quot; &quot; &quot; ## 5 ( 1 ) &quot;*&quot; &quot; &quot; ## 6 ( 1 ) &quot;*&quot; &quot; &quot; ## 7 ( 1 ) &quot;*&quot; &quot; &quot; ## 8 ( 1 ) &quot;*&quot; &quot;*&quot; summary(regsubsets(y ~ poly(x, 10, raw = TRUE), data = dat, method = &quot;backward&quot;)) ## Subset selection object ## Call: regsubsets.formula(y ~ poly(x, 10, raw = TRUE), data = dat, method = &quot;backward&quot;) ## 10 Variables (and intercept) ## Forced in Forced out ## poly(x, 10, raw = TRUE)1 FALSE FALSE ## poly(x, 10, raw = TRUE)2 FALSE FALSE ## poly(x, 10, raw = TRUE)3 FALSE FALSE ## poly(x, 10, raw = TRUE)4 FALSE FALSE ## poly(x, 10, raw = TRUE)5 FALSE FALSE ## poly(x, 10, raw = TRUE)6 FALSE FALSE ## poly(x, 10, raw = TRUE)7 FALSE FALSE ## poly(x, 10, raw = TRUE)8 FALSE FALSE ## poly(x, 10, raw = TRUE)9 FALSE FALSE ## poly(x, 10, raw = TRUE)10 FALSE FALSE ## 1 subsets of each size up to 8 ## Selection Algorithm: backward ## poly(x, 10, raw = TRUE)1 poly(x, 10, raw = TRUE)2 ## 1 ( 1 ) &quot;*&quot; &quot; &quot; ## 2 ( 1 ) &quot;*&quot; &quot;*&quot; ## 3 ( 1 ) &quot;*&quot; &quot;*&quot; ## 4 ( 1 ) &quot;*&quot; &quot;*&quot; ## 5 ( 1 ) &quot;*&quot; &quot;*&quot; ## 6 ( 1 ) &quot;*&quot; &quot;*&quot; ## 7 ( 1 ) &quot;*&quot; &quot;*&quot; ## 8 ( 1 ) &quot;*&quot; &quot;*&quot; ## poly(x, 10, raw = TRUE)3 poly(x, 10, raw = TRUE)4 ## 1 ( 1 ) &quot; &quot; &quot; &quot; ## 2 ( 1 ) &quot; &quot; &quot; &quot; ## 3 ( 1 ) &quot; &quot; &quot; &quot; ## 4 ( 1 ) &quot; &quot; &quot; &quot; ## 5 ( 1 ) &quot; &quot; &quot; &quot; ## 6 ( 1 ) &quot; &quot; &quot; &quot; ## 7 ( 1 ) &quot; &quot; &quot; &quot; ## 8 ( 1 ) &quot; &quot; &quot; &quot; ## poly(x, 10, raw = TRUE)5 poly(x, 10, raw = TRUE)6 ## 1 ( 1 ) &quot; &quot; &quot; &quot; ## 2 ( 1 ) &quot; &quot; &quot; &quot; ## 3 ( 1 ) &quot;*&quot; &quot; &quot; ## 4 ( 1 ) &quot;*&quot; &quot; &quot; ## 5 ( 1 ) &quot;*&quot; &quot; &quot; ## 6 ( 1 ) &quot;*&quot; &quot;*&quot; ## 7 ( 1 ) &quot;*&quot; &quot;*&quot; ## 8 ( 1 ) &quot;*&quot; &quot;*&quot; ## poly(x, 10, raw = TRUE)7 poly(x, 10, raw = TRUE)8 ## 1 ( 1 ) &quot; &quot; &quot; &quot; ## 2 ( 1 ) &quot; &quot; &quot; &quot; ## 3 ( 1 ) &quot; &quot; &quot; &quot; ## 4 ( 1 ) &quot;*&quot; &quot; &quot; ## 5 ( 1 ) &quot;*&quot; &quot; &quot; ## 6 ( 1 ) &quot;*&quot; &quot; &quot; ## 7 ( 1 ) &quot;*&quot; &quot;*&quot; ## 8 ( 1 ) &quot;*&quot; &quot;*&quot; ## poly(x, 10, raw = TRUE)9 poly(x, 10, raw = TRUE)10 ## 1 ( 1 ) &quot; &quot; &quot; &quot; ## 2 ( 1 ) &quot; &quot; &quot; &quot; ## 3 ( 1 ) &quot; &quot; &quot; &quot; ## 4 ( 1 ) &quot; &quot; &quot; &quot; ## 5 ( 1 ) &quot;*&quot; &quot; &quot; ## 6 ( 1 ) &quot;*&quot; &quot; &quot; ## 7 ( 1 ) &quot;*&quot; &quot; &quot; ## 8 ( 1 ) &quot;*&quot; &quot;*&quot; Now fit a lasso model to the simulated data, again using \\(X, X^2, ..., X^{10}\\) as predictors. Use cross-validation to select the optimal value of \\(\\lambda\\). Create plots of the cross-validation error as a function of \\(\\lambda\\). Report the resulting coefficient estimates, and discuss the results obtained. res &lt;- cv.glmnet(poly(dat$x, 10, raw = TRUE), dat$y, alpha = 1) (best &lt;- res$lambda.min) ## [1] 0.09804219 plot(res) out = glmnet(poly(dat$x, 10, raw = TRUE), dat$y, alpha = 1, lambda = res$lambda.min) predict(out, type = &quot;coefficients&quot;, s = best) ## 11 x 1 sparse Matrix of class &quot;dgCMatrix&quot; ## s1 ## (Intercept) 1.8457308 ## 1 2.9092918 ## 2 -1.9287428 ## 3 0.5161012 ## 4 . ## 5 . ## 6 . ## 7 . ## 8 . ## 9 . ## 10 . When fitting lasso, the model that minimizes MSE uses three predictors (as per the simulation). The coefficients estimated (2.9, -1.9 and 0.5) are similar to those used in the simulation. Now generate a response vector \\(Y\\) according to the model \\[Y = \\beta_0 + \\beta_7X^7 + \\epsilon,\\] and perform best subset selection and the lasso. Discuss the results obtained. dat$y &lt;- 2 - 2*x^2 +0.2*x^7 + ep summary(regsubsets(y ~ poly(x, 10, raw = TRUE), data = dat)) ## Subset selection object ## Call: regsubsets.formula(y ~ poly(x, 10, raw = TRUE), data = dat) ## 10 Variables (and intercept) ## Forced in Forced out ## poly(x, 10, raw = TRUE)1 FALSE FALSE ## poly(x, 10, raw = TRUE)2 FALSE FALSE ## poly(x, 10, raw = TRUE)3 FALSE FALSE ## poly(x, 10, raw = TRUE)4 FALSE FALSE ## poly(x, 10, raw = TRUE)5 FALSE FALSE ## poly(x, 10, raw = TRUE)6 FALSE FALSE ## poly(x, 10, raw = TRUE)7 FALSE FALSE ## poly(x, 10, raw = TRUE)8 FALSE FALSE ## poly(x, 10, raw = TRUE)9 FALSE FALSE ## poly(x, 10, raw = TRUE)10 FALSE FALSE ## 1 subsets of each size up to 8 ## Selection Algorithm: exhaustive ## poly(x, 10, raw = TRUE)1 poly(x, 10, raw = TRUE)2 ## 1 ( 1 ) &quot; &quot; &quot; &quot; ## 2 ( 1 ) &quot; &quot; &quot;*&quot; ## 3 ( 1 ) &quot; &quot; &quot;*&quot; ## 4 ( 1 ) &quot; &quot; &quot;*&quot; ## 5 ( 1 ) &quot; &quot; &quot;*&quot; ## 6 ( 1 ) &quot; &quot; &quot;*&quot; ## 7 ( 1 ) &quot; &quot; &quot;*&quot; ## 8 ( 1 ) &quot; &quot; &quot;*&quot; ## poly(x, 10, raw = TRUE)3 poly(x, 10, raw = TRUE)4 ## 1 ( 1 ) &quot; &quot; &quot; &quot; ## 2 ( 1 ) &quot; &quot; &quot; &quot; ## 3 ( 1 ) &quot;*&quot; &quot; &quot; ## 4 ( 1 ) &quot;*&quot; &quot; &quot; ## 5 ( 1 ) &quot;*&quot; &quot;*&quot; ## 6 ( 1 ) &quot;*&quot; &quot; &quot; ## 7 ( 1 ) &quot;*&quot; &quot; &quot; ## 8 ( 1 ) &quot;*&quot; &quot;*&quot; ## poly(x, 10, raw = TRUE)5 poly(x, 10, raw = TRUE)6 ## 1 ( 1 ) &quot; &quot; &quot; &quot; ## 2 ( 1 ) &quot; &quot; &quot; &quot; ## 3 ( 1 ) &quot; &quot; &quot; &quot; ## 4 ( 1 ) &quot;*&quot; &quot; &quot; ## 5 ( 1 ) &quot;*&quot; &quot; &quot; ## 6 ( 1 ) &quot;*&quot; &quot;*&quot; ## 7 ( 1 ) &quot;*&quot; &quot;*&quot; ## 8 ( 1 ) &quot;*&quot; &quot;*&quot; ## poly(x, 10, raw = TRUE)7 poly(x, 10, raw = TRUE)8 ## 1 ( 1 ) &quot;*&quot; &quot; &quot; ## 2 ( 1 ) &quot;*&quot; &quot; &quot; ## 3 ( 1 ) &quot;*&quot; &quot; &quot; ## 4 ( 1 ) &quot; &quot; &quot; &quot; ## 5 ( 1 ) &quot; &quot; &quot; &quot; ## 6 ( 1 ) &quot; &quot; &quot;*&quot; ## 7 ( 1 ) &quot; &quot; &quot;*&quot; ## 8 ( 1 ) &quot; &quot; &quot;*&quot; ## poly(x, 10, raw = TRUE)9 poly(x, 10, raw = TRUE)10 ## 1 ( 1 ) &quot; &quot; &quot; &quot; ## 2 ( 1 ) &quot; &quot; &quot; &quot; ## 3 ( 1 ) &quot; &quot; &quot; &quot; ## 4 ( 1 ) &quot;*&quot; &quot; &quot; ## 5 ( 1 ) &quot;*&quot; &quot; &quot; ## 6 ( 1 ) &quot;*&quot; &quot; &quot; ## 7 ( 1 ) &quot;*&quot; &quot;*&quot; ## 8 ( 1 ) &quot;*&quot; &quot;*&quot; res &lt;- cv.glmnet(poly(dat$x, 10, raw = TRUE), dat$y, alpha = 1) (best &lt;- res$lambda.min) ## [1] 1.126906 plot(res) out = glmnet(poly(dat$x, 10, raw = TRUE), dat$y, alpha = 1, lambda = best) predict(out, type = &quot;coefficients&quot;, s = best) ## 11 x 1 sparse Matrix of class &quot;dgCMatrix&quot; ## s1 ## (Intercept) 1.061389580 ## 1 . ## 2 -0.883080980 ## 3 . ## 4 -0.121018425 ## 5 0.028984084 ## 6 -0.009540039 ## 7 0.188796928 ## 8 . ## 9 . ## 10 . When fitting lasso, the model does not perfectly replicate the simulation (coefficients are retained for powers of \\(x\\) that were not simulated). 6.2.2 Question 9 In this exercise, we will predict the number of applications received using the other variables in the College data set. Split the data set into a training set and a test set. set.seed(42) train &lt;- sample(nrow(College), nrow(College)*2/3) test &lt;- setdiff(seq_len(nrow(College)), train) mse &lt;- list() Fit a linear model using least squares on the training set, and report the test error obtained. fit &lt;- lm(Apps ~ ., data = College[train, ]) (mse$lm &lt;- mean((predict(fit, College[test, ]) - College$Apps[test]) ^ 2)) ## [1] 1695269 Fit a ridge regression model on the training set, with \\(\\lambda\\) chosen by cross-validation. Report the test error obtained. mm &lt;- model.matrix(Apps ~ ., data = College[train, ]) fit2 &lt;- cv.glmnet(mm, College$Apps[train], alpha = 0) p &lt;- predict(fit2, model.matrix(Apps ~ ., data = College[test, ]), s = fit2$lambda.min) (mse$ridge &lt;- mean((p - College$Apps[test]) ^ 2)) ## [1] 2804369 Fit a lasso model on the training set, with \\(\\lambda\\) chosen by cross- validation. Report the test error obtained, along with the number of non-zero coefficient estimates. mm &lt;- model.matrix(Apps ~ ., data = College[train, ]) fit3 &lt;- cv.glmnet(mm, College$Apps[train], alpha = 1) p &lt;- predict(fit3, model.matrix(Apps ~ ., data = College[test, ]), s = fit3$lambda.min) (mse$lasso &lt;- mean((p - College$Apps[test]) ^ 2)) ## [1] 1822322 Fit a PCR model on the training set, with \\(M\\) chosen by cross-validation. Report the test error obtained, along with the value of \\(M\\) selected by cross-validation. fit4 &lt;- pcr(Apps ~ ., data = College[train, ], scale = TRUE, validation = &quot;CV&quot;) validationplot(fit4, val.type = &quot;MSEP&quot;) p &lt;- predict(fit4, College[test, ], ncomp = 17) (mse$pcr &lt;- mean((p - College$Apps[test])^2)) ## [1] 1695269 Fit a PLS model on the training set, with \\(M\\) chosen by cross-validation. Report the test error obtained, along with the value of \\(M\\) selected by cross-validation. fit5 &lt;- plsr(Apps ~ ., data = College[train, ], scale = TRUE, validation = &quot;CV&quot;) validationplot(fit5, val.type = &quot;MSEP&quot;) p &lt;- predict(fit5, College[test, ], ncomp = 12) (mse$pls &lt;- mean((p - College$Apps[test])^2)) ## [1] 1696902 Comment on the results obtained. How accurately can we predict the number of college applications received? Is there much difference among the test errors resulting from these five approaches? barplot(unlist(mse), ylab = &quot;Test MSE&quot;, horiz = TRUE) Ridge and lasso give the lowest test errors but the lowest is generated by the ridge regression model (in this specific case with this specific seed). 6.2.3 Question 10 We have seen that as the number of features used in a model increases, the training error will necessarily decrease, but the test error may not. We will now explore this in a simulated data set. Generate a data set with \\(p = 20\\) features, \\(n = 1,000\\) observations, and an associated quantitative response vector generated according to the model \\(Y =X\\beta + \\epsilon\\), where \\(\\beta\\) has some elements that are exactly equal to zero. set.seed(42) dat &lt;- matrix(rnorm(1000*20), nrow = 1000) colnames(dat) &lt;- paste0(&quot;b&quot;, 1:20) beta &lt;- rep(0, 20) beta[1:4] &lt;- c(5, 4, 2, 7) y &lt;- colSums((t(dat) * beta)) + rnorm(1000) dat &lt;- data.frame(dat) dat$y &lt;- y Split your data set into a training set containing 100 observations and a test set containing 900 observations. train &lt;- dat[1:100, ] test &lt;- dat[101:1000, ] Perform best subset selection on the training set, and plot the training set MSE associated with the best model of each size. fit &lt;- regsubsets(y ~ ., data = train, nvmax = 20) summary(fit) ## Subset selection object ## Call: regsubsets.formula(y ~ ., data = train, nvmax = 20) ## 20 Variables (and intercept) ## Forced in Forced out ## b1 FALSE FALSE ## b2 FALSE FALSE ## b3 FALSE FALSE ## b4 FALSE FALSE ## b5 FALSE FALSE ## b6 FALSE FALSE ## b7 FALSE FALSE ## b8 FALSE FALSE ## b9 FALSE FALSE ## b10 FALSE FALSE ## b11 FALSE FALSE ## b12 FALSE FALSE ## b13 FALSE FALSE ## b14 FALSE FALSE ## b15 FALSE FALSE ## b16 FALSE FALSE ## b17 FALSE FALSE ## b18 FALSE FALSE ## b19 FALSE FALSE ## b20 FALSE FALSE ## 1 subsets of each size up to 20 ## Selection Algorithm: exhaustive ## b1 b2 b3 b4 b5 b6 b7 b8 b9 b10 b11 b12 b13 b14 b15 b16 b17 ## 1 ( 1 ) &quot; &quot; &quot; &quot; &quot; &quot; &quot;*&quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; ## 2 ( 1 ) &quot;*&quot; &quot; &quot; &quot; &quot; &quot;*&quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; ## 3 ( 1 ) &quot;*&quot; &quot;*&quot; &quot; &quot; &quot;*&quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; ## 4 ( 1 ) &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; ## 5 ( 1 ) &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot;*&quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; ## 6 ( 1 ) &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot;*&quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot;*&quot; ## 7 ( 1 ) &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot;*&quot; &quot; &quot; &quot; &quot; &quot;*&quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot;*&quot; ## 8 ( 1 ) &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot;*&quot; &quot; &quot; &quot; &quot; &quot;*&quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot;*&quot; &quot;*&quot; ## 9 ( 1 ) &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot; &quot; &quot; &quot; &quot;*&quot; &quot; &quot; &quot; &quot; &quot;*&quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot;*&quot; &quot;*&quot; ## 10 ( 1 ) &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot; &quot; &quot; &quot; &quot;*&quot; &quot; &quot; &quot; &quot; &quot;*&quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot;*&quot; &quot;*&quot; ## 11 ( 1 ) &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot; &quot; &quot; &quot; &quot;*&quot; &quot; &quot; &quot; &quot; &quot;*&quot; &quot;*&quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot;*&quot; &quot;*&quot; ## 12 ( 1 ) &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot; &quot; &quot; &quot; &quot;*&quot; &quot; &quot; &quot; &quot; &quot;*&quot; &quot;*&quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot;*&quot; &quot;*&quot; ## 13 ( 1 ) &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot; &quot; &quot; &quot; &quot;*&quot; &quot; &quot; &quot; &quot; &quot;*&quot; &quot;*&quot; &quot; &quot; &quot;*&quot; &quot; &quot; &quot;*&quot; &quot;*&quot; ## 14 ( 1 ) &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot; &quot; &quot; &quot; &quot;*&quot; &quot; &quot; &quot; &quot; &quot;*&quot; &quot;*&quot; &quot; &quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; ## 15 ( 1 ) &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot; &quot; &quot;*&quot; &quot; &quot; &quot; &quot; &quot;*&quot; &quot;*&quot; &quot; &quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; ## 16 ( 1 ) &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot; &quot; &quot;*&quot; &quot; &quot; &quot; &quot; &quot;*&quot; &quot;*&quot; &quot; &quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; ## 17 ( 1 ) &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot; &quot; &quot;*&quot; &quot;*&quot; &quot; &quot; &quot;*&quot; &quot;*&quot; &quot; &quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; ## 18 ( 1 ) &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot; &quot; &quot;*&quot; &quot;*&quot; &quot; &quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; ## 19 ( 1 ) &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot; &quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; ## 20 ( 1 ) &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; ## b18 b19 b20 ## 1 ( 1 ) &quot; &quot; &quot; &quot; &quot; &quot; ## 2 ( 1 ) &quot; &quot; &quot; &quot; &quot; &quot; ## 3 ( 1 ) &quot; &quot; &quot; &quot; &quot; &quot; ## 4 ( 1 ) &quot; &quot; &quot; &quot; &quot; &quot; ## 5 ( 1 ) &quot; &quot; &quot; &quot; &quot; &quot; ## 6 ( 1 ) &quot; &quot; &quot; &quot; &quot; &quot; ## 7 ( 1 ) &quot; &quot; &quot; &quot; &quot; &quot; ## 8 ( 1 ) &quot; &quot; &quot; &quot; &quot; &quot; ## 9 ( 1 ) &quot; &quot; &quot; &quot; &quot; &quot; ## 10 ( 1 ) &quot; &quot; &quot; &quot; &quot;*&quot; ## 11 ( 1 ) &quot; &quot; &quot; &quot; &quot;*&quot; ## 12 ( 1 ) &quot;*&quot; &quot; &quot; &quot;*&quot; ## 13 ( 1 ) &quot;*&quot; &quot; &quot; &quot;*&quot; ## 14 ( 1 ) &quot;*&quot; &quot; &quot; &quot;*&quot; ## 15 ( 1 ) &quot;*&quot; &quot; &quot; &quot;*&quot; ## 16 ( 1 ) &quot;*&quot; &quot;*&quot; &quot;*&quot; ## 17 ( 1 ) &quot;*&quot; &quot;*&quot; &quot;*&quot; ## 18 ( 1 ) &quot;*&quot; &quot;*&quot; &quot;*&quot; ## 19 ( 1 ) &quot;*&quot; &quot;*&quot; &quot;*&quot; ## 20 ( 1 ) &quot;*&quot; &quot;*&quot; &quot;*&quot; plot(summary(fit)$rss / 100, ylab = &quot;MSE&quot;, type = &quot;o&quot;) Plot the test set MSE associated with the best model of each size. predict.regsubsets &lt;- function (object, newdata, id, ...) { form &lt;- as.formula (object$call[[2]]) mat &lt;- model.matrix (form , newdata) coefi &lt;- coef(object, id = id) xvars &lt;- names(coefi) mat[ ,xvars] %*% coefi } mse &lt;- sapply(1:20, function(i) mean((test$y - predict(fit, test, i))^2)) plot(mse, ylab = &quot;MSE&quot;, type = &quot;o&quot;, pch = 19) For which model size does the test set MSE take on its minimum value? Comment on your results. If it takes on its minimum value for a model containing only an intercept or a model containing all of the features, then play around with the way that you are generating the data in (a) until you come up with a scenario in which the test set MSE is minimized for an intermediate model size. which.min(mse) ## [1] 4 The min test MSE is found when model size is 4. This corresponds to the simulated data which has four non-zero coefficients. set.seed(42) dat &lt;- matrix(rnorm(1000*20), nrow = 1000) colnames(dat) &lt;- paste0(&quot;b&quot;, 1:20) beta &lt;- rep(0, 20) beta[1:9] &lt;- c(5, 4, 2, 7, 0.01, 0.001, 0.05, 0.1, 0.5) y &lt;- colSums((t(dat) * beta)) + rnorm(1000) dat &lt;- data.frame(dat) dat$y &lt;- y train &lt;- dat[1:100, ] test &lt;- dat[101:1000, ] fit &lt;- regsubsets(y ~ ., data = train, nvmax = 20) summary(fit) ## Subset selection object ## Call: regsubsets.formula(y ~ ., data = train, nvmax = 20) ## 20 Variables (and intercept) ## Forced in Forced out ## b1 FALSE FALSE ## b2 FALSE FALSE ## b3 FALSE FALSE ## b4 FALSE FALSE ## b5 FALSE FALSE ## b6 FALSE FALSE ## b7 FALSE FALSE ## b8 FALSE FALSE ## b9 FALSE FALSE ## b10 FALSE FALSE ## b11 FALSE FALSE ## b12 FALSE FALSE ## b13 FALSE FALSE ## b14 FALSE FALSE ## b15 FALSE FALSE ## b16 FALSE FALSE ## b17 FALSE FALSE ## b18 FALSE FALSE ## b19 FALSE FALSE ## b20 FALSE FALSE ## 1 subsets of each size up to 20 ## Selection Algorithm: exhaustive ## b1 b2 b3 b4 b5 b6 b7 b8 b9 b10 b11 b12 b13 b14 b15 b16 b17 ## 1 ( 1 ) &quot; &quot; &quot; &quot; &quot; &quot; &quot;*&quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; ## 2 ( 1 ) &quot;*&quot; &quot; &quot; &quot; &quot; &quot;*&quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; ## 3 ( 1 ) &quot;*&quot; &quot;*&quot; &quot; &quot; &quot;*&quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; ## 4 ( 1 ) &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; ## 5 ( 1 ) &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot;*&quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; ## 6 ( 1 ) &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot;*&quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot;*&quot; ## 7 ( 1 ) &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot;*&quot; &quot;*&quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot;*&quot; ## 8 ( 1 ) &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot;*&quot; &quot;*&quot; &quot; &quot; &quot;*&quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot;*&quot; ## 9 ( 1 ) &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot;*&quot; &quot;*&quot; &quot; &quot; &quot;*&quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot;*&quot; &quot;*&quot; ## 10 ( 1 ) &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot; &quot; &quot; &quot; &quot;*&quot; &quot;*&quot; &quot; &quot; &quot;*&quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot;*&quot; &quot;*&quot; ## 11 ( 1 ) &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot; &quot; &quot; &quot; &quot;*&quot; &quot;*&quot; &quot; &quot; &quot;*&quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot;*&quot; &quot;*&quot; ## 12 ( 1 ) &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot; &quot; &quot; &quot; &quot;*&quot; &quot;*&quot; &quot; &quot; &quot;*&quot; &quot;*&quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot;*&quot; &quot;*&quot; ## 13 ( 1 ) &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot; &quot; &quot; &quot; &quot;*&quot; &quot;*&quot; &quot; &quot; &quot;*&quot; &quot;*&quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot;*&quot; &quot;*&quot; ## 14 ( 1 ) &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot; &quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot; &quot; &quot;*&quot; &quot;*&quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot;*&quot; &quot;*&quot; ## 15 ( 1 ) &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot; &quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot; &quot; &quot;*&quot; &quot;*&quot; &quot; &quot; &quot;*&quot; &quot; &quot; &quot;*&quot; &quot;*&quot; ## 16 ( 1 ) &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot; &quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot; &quot; &quot;*&quot; &quot;*&quot; &quot; &quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; ## 17 ( 1 ) &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot; &quot; &quot;*&quot; &quot;*&quot; &quot; &quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; ## 18 ( 1 ) &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot; &quot; &quot;*&quot; &quot;*&quot; &quot; &quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; ## 19 ( 1 ) &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot; &quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; ## 20 ( 1 ) &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; ## b18 b19 b20 ## 1 ( 1 ) &quot; &quot; &quot; &quot; &quot; &quot; ## 2 ( 1 ) &quot; &quot; &quot; &quot; &quot; &quot; ## 3 ( 1 ) &quot; &quot; &quot; &quot; &quot; &quot; ## 4 ( 1 ) &quot; &quot; &quot; &quot; &quot; &quot; ## 5 ( 1 ) &quot; &quot; &quot; &quot; &quot; &quot; ## 6 ( 1 ) &quot; &quot; &quot; &quot; &quot; &quot; ## 7 ( 1 ) &quot; &quot; &quot; &quot; &quot; &quot; ## 8 ( 1 ) &quot; &quot; &quot; &quot; &quot; &quot; ## 9 ( 1 ) &quot; &quot; &quot; &quot; &quot; &quot; ## 10 ( 1 ) &quot; &quot; &quot; &quot; &quot; &quot; ## 11 ( 1 ) &quot; &quot; &quot; &quot; &quot;*&quot; ## 12 ( 1 ) &quot; &quot; &quot; &quot; &quot;*&quot; ## 13 ( 1 ) &quot;*&quot; &quot; &quot; &quot;*&quot; ## 14 ( 1 ) &quot;*&quot; &quot; &quot; &quot;*&quot; ## 15 ( 1 ) &quot;*&quot; &quot; &quot; &quot;*&quot; ## 16 ( 1 ) &quot;*&quot; &quot; &quot; &quot;*&quot; ## 17 ( 1 ) &quot;*&quot; &quot; &quot; &quot;*&quot; ## 18 ( 1 ) &quot;*&quot; &quot;*&quot; &quot;*&quot; ## 19 ( 1 ) &quot;*&quot; &quot;*&quot; &quot;*&quot; ## 20 ( 1 ) &quot;*&quot; &quot;*&quot; &quot;*&quot; mse &lt;- sapply(1:20, function(i) mean((test$y - predict(fit, test, i))^2)) plot(mse, ylab = &quot;MSE&quot;, type = &quot;o&quot;, pch = 19) which.min(mse) ## [1] 5 How does the model at which the test set MSE is minimized compare to the true model used to generate the data? Comment on the coefficient values. The min test MSE is found when model size is 5 but there are 9 non-zero coefficients. coef(fit, id = 5) ## (Intercept) b1 b2 b3 b4 b9 ## 0.03507654 5.06180121 3.82785027 2.20434996 7.05312844 0.57032008 The coefficient values are well estimated when high, but the smaller coefficients are dropped. Create a plot displaying \\(\\sqrt{\\sum_{j=1}^p (\\beta_j - \\hat{\\beta}{}_j^r)^2}\\) for a range of values of \\(r\\), where \\(\\hat{\\beta}{}_j^r\\) is the \\(j\\)th coefficient estimate for the best model containing \\(r\\) coefficients. Comment on what you observe. How does this compare to the test MSE plot from (d)? names(beta) &lt;- paste0(&quot;b&quot;, 1:20) b &lt;- data.frame(id = names(beta), b = beta) out &lt;- sapply(1:20, function(i) { c &lt;- coef(fit, id = i)[-1] c &lt;- data.frame(id = names(c), c = c) m &lt;- merge(b, c) sqrt(sum((m$b - m$c)^2)) }) plot(out, ylab = &quot;Mean squared coefficient error&quot;, type = &quot;o&quot;, pch = 19) The error of the coefficient estimates is minimized when model size is 5. This corresponds to the point when test MSE was minimized. 6.2.4 Question 11 We will now try to predict per capita crime rate in the Boston data set. Try out some of the regression methods explored in this chapter, such as best subset selection, the lasso, ridge regression, and PCR. Present and discuss results for the approaches that you consider. set.seed(1) train &lt;- sample(nrow(Boston), nrow(Boston)*2/3) test &lt;- setdiff(seq_len(nrow(Boston)), train) hist(log(Boston$crim)) Propose a model (or set of models) that seem to perform well on this data set, and justify your answer. Make sure that you are evaluating model performance using validation set error, cross-validation, or some other reasonable alternative, as opposed to using training error. We will try to fit models to log(Boston$crim) which is closer to a normal distribution. fit &lt;- lm(log(crim) ~ ., data = Boston[train, ]) mean((predict(fit, Boston[test, ]) - log(Boston$crim[test])) ^ 2) ## [1] 0.66578 mm &lt;- model.matrix(log(crim) ~ ., data = Boston[train, ]) fit2 &lt;- cv.glmnet(mm, log(Boston$crim[train]), alpha = 0) p &lt;- predict(fit2, model.matrix(log(crim) ~ ., data = Boston[test, ]), s = fit2$lambda.min) mean((p - log(Boston$crim[test])) ^ 2) ## [1] 0.6511807 mm &lt;- model.matrix(log(crim) ~ ., data = Boston[train, ]) fit3 &lt;- cv.glmnet(mm, log(Boston$crim[train]), alpha = 1) p &lt;- predict(fit3, model.matrix(log(crim) ~ ., data = Boston[test, ]), s = fit3$lambda.min) mean((p - log(Boston$crim[test])) ^ 2) ## [1] 0.6494337 fit4 &lt;- pcr(log(crim) ~ ., data = Boston[train, ], scale = TRUE, validation = &quot;CV&quot;) validationplot(fit4, val.type = &quot;MSEP&quot;) p &lt;- predict(fit4, Boston[test, ], ncomp = 8) mean((p - log(Boston$crim[test]))^2) ## [1] 0.6561043 fit5 &lt;- plsr(log(crim) ~ ., data = Boston[train, ], scale = TRUE, validation = &quot;CV&quot;) validationplot(fit5, val.type = &quot;MSEP&quot;) p &lt;- predict(fit5, Boston[test, ], ncomp = 6) mean((p - log(Boston$crim[test]))^2) ## [1] 0.6773353 In this case lasso (alpha = 1) seems to perform very slightly better than un-penalized regression. Some coefficients have been dropped: coef(fit3, s = fit3$lambda.min) ## 14 x 1 sparse Matrix of class &quot;dgCMatrix&quot; ## s1 ## (Intercept) -4.713172675 ## (Intercept) . ## zn -0.011043739 ## indus 0.022515402 ## chas . ## nox 3.856157215 ## rm . ## age 0.004210529 ## dis . ## rad 0.145604750 ## tax . ## ptratio -0.031787696 ## lstat 0.036112321 ## medv 0.004304181 Does your chosen model involve all of the features in the data set? Why or why not? Not all features are included due to the lasso penalization. "],["moving-beyond-linearity.html", "7 Moving Beyond Linearity 7.1 Conceptual 7.2 Applied", " 7 Moving Beyond Linearity 7.1 Conceptual 7.1.1 Question 1 It was mentioned in the chapter that a cubic regression spline with one knot at \\(\\xi\\) can be obtained using a basis of the form \\(x, x^2, x^3, (x-\\xi)^3_+\\), where \\((x-\\xi)^3_+ = (x-\\xi)^3\\) if \\(x&gt;\\xi\\) and equals 0 otherwise. We will now show that a function of the form \\[ f(x)=\\beta_0 +\\beta_1x+\\beta_2x^2 +\\beta_3x^3 +\\beta_4(x-\\xi)^3_+ \\] is indeed a cubic regression spline, regardless of the values of \\(\\beta_0, \\beta_1, \\beta_2, \\beta_3,\\beta_4\\). Find a cubic polynomial \\[ f_1(x) = a_1 + b_1x + c_1x^2 + d_1x^3 \\] such that \\(f(x) = f_1(x)\\) for all \\(x \\le \\xi\\). Express \\(a_1,b_1,c_1,d_1\\) in terms of \\(\\beta_0, \\beta_1, \\beta_2, \\beta_3, \\beta_4\\). In this case, for \\(x \\le \\xi\\), the cubic polynomial simply has terms \\(a_1 = \\beta_0\\), \\(b_1 = \\beta_1\\), \\(c_1 = \\beta_2\\), \\(d_1 = \\beta_3\\) Find a cubic polynomial \\[ f_2(x) = a_2 + b_2x + c_2x^2 + d_2x^3 \\] such that \\(f(x) = f_2(x)\\) for all \\(x &gt; \\xi\\). Express \\(a_2, b_2, c_2, d_2\\) in terms of \\(\\beta_0, \\beta_1, \\beta_2, \\beta_3, \\beta_4\\). We have now established that \\(f(x)\\) is a piecewise polynomial. For \\(x \\gt \\xi\\), the cubic polynomial would be (we include the \\(\\beta_4\\) term). \\[\\begin{align} f(x) = &amp; \\beta_0 + \\beta_1x + \\beta_2x^2 + \\beta_3x^3 + \\beta_4(x-\\xi)^3 \\\\ = &amp; \\beta_0 + \\beta_1x + \\beta_2x^2 + + \\beta_4(x^3 - 3x^2\\xi + 3x\\xi^2 -\\xi^3) \\\\ = &amp; \\beta_0 - \\beta_4\\xi^3 + (\\beta_1 + 3\\beta_4\\xi^2)x + (\\beta_2 - 3\\beta_4\\xi)x^2 + (\\beta_3 + \\beta_4)x^3 \\end{align}\\] Therefore, \\(a_1 = \\beta_0 - \\beta_4\\xi^3\\), \\(b_1 = \\beta_1 + 3\\beta_4\\xi^2\\), \\(c_1 = \\beta_2 - 3\\beta_4\\xi\\), \\(d_1 = \\beta_3 + \\beta_4\\) Show that \\(f_1(\\xi) = f_2(\\xi)\\). That is, \\(f(x)\\) is continuous at \\(\\xi\\). To do this, we replace \\(x\\) with \\(\\xi\\) in the above equations and simplify. \\[\\begin{align} f_1(\\xi) = \\beta_0 + \\beta_1\\xi + \\beta_2\\xi^2 + \\beta_3\\xi^3 \\end{align}\\] \\[\\begin{align} f_2(\\xi) = &amp; \\beta_0 - \\beta_4\\xi^3 + (\\beta_1 + 3\\beta_4\\xi^2)\\xi + (\\beta_2 - 3\\beta_4\\xi)\\xi^2 + (\\beta_3 + \\beta_4)\\xi^3 \\\\ = &amp; \\beta_0 - \\beta_4\\xi^3 + \\beta_1\\xi + 3\\beta_4\\xi^3 + \\beta_2\\xi^2 - 3\\beta_4\\xi^3 + \\beta_3\\xi^3 + \\beta_4\\xi^3 \\\\ = &amp; \\beta_0 + \\beta_1\\xi + \\beta_2\\xi^2 + \\beta_3\\xi^3 \\end{align}\\] Show that \\(f_1&#39;(\\xi) = f_2&#39;(\\xi)\\). That is, \\(f&#39;(x)\\) is continuous at \\(\\xi\\). To do this we differentiate the above with respect to \\(x\\). \\[ f_1&#39;(x) = \\beta_1 + 2\\beta_2x + 3\\beta_3x^2 f_1&#39;(\\xi) = \\beta_1 + 2\\beta_2\\xi + 3\\beta_3\\xi^2 \\] \\[\\begin{align} f_2&#39;(x) &amp; = \\beta_1 + 3\\beta_4\\xi^2 + 2(\\beta_2 - 3\\beta_4\\xi)x + 3(\\beta_3 + \\beta_4)x^2 \\\\ f_2&#39;(\\xi) &amp; = \\beta_1 + 3\\beta_4\\xi^2 + 2(\\beta_2 - 3\\beta_4\\xi)\\xi + 3(\\beta_3 + \\beta_4)\\xi^2 \\\\ &amp; = \\beta_1 + 3\\beta_4\\xi^2 + 2\\beta_2\\xi - 6\\beta_4\\xi^2 + 3\\beta_3\\xi^2 + 3\\beta_4\\xi^2 \\\\ &amp; = \\beta_1 + 2\\beta_2\\xi + 3\\beta_3\\xi^2 \\end{align}\\] Show that \\(f_1&#39;&#39;(\\xi) = f_2&#39;&#39;(\\xi)\\). That is, \\(f&#39;&#39;(x)\\) is continuous at \\(\\xi\\). Therefore, \\(f(x)\\) is indeed a cubic spline. \\[ f_1&#39;(x) = 2\\beta_2x + 6\\beta_3x \\\\ f_1&#39;&#39;(\\xi) = 2\\beta_2\\xi + 6\\beta_3\\xi \\] \\[ f_2&#39;&#39;(x) = 2\\beta_2 - 6\\beta_4\\xi + 6(\\beta_3 + \\beta_4)x \\\\ \\] \\[\\begin{align} f_2&#39;&#39;(\\xi) &amp; = 2\\beta_2 - 6\\beta_4\\xi + 6\\beta_3\\xi + 6\\beta_4\\xi \\\\ &amp; = 2\\beta_2 + 6\\beta_3\\xi \\end{align}\\] Hint: Parts (d) and (e) of this problem require knowledge of single-variable calculus. As a reminder, given a cubic polynomial \\[f_1(x) = a_1 + b_1x + c_1x^2 + d_1x^3,\\] the first derivative takes the form \\[f_1&#39;(x) = b_1 + 2c_1x + 3d_1x^2\\] and the second derivative takes the form \\[f_1&#39;&#39;(x) = 2c_1 + 6d_1x.\\] 7.1.2 Question 2 Suppose that a curve \\(\\hat{g}\\) is computed to smoothly fit a set of \\(n\\) points using the following formula: \\[ \\DeclareMathOperator*{\\argmin}{arg\\,min} % Jan Hlavacek \\hat{g} = \\argmin_g \\left(\\sum_{i=1}^n (y_i - g(x_i))^2 + \\lambda \\int \\left[ g^{(m)}(x) \\right]^2 dx \\right), \\] where \\(g^{(m)}\\) represents the \\(m\\)th derivative of \\(g\\) (and \\(g^{(0)} = g\\)). Provide example sketches of \\(\\hat{g}\\) in each of the following scenarios. \\(\\lambda=\\infty, m=0\\). Here we penalize the \\(g\\) and a infinite \\(\\lambda\\) means that this penalty dominates. This means that the \\(\\hat{g}\\) will be 0. \\(\\lambda=\\infty, m=1\\). Here we penalize the first derivative (the slope) of \\(g\\) and a infinite \\(\\lambda\\) means that this penalty dominates. Thus the slope will be 0 (and otherwise best fitting \\(x\\), i.e. at the mean of \\(x\\)). \\(\\lambda=\\infty, m=2\\). Here we penalize the second derivative (the change of slope) of \\(g\\) and a infinite \\(\\lambda\\) means that this penalty dominates. Thus the line will be straight (and otherwise best fitting \\(x\\)). \\(\\lambda=\\infty, m=3\\). Here we penalize the third derivative (the change of the change of slope) of \\(g\\) and a infinite \\(\\lambda\\) means that this penalty dominates. In other words, the curve will have a consistent rate of change (e.g. a quadratic function or similar). \\(\\lambda=0, m=3\\). Here we penalize the third derivative, but a value of \\(\\lambda = 0\\) means that there is no penalty. As a result, the curve is able to interpolate all points. 7.1.3 Question 3 Suppose we fit a curve with basis functions \\(b_1(X) = X\\), \\(b_2(X) = (X - 1)^2I(X \\geq 1)\\). (Note that \\(I(X \\geq 1)\\) equals 1 for \\(X \\geq 1\\) and 0 otherwise.) We fit the linear regression model \\[Y = \\beta_0 +\\beta_1b_1(X) + \\beta_2b_2(X) + \\epsilon,\\] and obtain coefficient estimates \\(\\hat{\\beta}_0 = 1, \\hat{\\beta}_1 = 1, \\hat{\\beta}_2 = -2\\). Sketch the estimated curve between \\(X = -2\\) and \\(X = 2\\). Note the intercepts, slopes, and other relevant information. x &lt;- seq(-2, 2, length.out = 1000) f &lt;- function(x) 1 + x + -2 * (x - 1)^2 * I(x &gt;= 1) plot(x, f(x), type = &quot;l&quot;) grid() 7.1.4 Question 4 Suppose we fit a curve with basis functions \\(b_1(X) = I(0 \\leq X \\leq 2) - (X -1)I(1 \\leq X \\leq 2),\\) \\(b_2(X) = (X -3)I(3 \\leq X \\leq 4) + I(4 \\lt X \\leq 5)\\). We fit the linear regression model \\[Y = \\beta_0 +\\beta_1b_1(X) + \\beta_2b_2(X) + \\epsilon,\\] and obtain coefficient estimates \\(\\hat{\\beta}_0 = 1, \\hat{\\beta}_1 = 1, \\hat{\\beta}_2 = 3\\). Sketch the estimated curve between \\(X = -2\\) and \\(X = 6\\). Note the intercepts, slopes, and other relevant information. x &lt;- seq(-2, 6, length.out = 1000) b1 &lt;- function(x) I(0 &lt;= x &amp; x &lt;= 2) - (x - 1) * I(1 &lt;= x &amp; x &lt;= 2) b2 &lt;- function(x) (x - 3) * I(3 &lt;= x &amp; x &lt;= 4) + I(4 &lt; x &amp; x &lt;= 5) f &lt;- function(x) 1 + 1*b1(x) + 3*b2(x) plot(x, f(x), type = &quot;l&quot;) grid() 7.1.5 Question 5 Consider two curves, \\(\\hat{g}\\) and \\(\\hat{g}_2\\), defined by \\[ \\hat{g}_1 = \\argmin_g \\left(\\sum_{i=1}^n (y_i - g(x_i))^2 + \\lambda \\int \\left[ g^{(3)}(x) \\right]^2 dx \\right), \\] \\[ \\hat{g}_2 = \\argmin_g \\left(\\sum_{i=1}^n (y_i - g(x_i))^2 + \\lambda \\int \\left[ g^{(4)}(x) \\right]^2 dx \\right), \\] where \\(g^{(m)}\\) represents the \\(m\\)th derivative of \\(g\\). As \\(\\lambda \\to \\infty\\), will \\(\\hat{g}_1\\) or \\(\\hat{g}_2\\) have the smaller training RSS? \\(\\hat{g}_2\\) is more flexible (by penalizing a higher derivative of \\(g\\)) and so will have a smaller training RSS. As \\(\\lambda \\to \\infty\\), will \\(\\hat{g}_1\\) or \\(\\hat{g}_2\\) have the smaller test RSS? We cannot tell which function will produce a smaller test RSS, but there is chance that \\(\\hat{g}_1\\) will if \\(\\hat{g}_2\\) overfits the data. For \\(\\lambda = 0\\), will \\(\\hat{g}_1\\) or \\(\\hat{g}_2\\) have the smaller training and test RSS? When \\(\\lambda = 0\\) there is no penalty, so both functions will give the same result: perfect interpolation of the training data. Thus training RSS will be 0 but test RSS could be high. 7.2 Applied 7.2.1 Question 6 In this exercise, you will further analyze the Wage data set considered throughout this chapter. Perform polynomial regression to predict wage using age. Use cross-validation to select the optimal degree \\(d\\) for the polynomial. What degree was chosen, and how does this compare to the results of hypothesis testing using ANOVA? Make a plot of the resulting polynomial fit to the data. library(ISLR2) library(boot) library(ggplot2) set.seed(42) res &lt;- sapply(1:6, function(i) { fit &lt;- glm(wage ~ poly(age, i), data = Wage) cv.glm(Wage, fit, K = 5)$delta[1] }) which.min(res) ## [1] 6 plot(1:6, res, xlab = &quot;Degree&quot;, ylab = &quot;Test MSE&quot;, type = &quot;l&quot;) abline(v = which.min(res), col = &quot;red&quot;, lty = 2) fit &lt;- glm(wage ~ poly(age, which.min(res)), data = Wage) plot(Wage$age, Wage$wage, pch = 19, cex = 0.4, col = alpha(&quot;steelblue&quot;, 0.4)) points(1:100, predict(fit, data.frame(age = 1:100)), type = &quot;l&quot;, col = &quot;red&quot;) summary(glm(wage ~ poly(age, 6), data = Wage)) ## ## Call: ## glm(formula = wage ~ poly(age, 6), data = Wage) ## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -98.521 -24.536 -4.848 15.471 202.108 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 111.7036 0.7286 153.316 &lt; 2e-16 *** ## poly(age, 6)1 447.0679 39.9063 11.203 &lt; 2e-16 *** ## poly(age, 6)2 -478.3158 39.9063 -11.986 &lt; 2e-16 *** ## poly(age, 6)3 125.5217 39.9063 3.145 0.00167 ** ## poly(age, 6)4 -77.9112 39.9063 -1.952 0.05099 . ## poly(age, 6)5 -35.8129 39.9063 -0.897 0.36956 ## poly(age, 6)6 62.7077 39.9063 1.571 0.11620 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for gaussian family taken to be 1592.512) ## ## Null deviance: 5222086 on 2999 degrees of freedom ## Residual deviance: 4766389 on 2993 degrees of freedom ## AIC: 30642 ## ## Number of Fisher Scoring iterations: 2 fit1 &lt;- lm(wage ~ poly(age, 1), data = Wage) fit2 &lt;- lm(wage ~ poly(age, 2), data = Wage) fit3 &lt;- lm(wage ~ poly(age, 3), data = Wage) fit4 &lt;- lm(wage ~ poly(age, 4), data = Wage) fit5 &lt;- lm(wage ~ poly(age, 5), data = Wage) anova(fit1, fit2, fit3, fit4, fit5) ## Analysis of Variance Table ## ## Model 1: wage ~ poly(age, 1) ## Model 2: wage ~ poly(age, 2) ## Model 3: wage ~ poly(age, 3) ## Model 4: wage ~ poly(age, 4) ## Model 5: wage ~ poly(age, 5) ## Res.Df RSS Df Sum of Sq F Pr(&gt;F) ## 1 2998 5022216 ## 2 2997 4793430 1 228786 143.5931 &lt; 2.2e-16 *** ## 3 2996 4777674 1 15756 9.8888 0.001679 ** ## 4 2995 4771604 1 6070 3.8098 0.051046 . ## 5 2994 4770322 1 1283 0.8050 0.369682 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 The selected degree is 4. When testing with ANOVA, degrees 1, 2 and 3 are highly significant and 4 is marginal. Fit a step function to predict wage using age, and perform cross-validation to choose the optimal number of cuts. Make a plot of the fit obtained. set.seed(42) res &lt;- sapply(2:10, function(i) { Wage$cats &lt;- cut(Wage$age, i) fit &lt;- glm(wage ~ cats, data = Wage) cv.glm(Wage, fit, K = 5)$delta[1] }) names(res) &lt;- 2:10 plot(2:10, res, xlab = &quot;Cuts&quot;, ylab = &quot;Test MSE&quot;, type = &quot;l&quot;) which.min(res) ## 8 ## 7 abline(v = names(which.min(res)), col = &quot;red&quot;, lty = 2) fit &lt;- glm(wage ~ cut(age, 8), data = Wage) plot(Wage$age, Wage$wage, pch = 19, cex = 0.4, col = alpha(&quot;steelblue&quot;, 0.4)) points(18:80, predict(fit, data.frame(age = 18:80)), type = &quot;l&quot;, col = &quot;red&quot;) 7.2.2 Question 7 The Wage data set contains a number of other features not explored in this chapter, such as marital status (maritl), job class (jobclass), and others. Explore the relationships between some of these other predictors and wage, and use non-linear fitting techniques in order to fit flexible models to the data. Create plots of the results obtained, and write a summary of your findings. plot(Wage$year, Wage$wage, pch = 19, cex = 0.4, col = alpha(&quot;steelblue&quot;, 0.4)) plot(Wage$age, Wage$wage, pch = 19, cex = 0.4, col = alpha(&quot;steelblue&quot;, 0.4)) plot(Wage$maritl, Wage$wage, pch = 19, cex = 0.4, col = alpha(&quot;steelblue&quot;, 0.4)) plot(Wage$jobclass, Wage$wage, pch = 19, cex = 0.4, col = alpha(&quot;steelblue&quot;, 0.4)) plot(Wage$education, Wage$wage, pch = 19, cex = 0.4, col = alpha(&quot;steelblue&quot;, 0.4)) We have a mix of categorical and continuous variables and also want to incorporate non-linear aspects of the continuous variables. A GAM is a good choice to model this situation. library(gam) ## Loading required package: splines ## Loading required package: foreach ## Loaded gam 1.20.2 fit0 &lt;- gam(wage ~ s(year, 4) + s(age, 5) + education, data = Wage) fit2 &lt;- gam(wage ~ s(year, 4) + s(age, 5) + education + maritl, data = Wage) fit1 &lt;- gam(wage ~ s(year, 4) + s(age, 5) + education + jobclass, data = Wage) fit3 &lt;- gam(wage ~ s(year, 4) + s(age, 5) + education + jobclass + maritl, data = Wage) anova(fit0, fit1, fit2, fit3) ## Analysis of Deviance Table ## ## Model 1: wage ~ s(year, 4) + s(age, 5) + education ## Model 2: wage ~ s(year, 4) + s(age, 5) + education + jobclass ## Model 3: wage ~ s(year, 4) + s(age, 5) + education + maritl ## Model 4: wage ~ s(year, 4) + s(age, 5) + education + jobclass + maritl ## Resid. Df Resid. Dev Df Deviance Pr(&gt;Chi) ## 1 2986 3689770 ## 2 2985 3677553 1 12218 0.0014286 ** ## 3 2982 3595688 3 81865 1.071e-14 *** ## 4 2981 3581781 1 13907 0.0006687 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 par(mfrow = c(2, 3)) plot(fit3, se = TRUE, col = &quot;blue&quot;) 7.2.3 Question 8 Fit some of the non-linear models investigated in this chapter to the Auto data set. Is there evidence for non-linear relationships in this data set? Create some informative plots to justify your answer. Here we want to explore a range of non-linear models. First let’s look at the relationships between the variables in the data. pairs(Auto, cex = 0.4, pch = 19) It does appear that there are some non-linear relationships (e.g. horsepower / weight and mpg). We will pick one variable (horsepower) to predict mpg and try the range of models discussed in this chapter. We will measure test MSE through cross-validation to compare the models. library(tidyverse) ## ── Attaching packages ─────────────────────────────────────── tidyverse 1.3.2 ── ## ✔ tibble 3.1.8 ✔ dplyr 1.0.10 ## ✔ tidyr 1.2.1 ✔ stringr 1.4.1 ## ✔ readr 2.1.3 ✔ forcats 0.5.2 ## ✔ purrr 0.3.5 ## ── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ── ## ✖ purrr::accumulate() masks foreach::accumulate() ## ✖ dplyr::filter() masks stats::filter() ## ✖ dplyr::lag() masks stats::lag() ## ✖ purrr::when() masks foreach::when() set.seed(42) fit &lt;- glm(mpg ~ horsepower, data = Auto) err &lt;- cv.glm(Auto, fit, K = 10)$delta[1] fit1 &lt;- glm(mpg ~ poly(horsepower, 4), data = Auto) err1 &lt;- cv.glm(Auto, fit1, K = 10)$delta[1] q &lt;- quantile(Auto$horsepower) Auto$hp_cats &lt;- cut(Auto$horsepower, breaks = q, include.lowest = TRUE) fit2 &lt;- glm(mpg ~ hp_cats, data = Auto) err2 &lt;- cv.glm(Auto, fit2, K = 10)$delta[1] fit3 &lt;- glm(mpg ~ bs(horsepower, df = 4), data = Auto) err3 &lt;- cv.glm(Auto, fit3, K = 10)$delta[1] ## Warning in bs(horsepower, degree = 3L, knots = c(`50%` = 92), Boundary.knots = ## c(46L, : some &#39;x&#39; values beyond boundary knots may cause ill-conditioned bases ## Warning in bs(horsepower, degree = 3L, knots = c(`50%` = 92), Boundary.knots = ## c(46L, : some &#39;x&#39; values beyond boundary knots may cause ill-conditioned bases fit4 &lt;- glm(mpg ~ ns(horsepower, 4), data = Auto) err4 &lt;- cv.glm(Auto, fit4, K = 10)$delta[1] fit5 &lt;- gam(mpg ~ s(horsepower, df = 4), data = Auto) # rough 10-fold cross-validation for gam. err5 &lt;- mean(replicate(10, { b &lt;- cut(sample(seq_along(Auto$horsepower)), 10) pred &lt;- numeric() for (i in 1:10) { train &lt;- b %in% levels(b)[-i] f &lt;- gam(mpg ~ s(horsepower, df = 4), data = Auto[train, ]) pred[!train] &lt;- predict(f, Auto[!train, ]) } mean((Auto$mpg - pred)^2) # MSE })) c(err, err1, err2, err3, err4, err5) ## [1] 24.38418 19.94222 20.37940 18.92802 19.33556 19.02999 anova(fit, fit1, fit2, fit3, fit4, fit5) ## Analysis of Deviance Table ## ## Model 1: mpg ~ horsepower ## Model 2: mpg ~ poly(horsepower, 4) ## Model 3: mpg ~ hp_cats ## Model 4: mpg ~ bs(horsepower, df = 4) ## Model 5: mpg ~ ns(horsepower, 4) ## Model 6: mpg ~ s(horsepower, df = 4) ## Resid. Df Resid. Dev Df Deviance ## 1 390 9385.9 ## 2 387 7399.5 3.00000000 1986.39 ## 3 388 7805.4 -1.00000000 -405.92 ## 4 387 7276.5 1.00000000 528.94 ## 5 387 7248.6 0.00000000 27.91 ## 6 387 7267.7 0.00013612 -19.10 x &lt;- seq(min(Auto$horsepower), max(Auto$horsepower), length.out=1000) pred &lt;- data.frame( x = x, &quot;Linear&quot; = predict(fit, data.frame(horsepower = x)), &quot;Polynomial&quot; = predict(fit1, data.frame(horsepower = x)), &quot;Step&quot; = predict(fit2, data.frame(hp_cats = cut(x, breaks = q, include.lowest = TRUE))), &quot;Regression spline&quot; = predict(fit3, data.frame(horsepower = x)), &quot;Natural spline&quot; = predict(fit4, data.frame(horsepower = x)), &quot;Smoothing spline&quot; = predict(fit5, data.frame(horsepower = x)), check.names = FALSE ) pred &lt;- pivot_longer(pred, -x) ggplot(Auto, aes(horsepower, mpg)) + geom_point(color = alpha(&quot;steelblue&quot;, 0.4)) + geom_line(data = pred, aes(x, value, color = name)) + theme_bw() 7.2.4 Question 9 This question uses the variables dis (the weighted mean of distances to five Boston employment centers) and nox (nitrogen oxides concentration in parts per 10 million) from the Boston data. We will treat dis as the predictor and nox as the response. Use the poly() function to fit a cubic polynomial regression to predict nox using dis. Report the regression output, and plot the resulting data and polynomial fits. fit &lt;- glm(nox ~ poly(dis, 3), data = Boston) summary(fit) ## ## Call: ## glm(formula = nox ~ poly(dis, 3), data = Boston) ## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -0.121130 -0.040619 -0.009738 0.023385 0.194904 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 0.554695 0.002759 201.021 &lt; 2e-16 *** ## poly(dis, 3)1 -2.003096 0.062071 -32.271 &lt; 2e-16 *** ## poly(dis, 3)2 0.856330 0.062071 13.796 &lt; 2e-16 *** ## poly(dis, 3)3 -0.318049 0.062071 -5.124 4.27e-07 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for gaussian family taken to be 0.003852802) ## ## Null deviance: 6.7810 on 505 degrees of freedom ## Residual deviance: 1.9341 on 502 degrees of freedom ## AIC: -1370.9 ## ## Number of Fisher Scoring iterations: 2 plot(nox ~ dis, data = Boston, col = alpha(&quot;steelblue&quot;, 0.4), pch = 19) x &lt;- seq(min(Boston$dis), max(Boston$dis), length.out = 1000) lines(x, predict(fit, data.frame(dis = x)), col = &quot;red&quot;, lty = 2) Plot the polynomial fits for a range of different polynomial degrees (say, from 1 to 10), and report the associated residual sum of squares. fits &lt;- lapply(1:10, function(i) glm(nox ~ poly(dis, i), data = Boston)) x &lt;- seq(min(Boston$dis), max(Boston$dis), length.out=1000) pred &lt;- data.frame(lapply(fits, function(fit) predict(fit, data.frame(dis = x)))) colnames(pred) &lt;- 1:10 pred$x &lt;- x pred &lt;- pivot_longer(pred, !x) ggplot(Boston, aes(dis, nox)) + geom_point(color = alpha(&quot;steelblue&quot;, 0.4)) + geom_line(data = pred, aes(x, value, color = name)) + theme_bw() # residual sum of squares do.call(anova, fits)[, 2] ## [1] 2.768563 2.035262 1.934107 1.932981 1.915290 1.878257 1.849484 1.835630 ## [9] 1.833331 1.832171 Perform cross-validation or another approach to select the optimal degree for the polynomial, and explain your results. res &lt;- sapply(1:10, function(i) { fit &lt;- glm(nox ~ poly(dis, i), data = Boston) cv.glm(Boston, fit, K = 10)$delta[1] }) which.min(res) ## [1] 4 The optimal degree is 3 based on cross-validation. Higher values tend to lead to overfitting. Use the bs() function to fit a regression spline to predict nox using dis. Report the output for the fit using four degrees of freedom. How did you choose the knots? Plot the resulting fit. fit &lt;- glm(nox ~ bs(dis, df = 4), data = Boston) summary(fit) ## ## Call: ## glm(formula = nox ~ bs(dis, df = 4), data = Boston) ## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -0.124622 -0.039259 -0.008514 0.020850 0.193891 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 0.73447 0.01460 50.306 &lt; 2e-16 *** ## bs(dis, df = 4)1 -0.05810 0.02186 -2.658 0.00812 ** ## bs(dis, df = 4)2 -0.46356 0.02366 -19.596 &lt; 2e-16 *** ## bs(dis, df = 4)3 -0.19979 0.04311 -4.634 4.58e-06 *** ## bs(dis, df = 4)4 -0.38881 0.04551 -8.544 &lt; 2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for gaussian family taken to be 0.003837874) ## ## Null deviance: 6.7810 on 505 degrees of freedom ## Residual deviance: 1.9228 on 501 degrees of freedom ## AIC: -1371.9 ## ## Number of Fisher Scoring iterations: 2 plot(nox ~ dis, data = Boston, col = alpha(&quot;steelblue&quot;, 0.4), pch = 19) x &lt;- seq(min(Boston$dis), max(Boston$dis), length.out = 1000) lines(x, predict(fit, data.frame(dis = x)), col = &quot;red&quot;, lty = 2) Knots are chosen based on quantiles of the data. Now fit a regression spline for a range of degrees of freedom, and plot the resulting fits and report the resulting RSS. Describe the results obtained. fits &lt;- lapply(3:10, function(i) { glm(nox ~ bs(dis, df = i), data = Boston) }) x &lt;- seq(min(Boston$dis), max(Boston$dis), length.out = 1000) pred &lt;- data.frame(lapply(fits, function(fit) predict(fit, data.frame(dis = x)))) colnames(pred) &lt;- 3:10 pred$x &lt;- x pred &lt;- pivot_longer(pred, !x) ggplot(Boston, aes(dis, nox)) + geom_point(color = alpha(&quot;steelblue&quot;, 0.4)) + geom_line(data = pred, aes(x, value, color = name)) + theme_bw() At high numbers of degrees of freedom the splines overfit the data (particularly at extreme ends of the distribution of the predictor variable). Perform cross-validation or another approach in order to select the best degrees of freedom for a regression spline on this data. Describe your results. set.seed(42) err &lt;- sapply(3:10, function(i) { fit &lt;- glm(nox ~ bs(dis, df = i), data = Boston) suppressWarnings(cv.glm(Boston, fit, K = 10)$delta[1]) }) which.min(err) ## [1] 8 This approach would select 4 degrees of freedom for the spline. 7.2.5 Question 10 This question relates to the College data set. Split the data into a training set and a test set. Using out-of-state tuition as the response and the other variables as the predictors, perform forward stepwise selection on the training set in order to identify a satisfactory model that uses just a subset of the predictors. library(leaps) # helper function to predict from a regsubsets model predict.regsubsets &lt;- function(object, newdata, id, ...) { form &lt;- as.formula(object$call[[2]]) mat &lt;- model.matrix(form, newdata) coefi &lt;- coef(object, id = id) xvars &lt;- names(coefi) mat[, xvars] %*% coefi } set.seed(42) train &lt;- rep(TRUE, nrow(College)) train[sample(1:nrow(College), nrow(College) * 1 / 3)] &lt;- FALSE fit &lt;- regsubsets(Outstate ~ ., data = College[train, ], nvmax = 17, method = &quot;forward&quot;) plot(summary(fit)$bic, type = &quot;b&quot;) which.min(summary(fit)$bic) ## [1] 11 # or via cross-validation err &lt;- sapply(1:17, function(i) { x &lt;- coef(fit, id = i) mean((College$Outstate[!train] - predict(fit, College[!train, ], i))^2) }) which.min(err) ## [1] 16 min(summary(fit)$bic) ## [1] -690.9375 For the sake of simplicity we’ll choose 6 coef(fit, id = 6) ## (Intercept) PrivateYes Room.Board PhD perc.alumni ## -3540.0544008 2736.4231642 0.9061752 33.7848157 47.1998115 ## Expend Grad.Rate ## 0.2421685 33.3137332 Fit a GAM on the training data, using out-of-state tuition as the response and the features selected in the previous step as the predictors. Plot the results, and explain your findings. fit &lt;- gam(Outstate ~ Private + s(Room.Board, 2) + s(PhD, 2) + s(perc.alumni, 2) + s(Expend, 2) + s(Grad.Rate, 2), data = College[train, ]) Evaluate the model obtained on the test set, and explain the results obtained. pred &lt;- predict(fit, College[!train, ]) err_gam &lt;- mean((College$Outstate[!train] - pred)^2) plot(err, ylim = c(min(err_gam, err), max(err)), type = &quot;b&quot;) abline(h = err_gam, col = &quot;red&quot;, lty = 2) # r-squared 1 - err_gam / mean((College$Outstate[!train] - mean(College$Outstate[!train]))^2) ## [1] 0.7655905 For which variables, if any, is there evidence of a non-linear relationship with the response? summary(fit) ## ## Call: gam(formula = Outstate ~ Private + s(Room.Board, 2) + s(PhD, ## 2) + s(perc.alumni, 2) + s(Expend, 2) + s(Grad.Rate, 2), ## data = College[train, ]) ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -7112.59 -1188.98 33.13 1238.54 8738.65 ## ## (Dispersion Parameter for gaussian family taken to be 3577008) ## ## Null Deviance: 8471793308 on 517 degrees of freedom ## Residual Deviance: 1809966249 on 506.0001 degrees of freedom ## AIC: 9300.518 ## ## Number of Local Scoring Iterations: NA ## ## Anova for Parametric Effects ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## Private 1 2327235738 2327235738 650.610 &lt; 2.2e-16 *** ## s(Room.Board, 2) 1 1741918028 1741918028 486.976 &lt; 2.2e-16 *** ## s(PhD, 2) 1 668408518 668408518 186.863 &lt; 2.2e-16 *** ## s(perc.alumni, 2) 1 387819183 387819183 108.420 &lt; 2.2e-16 *** ## s(Expend, 2) 1 625813340 625813340 174.954 &lt; 2.2e-16 *** ## s(Grad.Rate, 2) 1 111881207 111881207 31.278 3.664e-08 *** ## Residuals 506 1809966249 3577008 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Anova for Nonparametric Effects ## Npar Df Npar F Pr(F) ## (Intercept) ## Private ## s(Room.Board, 2) 1 2.224 0.13648 ## s(PhD, 2) 1 5.773 0.01664 * ## s(perc.alumni, 2) 1 0.365 0.54581 ## s(Expend, 2) 1 61.182 3.042e-14 *** ## s(Grad.Rate, 2) 1 4.126 0.04274 * ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Non-linear relationships are significant for Expend and PhD. 7.2.6 Question 11 In Section 7.7, it was mentioned that GAMs are generally fit using a backfitting approach. The idea behind backfitting is actually quite simple. We will now explore backfitting in the context of multiple linear regression. Suppose that we would like to perform multiple linear regression, but we do not have software to do so. Instead, we only have software to perform simple linear regression. Therefore, we take the following iterative approach: we repeatedly hold all but one coefficient estimate fixed at its current value, and update only that coefficient estimate using a simple linear regression. The process is continued until convergence—that is, until the coefficient estimates stop changing. We now try this out on a toy example. Generate a response \\(Y\\) and two predictors \\(X_1\\) and \\(X_2\\), with \\(n = 100\\). set.seed(42) x1 &lt;- rnorm(100) x2 &lt;- rnorm(100) y &lt;- 2 + 0.2 * x1 + 4 * x2 + rnorm(100) Initialize \\(\\hat{\\beta}_1\\) to take on a value of your choice. It does not matter 1 what value you choose. beta1 &lt;- 20 Keeping \\(\\hat{\\beta}_1\\) fixed, fit the model \\[Y - \\hat{\\beta}_1X_1 = \\beta_0 + \\beta_2X_2 + \\epsilon.\\] You can do this as follows: &gt; a &lt;- y - beta1 * x1 &gt; beta2 &lt;- lm(a ~ x2)$coef[2] a &lt;- y - beta1*x1 beta2 &lt;- lm(a ~ x2)$coef[2] Keeping \\(\\hat{\\beta}_2\\) fixed, fit the model \\[Y - \\hat{\\beta}_2X_2 = \\beta_0 + \\beta_1 X_1 + \\epsilon.\\] You can do this as follows: &gt; a &lt;- y - beta2 * x2 &gt; beta1 &lt;- lm(a ~ x1)$coef[2] a &lt;- y - beta2 * x2 beta1 &lt;- lm(a ~ x1)$coef[2] Write a for loop to repeat (c) and (d) 1,000 times. Report the estimates of \\(\\hat{\\beta}_0, \\hat{\\beta}_1,\\) and \\(\\hat{\\beta}_2\\) at each iteration of the for loop. Create a plot in which each of these values is displayed, with \\(\\hat{\\beta}_0, \\hat{\\beta}_1,\\) and \\(\\hat{\\beta}_2\\) each shown in a different color. res &lt;- matrix(NA, nrow = 1000, ncol = 3) colnames(res) &lt;- c(&quot;beta0&quot;, &quot;beta1&quot;, &quot;beta2&quot;) beta1 &lt;- 20 for (i in 1:1000) { beta2 &lt;- lm(y - beta1*x1 ~ x2)$coef[2] beta1 &lt;- lm(y - beta2*x2 ~ x1)$coef[2] beta0 &lt;- lm(y - beta2*x2 ~ x1)$coef[1] res[i, ] &lt;- c(beta0, beta1, beta2) } res &lt;- as.data.frame(res) res$Iteration &lt;- 1:1000 res &lt;- pivot_longer(res, !Iteration) p &lt;- ggplot(res, aes(x=Iteration, y=value, color=name)) + geom_line() + geom_point() + scale_x_continuous(trans = &quot;log10&quot;) p Compare your answer in (e) to the results of simply performing multiple linear regression to predict \\(Y\\) using \\(X_1\\) and \\(X_2\\). Use the abline() function to overlay those multiple linear regression coefficient estimates on the plot obtained in (e). fit &lt;- lm(y ~ x1 + x2) coef(fit) ## (Intercept) x1 x2 ## 2.00176627 0.05629075 4.08529318 p + geom_hline(yintercept = coef(fit), lty = 2) On this data set, how many backfitting iterations were required in order to obtain a “good” approximation to the multiple regression coefficient estimates? In this case, good estimates were obtained after 3 iterations. 7.2.7 Question 12 This problem is a continuation of the previous exercise. In a toy example with \\(p = 100\\), show that one can approximate the multiple linear regression coefficient estimates by repeatedly performing simple linear regression in a backfitting procedure. How many backfitting iterations are required in order to obtain a “good” approximation to the multiple regression coefficient estimates? Create a plot to justify your answer. set.seed(42) p &lt;- 100 n &lt;- 1000 betas &lt;- rnorm(p) * 5 x &lt;- matrix(rnorm(n * p), ncol = p, nrow = n) y &lt;- (x %*% betas) + rnorm(n) # ignore beta0 for simplicity # multiple regression fit &lt;- lm(y ~ x - 1) coef(fit) ## x1 x2 x3 x4 x5 x6 ## 6.9266184 -2.8428817 1.8686821 3.1466472 1.9601927 -0.5529214 ## x7 x8 x9 x10 x11 x12 ## 7.4786723 -0.4454637 10.0816005 -0.2391234 6.5832468 11.4451280 ## x13 x14 x15 x16 x17 x18 ## -6.9684368 -1.3604495 -0.6310041 3.1786639 -1.4470502 -13.2957027 ## x19 x20 x21 x22 x23 x24 ## -12.2061834 6.5765842 -1.5227262 -8.8855906 -0.8422954 6.1189230 ## x25 x26 x27 x28 x29 x30 ## 9.4395267 -2.1697854 -1.2738835 -8.8457987 2.2851699 -3.1922704 ## x31 x32 x33 x34 x35 x36 ## 2.2812995 3.4695892 5.1162617 -3.0423873 2.4985589 -8.5952764 ## x37 x38 x39 x40 x41 x42 ## -3.9539370 -4.2616463 -12.0038342 0.1981058 1.0559250 -1.8205017 ## x43 x44 x45 x46 x47 x48 ## 3.7739990 -3.6240020 -6.8575534 2.1042998 -4.0228773 7.1880298 ## x49 x50 x51 x52 x53 x54 ## -2.1967821 3.3137115 1.6406524 -3.9402065 7.9067705 3.1815846 ## x55 x56 x57 x58 x59 x60 ## 0.4504175 1.4003479 3.3999814 0.4317695 -14.9255798 1.3816878 ## x61 x62 x63 x64 x65 x66 ## -1.8071634 0.9907740 2.9771540 6.9528872 -3.5956916 6.5283946 ## x67 x68 x69 x70 x71 x72 ## 1.6798820 5.1911857 4.5573256 3.5961319 -5.1909352 -0.4869003 ## x73 x74 x75 x76 x77 x78 ## 3.1472166 -4.7898363 -2.7402076 2.9247173 3.8659938 2.3686379 ## x79 x80 x81 x82 x83 x84 ## -4.4261734 -5.5020688 7.5807239 1.3010702 0.4378713 -0.5856580 ## x85 x86 x87 x88 x89 x90 ## -5.9799328 3.0089329 -1.1230969 -0.8857679 4.7211363 4.1042952 ## x91 x92 x93 x94 x95 x96 ## 6.9492037 -2.3959211 3.2188522 6.9947040 -5.5392641 -4.3114784 ## x97 x98 x99 x100 ## -5.7287292 -7.3148812 0.3454408 3.2830658 # backfitting backfit &lt;- function(x, y, iter = 20) { beta &lt;- matrix(0, ncol = ncol(x), nrow = iter + 1) for (i in 1:iter) { for (k in 1:ncol(x)) { residual &lt;- y - (x[, -k] %*% beta[i, -k]) beta[i + 1, k] &lt;- lm(residual ~ x[, k])$coef[2] } } beta[-1, ] } res &lt;- backfit(x, y) error &lt;- rowMeans(sweep(res, 2, betas)^2) plot(error, log = &quot;x&quot;, type = &quot;b&quot;) # backfitting error error[length(error)] ## [1] 0.001142494 # lm error mean((coef(fit) - betas)^2) ## [1] 0.001138655 We need around 5 to 6 iterations to obtain a good estimate of the coefficients. "],["tree-based-methods.html", "8 Tree-Based Methods 8.1 Conceptual 8.2 Applied", " 8 Tree-Based Methods 8.1 Conceptual 8.1.1 Question 1 Draw an example (of your own invention) of a partition of two-dimensional feature space that could result from recursive binary splitting. Your example should contain at least six regions. Draw a decision tree corresponding to this partition. Be sure to label all aspects of your figures, including the regions \\(R_1, R_2, ...,\\) the cutpoints \\(t_1, t_2, ...,\\) and so forth. Hint: Your result should look something like Figures 8.1 and 8.2. library(showtext) showtext::showtext_auto() library(ggplot2) library(tidyverse) library(ggtree) tree &lt;- ape::read.tree(text = &quot;(((R1:1,R2:1)N1:2,R3:4)N2:2,(R4:2,(R5:1,R6:1)R3:2)N4:5)R;&quot;) tree$node.label &lt;- c(&quot;Age &lt; 40&quot;, &quot;Weight &lt; 100&quot;, &quot;Weight &lt; 70&quot;, &quot;Age &lt; 60&quot;, &quot;Weight &lt; 80&quot;) ggtree(tree, ladderize = FALSE) + scale_x_reverse() + coord_flip() + geom_tiplab(vjust = 2, hjust = 0.5) + geom_text2(aes(label=label, subset=!isTip), hjust = -0.1, vjust = -1) plot(NULL, xlab=&quot;Age (years)&quot;, ylab=&quot;Weight (kg)&quot;, xlim = c(0, 100), ylim = c(40, 160), xaxs = &quot;i&quot;, yaxs = &quot;i&quot;) abline(v = 40, col = &quot;red&quot;, lty = 2) lines(c(0, 40), c(100, 100), col = &quot;blue&quot;, lty = 2) lines(c(0, 40), c(70, 70), col = &quot;blue&quot;, lty = 2) abline(v = 60, col = &quot;red&quot;, lty = 2) lines(c(60, 100), c(80, 80), col = &quot;blue&quot;, lty = 2) text( c(20, 20, 20, 50, 80, 80), c(55, 85, 130, 100, 60, 120), labels = c(&quot;R1&quot;, &quot;R2&quot;, &quot;R3&quot;, &quot;R4&quot;, &quot;R5&quot;, &quot;R6&quot;) ) 8.1.2 Question 2 It is mentioned in Section 8.2.3 that boosting using depth-one trees (or stumps) leads to an additive model: that is, a model of the form \\[ f(X) = \\sum_{j=1}^p f_j(X_j). \\] Explain why this is the case. You can begin with (8.12) in Algorithm 8.2. Equation 8.1 is: \\[ f(x) = \\sum_{b=1}^B(\\lambda \\hat{f}^b(x) \\] where \\(\\hat{f}^b(x)\\) represents the \\(b\\)th tree with (in this case) 1 split. Since 1-depth trees involve only one variable, and the total function for \\(x\\) involves adding the outcome for each, this model is an additive. Depth 2 trees would allow for interactions between two variables. 8.1.3 Question 3 Consider the Gini index, classification error, and cross-entropy in a simple classification setting with two classes. Create a single plot that displays each of these quantities as a function of \\(\\hat{p}_{m1}\\). The \\(x\\)-axis should display \\(\\hat{p}_{m1}\\), ranging from 0 to 1, and the \\(y\\)-axis should display the value of the Gini index, classification error, and entropy. Hint: In a setting with two classes, \\(\\hat{p}_{m1} = 1 - \\hat{p}_{m2}\\). You could make this plot by hand, but it will be much easier to make in R. The Gini index is defined by \\[G = \\sum_{k=1}^{K} \\hat{p}_{mk}(1 - \\hat{p}_{mk})\\] Entropy is given by \\[D = -\\sum_{k=1}^{K} \\hat{p}_{mk}\\log(\\hat{p}_{mk})\\] The classification error is \\[E = 1 - \\max_k(\\hat{p}_{mk})\\] # Function definitions are for when there&#39;s two classes only p &lt;- seq(0, 1, length.out = 100) data.frame( x = p, &quot;Gini index&quot; = p * (1 - p) * 2, &quot;Entropy&quot; = -(p * log(p) + (1 - p) * log(1 - p)), &quot;Classification error&quot; = 1 - pmax(p, 1 - p), check.names = FALSE ) |&gt; pivot_longer(!x) |&gt; ggplot(aes(x = x, y = value, color = name)) + geom_line(na.rm = TRUE) 8.1.4 Question 4 This question relates to the plots in Figure 8.12. Sketch the tree corresponding to the partition of the predictor space illustrated in the left-hand panel of Figure 8.12. The numbers inside the boxes indicate the mean of \\(Y\\) within each region. tree &lt;- ape::read.tree(text = &quot;(((3:1.5,(10:1,0:1)A:1)B:1,15:2)C:1,5:2)D;&quot;) tree$node.label &lt;- c(&quot;X1 &lt; 1&quot;, &quot;X2 &lt; 1&quot;, &quot;X1 &lt; 0&quot;, &quot;X2 &lt; 0&quot;) ggtree(tree, ladderize = FALSE) + scale_x_reverse() + coord_flip() + geom_tiplab(vjust = 2, hjust = 0.5) + geom_text2(aes(label=label, subset=!isTip), hjust = -0.1, vjust = -1) Create a diagram similar to the left-hand panel of Figure 8.12, using the tree illustrated in the right-hand panel of the same figure. You should divide up the predictor space into the correct regions, and indicate the mean for each region. plot(NULL, xlab=&quot;X1&quot;, ylab=&quot;X2&quot;, xlim = c(-1, 2), ylim = c(0, 3), xaxs = &quot;i&quot;, yaxs = &quot;i&quot;) abline(h = 1, col = &quot;red&quot;, lty = 2) lines(c(1, 1), c(0, 1), col = &quot;blue&quot;, lty = 2) lines(c(-1, 2), c(2, 2), col = &quot;red&quot;, lty = 2) lines(c(0, 0), c(1, 2), col = &quot;blue&quot;, lty = 2) text( c(0, 1.5, -0.5, 1, 0.5), c(0.5, 0.5, 1.5, 1.5, 2.5), labels = c(&quot;-1.80&quot;, &quot;0.63&quot;, &quot;-1.06&quot;, &quot;0.21&quot;, &quot;2.49&quot;) ) 8.1.5 Question 5 Suppose we produce ten bootstrapped samples from a data set containing red and green classes. We then apply a classification tree to each bootstrapped sample and, for a specific value of \\(X\\), produce 10 estimates of \\(P(\\textrm{Class is Red}|X)\\): \\[0.1, 0.15, 0.2, 0.2, 0.55, 0.6, 0.6, 0.65, 0.7, \\textrm{and } 0.75.\\] There are two common ways to combine these results together into a single class prediction. One is the majority vote approach discussed in this chapter. The second approach is to classify based on the average probability. In this example, what is the final classification under each of these two approaches? x &lt;- c(0.1, 0.15, 0.2, 0.2, 0.55, 0.6, 0.6, 0.65, 0.7, 0.75) ifelse(mean(x &gt; 0.5), &quot;red&quot;, &quot;green&quot;) # majority vote ## [1] &quot;red&quot; ifelse(mean(x) &gt; 0.5, &quot;red&quot;, &quot;green&quot;) # average probability ## [1] &quot;green&quot; 8.1.6 Question 6 Provide a detailed explanation of the algorithm that is used to fit a regression tree. First we perform binary recursive splitting of the data, to minimize RSS at each split. This is continued until there are n samples present in each leaf. Then we prune the tree to a set of subtrees determined by a parameter \\(\\alpha\\). Using K-fold CV, we select \\(\\alpha\\) to minimize the cross validation error. The final tree is then calculated using the complete dataset with the selected \\(\\alpha\\) value. 8.2 Applied 8.2.1 Question 7 In the lab, we applied random forests to the Boston data using mtry = 6 and using ntree = 25 and ntree = 500. Create a plot displaying the test error resulting from random forests on this data set for a more comprehensive range of values for mtry and ntree. You can model your plot after Figure 8.10. Describe the results obtained. library(ISLR2) library(randomForest) ## randomForest 4.7-1.1 ## Type rfNews() to see new features/changes/bug fixes. ## ## Attaching package: &#39;randomForest&#39; ## The following object is masked from &#39;package:ggtree&#39;: ## ## margin ## The following object is masked from &#39;package:dplyr&#39;: ## ## combine ## The following object is masked from &#39;package:ggplot2&#39;: ## ## margin set.seed(42) train &lt;- sample(c(TRUE, FALSE), nrow(Boston), replace = TRUE) rf_err &lt;- function(mtry) { randomForest( Boston[train, -13], y = Boston[train, 13], xtest = Boston[!train, -13], ytest = Boston[!train, 13], mtry = mtry, ntree = 500 )$test$mse } res &lt;- lapply(c(1, 2, 3, 5, 7, 10, 12), rf_err) names(res) &lt;- c(1, 2, 3, 5, 7, 10, 12) data.frame(res, check.names = FALSE) |&gt; mutate(n = 1:500) |&gt; pivot_longer(!n) |&gt; ggplot(aes(x = n, y = value, color = name)) + geom_line(na.rm = TRUE) + xlab(&quot;Number of trees&quot;) + ylab(&quot;Error&quot;) + scale_y_log10() + scale_color_discrete(name = &quot;No. variables at\\neach split&quot;) 8.2.2 Question 8 In the lab, a classification tree was applied to the Carseats data set after converting Sales into a qualitative response variable. Now we will seek to predict Sales using regression trees and related approaches, treating the response as a quantitative variable. Split the data set into a training set and a test set. set.seed(42) train &lt;- sample(c(TRUE, FALSE), nrow(Carseats), replace = TRUE) Fit a regression tree to the training set. Plot the tree, and interpret the results. What test error rate do you obtain? library(tree) tr &lt;- tree(Sales ~ ., data = Carseats[train, ]) summary(tr) ## ## Regression tree: ## tree(formula = Sales ~ ., data = Carseats[train, ]) ## Variables actually used in tree construction: ## [1] &quot;ShelveLoc&quot; &quot;Price&quot; &quot;Income&quot; &quot;Advertising&quot; &quot;CompPrice&quot; ## [6] &quot;Age&quot; ## Number of terminal nodes: 16 ## Residual mean deviance: 2.356 = 424.1 / 180 ## Distribution of residuals: ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## -4.54900 -0.82980 0.03075 0.00000 0.89250 4.83100 plot(tr) text(tr, pretty = 0, digits = 2, cex = 0.8) carseats_mse &lt;- function(model) { p &lt;- predict(model, newdata = Carseats[!train, ]) mean((p - Carseats[!train, &quot;Sales&quot;])^2) } carseats_mse(tr) ## [1] 4.559764 Use cross-validation in order to determine the optimal level of tree complexity. Does pruning the tree improve the test error rate? res &lt;- cv.tree(tr) plot(res$size, res$dev, type = &quot;b&quot;, xlab = &quot;Tree size&quot;, ylab = &quot;Deviance&quot;) min &lt;- which.min(res$dev) abline(v = res$size[min], lty = 2, col = &quot;red&quot;) Pruning improves performance very slightly (though this is not repeatable in different rounds of cross-validation). Arguably, a good balance is achieved when the tree size is 11. ptr &lt;- prune.tree(tr, best = 11) plot(ptr) text(ptr, pretty = 0, digits = 2, cex = 0.8) carseats_mse(ptr) ## [1] 4.625875 Use the bagging approach in order to analyze this data. What test error rate do you obtain? Use the importance() function to determine which variables are most important. # Here we can use random Forest with mtry = 10 = p (the number of predictor # variables) to perform bagging bagged &lt;- randomForest(Sales ~ ., data = Carseats[train, ], mtry = 10, ntree = 200, importance = TRUE) carseats_mse(bagged) ## [1] 2.762861 importance(bagged) ## %IncMSE IncNodePurity ## CompPrice 11.2608998 104.474222 ## Income 5.0953983 73.275066 ## Advertising 12.9011190 125.886762 ## Population 3.4071044 60.095200 ## Price 34.6904380 450.952728 ## ShelveLoc 33.7059874 374.808575 ## Age 7.9101141 143.652934 ## Education -2.1154997 32.712444 ## Urban 0.9604097 7.029648 ## US 3.1336559 6.287048 The test error rate is ~2.8 which is a substantial improvement over the pruned regression tree above. Use random forests to analyze this data. What test error rate do you obtain? Use the importance() function to determine which variables are most important. Describe the effect of \\(m\\), the number of variables considered at each split, on the error rate obtained. rf &lt;- randomForest(Sales ~ ., data = Carseats[train, ], mtry = 3, ntree = 500, importance = TRUE) carseats_mse(rf) ## [1] 3.439357 importance(rf) ## %IncMSE IncNodePurity ## CompPrice 8.5717587 122.75189 ## Income 2.8955756 116.33951 ## Advertising 13.0681194 128.13563 ## Population 2.0475415 104.03803 ## Price 34.7934136 342.84663 ## ShelveLoc 39.0704834 292.56638 ## Age 7.7941744 135.69061 ## Education 0.8770806 64.67614 ## Urban -0.3301478 13.83594 ## US 6.2716539 22.07306 The test error rate is ~3.0 which is a substantial improvement over the pruned regression tree above, although not quite as good as the bagging approach. Now analyze the data using BART, and report your results. library(BART) ## Loading required package: nlme ## ## Attaching package: &#39;nlme&#39; ## The following object is masked from &#39;package:ggtree&#39;: ## ## collapse ## The following object is masked from &#39;package:dplyr&#39;: ## ## collapse ## Loading required package: nnet ## Loading required package: survival # For ease, we&#39;ll create a fake &quot;predict&quot; method that just returns # yhat.test.mean regardless of provided &quot;newdata&quot; predict.wbart &lt;- function(model, ...) model$yhat.test.mean bartfit &lt;- gbart(Carseats[train, 2:11], Carseats[train, 1], x.test = Carseats[!train, 2:11]) ## *****Calling gbart: type=1 ## *****Data: ## data:n,p,np: 196, 14, 204 ## y1,yn: 2.070867, 2.280867 ## x1,x[n*p]: 138.000000, 1.000000 ## xp1,xp[np*p]: 141.000000, 1.000000 ## *****Number of Trees: 200 ## *****Number of Cut Points: 58 ... 1 ## *****burn,nd,thin: 100,1000,1 ## *****Prior:beta,alpha,tau,nu,lambda,offset: 2,0.95,0.287616,3,0.21118,7.42913 ## *****sigma: 1.041218 ## *****w (weights): 1.000000 ... 1.000000 ## *****Dirichlet:sparse,theta,omega,a,b,rho,augment: 0,0,1,0.5,1,14,0 ## *****printevery: 100 ## ## MCMC ## done 0 (out of 1100) ## done 100 (out of 1100) ## done 200 (out of 1100) ## done 300 (out of 1100) ## done 400 (out of 1100) ## done 500 (out of 1100) ## done 600 (out of 1100) ## done 700 (out of 1100) ## done 800 (out of 1100) ## done 900 (out of 1100) ## done 1000 (out of 1100) ## time: 3s ## trcnt,tecnt: 1000,1000 carseats_mse(bartfit) ## [1] 1.631285 The test error rate is ~1.6 which is an improvement over random forest and bagging. 8.2.3 Question 9 This problem involves the OJ data set which is part of the ISLR2 package. Create a training set containing a random sample of 800 observations, and a test set containing the remaining observations. set.seed(42) train &lt;- sample(1:nrow(OJ), 800) test &lt;- setdiff(1:nrow(OJ), train) Fit a tree to the training data, with Purchase as the response and the other variables except for Buy as predictors. Use the summary() function to produce summary statistics about the tree, and describe the results obtained. What is the training error rate? How many terminal nodes does the tree have? tr &lt;- tree(Purchase ~ ., data = OJ[train, ]) summary(tr) ## ## Classification tree: ## tree(formula = Purchase ~ ., data = OJ[train, ]) ## Variables actually used in tree construction: ## [1] &quot;LoyalCH&quot; &quot;SalePriceMM&quot; &quot;PriceDiff&quot; ## Number of terminal nodes: 8 ## Residual mean deviance: 0.7392 = 585.5 / 792 ## Misclassification error rate: 0.1638 = 131 / 800 Type in the name of the tree object in order to get a detailed text output. Pick one of the terminal nodes, and interpret the information displayed. tr ## node), split, n, deviance, yval, (yprob) ## * denotes terminal node ## ## 1) root 800 1066.00 CH ( 0.61500 0.38500 ) ## 2) LoyalCH &lt; 0.48285 285 296.00 MM ( 0.21404 0.78596 ) ## 4) LoyalCH &lt; 0.064156 64 0.00 MM ( 0.00000 1.00000 ) * ## 5) LoyalCH &gt; 0.064156 221 260.40 MM ( 0.27602 0.72398 ) ## 10) SalePriceMM &lt; 2.04 128 123.50 MM ( 0.18750 0.81250 ) * ## 11) SalePriceMM &gt; 2.04 93 125.00 MM ( 0.39785 0.60215 ) * ## 3) LoyalCH &gt; 0.48285 515 458.10 CH ( 0.83689 0.16311 ) ## 6) LoyalCH &lt; 0.753545 230 282.70 CH ( 0.69565 0.30435 ) ## 12) PriceDiff &lt; 0.265 149 203.00 CH ( 0.57718 0.42282 ) ## 24) PriceDiff &lt; -0.165 32 38.02 MM ( 0.28125 0.71875 ) * ## 25) PriceDiff &gt; -0.165 117 150.30 CH ( 0.65812 0.34188 ) ## 50) LoyalCH &lt; 0.703993 105 139.60 CH ( 0.61905 0.38095 ) * ## 51) LoyalCH &gt; 0.703993 12 0.00 CH ( 1.00000 0.00000 ) * ## 13) PriceDiff &gt; 0.265 81 47.66 CH ( 0.91358 0.08642 ) * ## 7) LoyalCH &gt; 0.753545 285 111.70 CH ( 0.95088 0.04912 ) * Create a plot of the tree, and interpret the results. plot(tr) text(tr, pretty = 0, digits = 2, cex = 0.8) Predict the response on the test data, and produce a confusion matrix comparing the test labels to the predicted test labels. What is the test error rate? table(predict(tr, OJ[test, ], type = &quot;class&quot;), OJ[test, &quot;Purchase&quot;]) ## ## CH MM ## CH 125 15 ## MM 36 94 Apply the cv.tree() function to the training set in order to determine the optimal tree size. set.seed(42) res &lt;- cv.tree(tr) Produce a plot with tree size on the \\(x\\)-axis and cross-validated classification error rate on the \\(y\\)-axis. plot(res$size, res$dev, type = &quot;b&quot;, xlab = &quot;Tree size&quot;, ylab = &quot;Deviance&quot;) min &lt;- which.min(res$dev) abline(v = res$size[min], lty = 2, col = &quot;red&quot;) Which tree size corresponds to the lowest cross-validated classification error rate? res$size[min] ## [1] 6 Produce a pruned tree corresponding to the optimal tree size obtained using cross-validation. If cross-validation does not lead to selection of a pruned tree, then create a pruned tree with five terminal nodes. ptr &lt;- prune.tree(tr, best = res$size[min]) plot(ptr) text(ptr, pretty = 0, digits = 2, cex = 0.8) Compare the training error rates between the pruned and unpruned trees. Which is higher? oj_misclass &lt;- function(model) { summary(model)$misclass[1] / summary(model)$misclass[2] } oj_misclass(tr) ## [1] 0.16375 oj_misclass(ptr) ## [1] 0.16375 The training misclassification error rate is slightly higher for the pruned tree. Compare the test error rates between the pruned and unpruned trees. Which is higher? oj_err &lt;- function(model) { p &lt;- predict(model, newdata = OJ[test, ], type = &quot;class&quot;) mean(p != OJ[test, &quot;Purchase&quot;]) } oj_err(tr) ## [1] 0.1888889 oj_err(ptr) ## [1] 0.1888889 The test misclassification error rate is slightly higher for the pruned tree. 8.2.4 Question 10 We now use boosting to predict Salary in the Hitters data set. Remove the observations for whom the salary information is unknown, and then log-transform the salaries. dat &lt;- Hitters dat &lt;- dat[!is.na(dat$Salary), ] dat$Salary &lt;- log(dat$Salary) Create a training set consisting of the first 200 observations, and a test set consisting of the remaining observations. train &lt;- 1:200 test &lt;- setdiff(1:nrow(dat), train) Perform boosting on the training set with 1,000 trees for a range of values of the shrinkage parameter \\(\\lambda\\). Produce a plot with different shrinkage values on the \\(x\\)-axis and the corresponding training set MSE on the \\(y\\)-axis. library(gbm) ## Loaded gbm 2.1.8.1 set.seed(42) lambdas &lt;- 10 ^ seq(-3, 0, by = 0.1) fits &lt;- lapply(lambdas, function(lam) { gbm(Salary ~ ., data = dat[train, ], distribution = &quot;gaussian&quot;, n.trees = 1000, shrinkage = lam) }) errs &lt;- sapply(fits, function(fit) { p &lt;- predict(fit, dat[train, ], n.trees = 1000) mean((p - dat[train, ]$Salary)^2) }) plot(lambdas, errs, type = &quot;b&quot;, xlab = &quot;Shrinkage values&quot;, ylab = &quot;Training MSE&quot;, log = &quot;xy&quot;) Produce a plot with different shrinkage values on the \\(x\\)-axis and the corresponding test set MSE on the \\(y\\)-axis. errs &lt;- sapply(fits, function(fit) { p &lt;- predict(fit, dat[test, ], n.trees = 1000) mean((p - dat[test, ]$Salary)^2) }) plot(lambdas, errs, type = &quot;b&quot;, xlab = &quot;Shrinkage values&quot;, ylab = &quot;Training MSE&quot;, log = &quot;xy&quot;) min(errs) ## [1] 0.249881 abline(v = lambdas[which.min(errs)], lty = 2, col = &quot;red&quot;) Compare the test MSE of boosting to the test MSE that results from applying two of the regression approaches seen in Chapters 3 and 6. Linear regression fit1 &lt;- lm(Salary ~ ., data = dat[train, ]) mean((predict(fit1, dat[test, ]) - dat[test, &quot;Salary&quot;])^2) ## [1] 0.4917959 Ridge regression library(glmnet) ## Loading required package: Matrix ## ## Attaching package: &#39;Matrix&#39; ## The following object is masked from &#39;package:ggtree&#39;: ## ## expand ## The following objects are masked from &#39;package:tidyr&#39;: ## ## expand, pack, unpack ## Loaded glmnet 4.1-4 x &lt;- model.matrix(Salary ~ ., data = dat[train, ]) x.test &lt;- model.matrix(Salary ~ ., data = dat[test, ]) y &lt;- dat[train, &quot;Salary&quot;] fit2 &lt;- glmnet(x, y, alpha = 1) mean((predict(fit2, s = 0.1, newx = x.test) - dat[test, &quot;Salary&quot;])^2) ## [1] 0.4389054 Which variables appear to be the most important predictors in the boosted model? summary(fits[[which.min(errs)]]) ## var rel.inf ## CAtBat CAtBat 16.4755242 ## CRBI CRBI 9.0670759 ## CHits CHits 8.9307168 ## CRuns CRuns 7.6839786 ## CWalks CWalks 7.1014886 ## PutOuts PutOuts 6.7869382 ## AtBat AtBat 5.8567916 ## Walks Walks 5.8479836 ## Years Years 5.3349489 ## Assists Assists 5.0076392 ## CHmRun CHmRun 4.6606919 ## RBI RBI 3.9255396 ## Hits Hits 3.8123124 ## HmRun HmRun 3.4462640 ## Runs Runs 2.4779866 ## Errors Errors 2.2341326 ## NewLeague NewLeague 0.5788283 ## Division Division 0.4880237 ## League League 0.2831352 Now apply bagging to the training set. What is the test set MSE for this approach? set.seed(42) bagged &lt;- randomForest(Salary ~ ., data = dat[train, ], mtry = 19, ntree = 1000) mean((predict(bagged, newdata = dat[test, ]) - dat[test, &quot;Salary&quot;])^2) ## [1] 0.2278813 8.2.5 Question 11 This question uses the Caravan data set. Create a training set consisting of the first 1,000 observations, and a test set consisting of the remaining observations. train &lt;- 1:1000 test &lt;- setdiff(1:nrow(Caravan), train) Fit a boosting model to the training set with Purchase as the response and the other variables as predictors. Use 1,000 trees, and a shrinkage value of 0.01. Which predictors appear to be the most important? set.seed(42) fit &lt;- gbm(Purchase == &quot;Yes&quot; ~ ., data = Caravan[train, ], n.trees = 1000, shrinkage = 0.01) ## Distribution not specified, assuming bernoulli ... ## Warning in gbm.fit(x = x, y = y, offset = offset, distribution = distribution, : ## variable 50: PVRAAUT has no variation. ## Warning in gbm.fit(x = x, y = y, offset = offset, distribution = distribution, : ## variable 71: AVRAAUT has no variation. head(summary(fit)) ## var rel.inf ## PPERSAUT PPERSAUT 15.243041 ## MKOOPKLA MKOOPKLA 10.220498 ## MOPLHOOG MOPLHOOG 7.584734 ## MBERMIDD MBERMIDD 5.983650 ## PBRAND PBRAND 4.557491 ## ABRAND ABRAND 4.076017 Use the boosting model to predict the response on the test data. Predict that a person will make a purchase if the estimated probability of purchase is greater than 20%. Form a confusion matrix. What fraction of the people predicted to make a purchase do in fact make one? How does this compare with the results obtained from applying KNN or logistic regression to this data set? p &lt;- predict(fit, Caravan[test, ], n.trees = 1000, type = &quot;response&quot;) table(p &gt; 0.2, Caravan[test, &quot;Purchase&quot;] == &quot;Yes&quot;) ## ## FALSE TRUE ## FALSE 4415 257 ## TRUE 118 32 sum(p &gt; 0.2 &amp; Caravan[test, &quot;Purchase&quot;] == &quot;Yes&quot;) / sum(p &gt; 0.2) ## [1] 0.2133333 141 (109 + 32) are predicted to purchase. Of these 32 do which is 21%. # Logistic regression fit &lt;- glm(Purchase == &quot;Yes&quot; ~ ., data = Caravan[train, ], family = &quot;binomial&quot;) ## Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred p &lt;- predict(fit, Caravan[test, ], type = &quot;response&quot;) ## Warning in predict.lm(object, newdata, se.fit, scale = 1, type = if (type == : ## prediction from a rank-deficient fit may be misleading table(p &gt; 0.2, Caravan[test, &quot;Purchase&quot;] == &quot;Yes&quot;) ## ## FALSE TRUE ## FALSE 4183 231 ## TRUE 350 58 sum(p &gt; 0.2 &amp; Caravan[test, &quot;Purchase&quot;] == &quot;Yes&quot;) / sum(p &gt; 0.2) ## [1] 0.1421569 For logistic regression we correctly predict 14% of those predicted to purchase. library(class) # KNN fit &lt;- knn(Caravan[train, -86], Caravan[test, -86], Caravan$Purchase[train]) table(fit, Caravan[test, &quot;Purchase&quot;] == &quot;Yes&quot;) ## ## fit FALSE TRUE ## No 4260 263 ## Yes 273 26 sum(fit == &quot;Yes&quot; &amp; Caravan[test, &quot;Purchase&quot;] == &quot;Yes&quot;) / sum(fit == &quot;Yes&quot;) ## [1] 0.08695652 For KNN we correctly predict 8.7% of those predicted to purchase. 8.2.6 Question 12 Apply boosting, bagging, random forests and BART to a data set of your choice. Be sure to fit the models on a training set and to evaluate their performance on a test set. How accurate are the results compared to simple methods like linear or logistic regression? Which of these approaches yields the best performance? Here I’m going to use the College dataset (used in Question 10 from Chapter 7 to compare performance with the GAM we previously built). In this model we were trying to predict Outstate using the other variables in College. library(gam) ## Loading required package: splines ## Loading required package: foreach ## ## Attaching package: &#39;foreach&#39; ## The following objects are masked from &#39;package:purrr&#39;: ## ## accumulate, when ## Loaded gam 1.20.2 set.seed(42) train &lt;- sample(1:nrow(College), 400) test &lt;- setdiff(1:nrow(College), train) # Linear regression lr &lt;- gam(Outstate ~ ., data = College[train, ]) # GAM from chapter 7 gam &lt;- gam(Outstate ~ Private + s(Room.Board, 2) + s(PhD, 2) + s(perc.alumni, 2) + s(Expend, 2) + s(Grad.Rate, 2), data = College[train, ]) # Boosting boosted &lt;- gbm(Outstate ~ ., data = College[train, ], n.trees = 1000, shrinkage = 0.01) ## Distribution not specified, assuming gaussian ... # Bagging (random forest with mtry = no. predictors) bagged &lt;- randomForest(Outstate ~ ., data = College[train, ], mtry = 17, ntree = 1000) # Random forest with mtry = sqrt(no. predictors) rf &lt;- randomForest(Outstate ~ ., data = College[train, ], mtry = 4, ntree = 1000) # BART pred &lt;- setdiff(colnames(College), &quot;Outstate&quot;) bart &lt;- gbart(College[train, pred], College[train, &quot;Outstate&quot;], x.test = College[test, pred]) ## *****Calling gbart: type=1 ## *****Data: ## data:n,p,np: 400, 18, 377 ## y1,yn: -4030.802500, 77.197500 ## x1,x[n*p]: 1.000000, 71.000000 ## xp1,xp[np*p]: 0.000000, 99.000000 ## *****Number of Trees: 200 ## *****Number of Cut Points: 1 ... 75 ## *****burn,nd,thin: 100,1000,1 ## *****Prior:beta,alpha,tau,nu,lambda,offset: 2,0.95,301.581,3,715815,10580.8 ## *****sigma: 1916.969943 ## *****w (weights): 1.000000 ... 1.000000 ## *****Dirichlet:sparse,theta,omega,a,b,rho,augment: 0,0,1,0.5,1,18,0 ## *****printevery: 100 ## ## MCMC ## done 0 (out of 1100) ## done 100 (out of 1100) ## done 200 (out of 1100) ## done 300 (out of 1100) ## done 400 (out of 1100) ## done 500 (out of 1100) ## done 600 (out of 1100) ## done 700 (out of 1100) ## done 800 (out of 1100) ## done 900 (out of 1100) ## done 1000 (out of 1100) ## time: 5s ## trcnt,tecnt: 1000,1000 mse &lt;- function(model, ...) { pred &lt;- predict(model, College[test, ], ...) mean((College$Outstate[test] - pred)^2) } res &lt;- c( &quot;Linear regression&quot; = mse(lr), &quot;GAM&quot; = mse(gam), &quot;Boosting&quot; = mse(boosted, n.trees = 1000), &quot;Bagging&quot; = mse(bagged), &quot;Random forest&quot; = mse(rf), &quot;BART&quot; = mse(bart) ) res &lt;- data.frame(&quot;MSE&quot; = res) res$Model &lt;- factor(row.names(res), levels = rev(row.names(res))) ggplot(res, aes(Model, MSE)) + coord_flip() + geom_bar(stat = &quot;identity&quot;, fill = &quot;steelblue&quot;) In this case, it looks like bagging produces the best performing model in terms of test mean square error. "],["support-vector-machines.html", "9 Support Vector Machines 9.1 Conceptual 9.2 Applied", " 9 Support Vector Machines 9.1 Conceptual 9.1.1 Question 1 This problem involves hyperplanes in two dimensions. Sketch the hyperplane \\(1 + 3X_1 − X_2 = 0\\). Indicate the set of points for which \\(1 + 3X_1 − X_2 &gt; 0\\), as well as the set of points for which \\(1 + 3X_1 − X_2 &lt; 0\\). library(ggplot2) xlim &lt;- c(-10, 10) ylim &lt;- c(-30, 30) points &lt;- expand.grid( X1 = seq(xlim[1], xlim[2], length.out = 50), X2 = seq(ylim[1], ylim[2], length.out = 50) ) p &lt;- ggplot(points, aes(x = X1, y = X2)) + geom_abline(intercept = 1, slope = 3) + # X2 = 1 + 3X1 theme_bw() p + geom_point(aes(color = 1 + 3*X1 - X2 &gt; 0), size = 0.1) + scale_color_discrete(name = &quot;1 + 3X1 − X2 &gt; 0&quot;) On the same plot, sketch the hyperplane \\(−2 + X_1 + 2X_2 = 0\\). Indicate the set of points for which \\(−2 + X_1 + 2X_2 &gt; 0\\), as well as the set of points for which \\(−2 + X_1 + 2X_2 &lt; 0\\). p + geom_abline(intercept = 1, slope = -1/2) + # X2 = 1 - X1/2 geom_point( aes(color = interaction(1 + 3*X1 - X2 &gt; 0, -2 + X1 + 2*X2 &gt; 0)), size = 0.5 ) + scale_color_discrete(name = &quot;(1 + 3X1 − X2 &gt; 0).(−2 + X1 + 2X2 &gt; 0)&quot;) 9.1.2 Question 2 We have seen that in \\(p = 2\\) dimensions, a linear decision boundary takes the form \\(\\beta_0 + \\beta_1X_1 + \\beta_2X_2 = 0\\). We now investigate a non-linear decision boundary. Sketch the curve \\[(1+X_1)^2 +(2−X_2)^2 = 4\\]. points &lt;- expand.grid( X1 = seq(-4, 2, length.out = 100), X2 = seq(-1, 5, length.out = 100) ) p &lt;- ggplot(points, aes(x = X1, y = X2, z = (1 + X1)^2 + (2 - X2)^2 - 4)) + geom_contour(breaks = 0, colour = &quot;black&quot;) + theme_bw() p On your sketch, indicate the set of points for which \\[(1 + X_1)^2 + (2 − X_2)^2 &gt; 4,\\] as well as the set of points for which \\[(1 + X_1)^2 + (2 − X_2)^2 \\leq 4.\\] p + geom_point(aes(color = (1 + X1)^2 + (2 - X2)^2 - 4 &gt; 0), size = 0.1) Suppose that a classifier assigns an observation to the blue class if \\[(1 + X_1)^2 + (2 − X_2)^2 &gt; 4,\\] and to the red class otherwise. To what class is the observation \\((0, 0)\\) classified? \\((−1, 1)\\)? \\((2, 2)\\)? \\((3, 8)\\)? points &lt;- data.frame( X1 = c(0, -1, 2, 3), X2 = c(0, 1, 2, 8) ) ifelse((1 + points$X1)^2 + (2 - points$X2)^2 &gt; 4, &quot;blue&quot;, &quot;red&quot;) ## [1] &quot;blue&quot; &quot;red&quot; &quot;blue&quot; &quot;blue&quot; Argue that while the decision boundary in (c) is not linear in terms of \\(X_1\\) and \\(X_2\\), it is linear in terms of \\(X_1\\), \\(X_1^2\\), \\(X_2\\), and \\(X_2^2\\). The decision boundary is \\[(1 + X_1)^2 + (2 − X_2)^2 -4 = 0\\] which we can expand to: \\[1 + 2X_1 + X_1^2 + 4 − 4X_2 + X_2^2 - 4 = 0\\] which is linear in terms of \\(X_1\\), \\(X_1^2\\), \\(X_2\\), \\(X_2^2\\). 9.1.3 Question 3 Here we explore the maximal margin classifier on a toy data set. We are given \\(n = 7\\) observations in \\(p = 2\\) dimensions. For each observation, there is an associated class label. Obs. \\(X_1\\) \\(X_2\\) \\(Y\\) 1 3 4 Red 2 2 2 Red 3 4 4 Red 4 1 4 Red 5 2 1 Blue 6 4 3 Blue 7 4 1 Blue Sketch the observations. data &lt;- data.frame( X1 = c(3, 2, 4, 1, 2, 4, 4), X2 = c(4, 2, 4, 4, 1, 3, 1), Y = c(rep(&quot;Red&quot;, 4), rep(&quot;Blue&quot;, 3)) ) p &lt;- ggplot(data, aes(x = X1, y = X2, color = Y)) + geom_point(size = 2) + scale_colour_identity() + coord_cartesian(xlim = c(0.5, 4.5), ylim = c(0.5, 4.5)) p Sketch the optimal separating hyperplane, and provide the equation for this hyperplane (of the form (9.1)). library(e1071) fit &lt;- svm(as.factor(Y) ~ ., data = data, kernel = &quot;linear&quot;, cost = 10, scale = FALSE) # Extract beta_0, beta_1, beta_2 beta &lt;- c( -fit$rho, drop(t(fit$coefs) %*% as.matrix(data[fit$index, 1:2])) ) names(beta) &lt;- c(&quot;B0&quot;, &quot;B1&quot;, &quot;B2&quot;) p &lt;- p + geom_abline(intercept = -beta[1] / beta[3], slope = -beta[2] / beta[3], lty = 2) p Describe the classification rule for the maximal margin classifier. It should be something along the lines of “Classify to Red if \\(\\beta_0 + \\beta_1X_1 + \\beta_2X_2 &gt; 0\\), and classify to Blue otherwise.” Provide the values for \\(\\beta_0, \\beta_1,\\) and \\(\\beta_2\\). Classify to red if \\(\\beta_0 + \\beta_1X_1 + \\beta_2X_2 &gt; 0\\) and blue otherwise where \\(\\beta_0 = 1\\), \\(\\beta_1 = -2\\), \\(\\beta_2 = 2\\). On your sketch, indicate the margin for the maximal margin hyperplane. p &lt;- p + geom_ribbon( aes(x = x, ymin = ymin, ymax = ymax), data = data.frame(x = c(0, 5), ymin = c(-1, 4), ymax = c(0, 5)), alpha = 0.1, fill = &quot;blue&quot;, inherit.aes = FALSE ) p Indicate the support vectors for the maximal margin classifier. p &lt;- p + geom_point(data = data[fit$index, ], size = 4) p The support vectors (from the svm fit object) are shown above. Arguably, there’s another support vector, since four points exactly touch the margin. Argue that a slight movement of the seventh observation would not affect the maximal margin hyperplane. p + geom_point(data = data[7, , drop = FALSE], size = 4, color = &quot;purple&quot;) The 7th point is shown in purple above. It is not a support vector, and not close to the margin, so small changes in its X1, X2 values would not affect the current calculated margin. Sketch a hyperplane that is not the optimal separating hyperplane, and provide the equation for this hyperplane. A non-optimal hyperline that still separates the blue and red points would be one that touches the (red) point at X1 = 2, X2 = 2 and the (blue) point at X1 = 4, X2 = 3. This gives line \\(y = x/2 + 1\\) or, when \\(\\beta_0 = -1\\), \\(\\beta_1 = -1/2\\), \\(\\beta_2 = 1\\). p + geom_abline(intercept = 1, slope = 0.5, lty = 2, col = &quot;red&quot;) Draw an additional observation on the plot so that the two classes are no longer separable by a hyperplane. p + geom_point(data = data.frame(X1 = 1, X2 = 3, Y = &quot;Blue&quot;), shape = 15, size = 4) 9.2 Applied 9.2.1 Question 4 Generate a simulated two-class data set with 100 observations and two features in which there is a visible but non-linear separation between the two classes. Show that in this setting, a support vector machine with a polynomial kernel (with degree greater than 1) or a radial kernel will outperform a support vector classifier on the training data. Which technique performs best on the test data? Make plots and report training and test error rates in order to back up your assertions. set.seed(10) data &lt;- data.frame( x = runif(100), y = runif(100) ) score &lt;- (2*data$x-0.5)^2 + (data$y)^2 - 0.5 data$class &lt;- factor(ifelse(score &gt; 0, &quot;red&quot;, &quot;blue&quot;)) p &lt;- ggplot(data, aes(x = x, y = y, color = class)) + geom_point(size = 2) + scale_colour_identity() p train &lt;- 1:50 test &lt;- 51:100 fits &lt;- list( &quot;Radial&quot; = svm(class ~ ., data = data[train, ], kernel = &quot;radial&quot;), &quot;Polynomial&quot; = svm(class ~ ., data = data[train, ], kernel = &quot;polynomial&quot;, degree = 2), &quot;Linear&quot; = svm(class ~ ., data = data[train, ], kernel = &quot;linear&quot;) ) err &lt;- function(model, data) { out &lt;- table(predict(model, data), data$class) (out[1, 2] + out[2, 1]) / sum(out) } plot(fits[[1]], data) plot(fits[[2]], data) plot(fits[[3]], data) sapply(fits, err, data = data[train, ]) ## Radial Polynomial Linear ## 0.04 0.30 0.10 sapply(fits, err, data = data[test, ]) ## Radial Polynomial Linear ## 0.06 0.48 0.14 In this case, the radial kernel performs best, followed by a linear kernel with the 2nd degree polynomial performing worst. The ordering of these models is the same for the training and test data sets. 9.2.2 Question 5 We have seen that we can fit an SVM with a non-linear kernel in order to perform classification using a non-linear decision boundary. We will now see that we can also obtain a non-linear decision boundary by performing logistic regression using non-linear transformations of the features. Generate a data set with \\(n = 500\\) and \\(p = 2\\), such that the observations belong to two classes with a quadratic decision boundary between them. For instance, you can do this as follows: &gt; x1 &lt;- runif(500) - 0.5 &gt; x2 &lt;- runif(500) - 0.5 &gt; y &lt;- 1 * (x1^2 - x2^2 &gt; 0) set.seed(42) train &lt;- data.frame( x1 = runif(500) - 0.5, x2 = runif(500) - 0.5 ) train$y &lt;- factor(as.numeric((train$x1^2 - train$x2^2 &gt; 0))) Plot the observations, colored according to their class labels. Your plot should display \\(X_1\\) on the \\(x\\)-axis, and \\(X_2\\) on the \\(y\\)-axis. p &lt;- ggplot(train, aes(x = x1, y = x2, color = y)) + geom_point(size = 2) p Fit a logistic regression model to the data, using \\(X_1\\) and \\(X_2\\) as predictors. fit1 &lt;- glm(y ~ ., data = train, family = &quot;binomial&quot;) Apply this model to the training data in order to obtain a predicted class label for each training observation. Plot the observations, colored according to the predicted class labels. The decision boundary should be linear. plot_model &lt;- function(fit) { if (inherits(fit, &quot;svm&quot;)) { train$p &lt;- predict(fit) } else { train$p &lt;- factor(as.numeric(predict(fit) &gt; 0)) } ggplot(train, aes(x = x1, y = x2, color = p)) + geom_point(size = 2) } plot_model(fit1) Now fit a logistic regression model to the data using non-linear functions of \\(X_1\\) and \\(X_2\\) as predictors (e.g. \\(X_1^2, X_1 \\times X_2, \\log(X_2),\\) and so forth). fit2 &lt;- glm(y ~ poly(x1, 2) + poly(x2, 2), data = train, family = &quot;binomial&quot;) ## Warning: glm.fit: algorithm did not converge ## Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred Apply this model to the training data in order to obtain a predicted class label for each training observation. Plot the observations, colored according to the predicted class labels. The decision boundary should be obviously non-linear. If it is not, then repeat (a)-(e) until you come up with an example in which the predicted class labels are obviously non-linear. plot_model(fit2) Fit a support vector classifier to the data with \\(X_1\\) and \\(X_2\\) as predictors. Obtain a class prediction for each training observation. Plot the observations, colored according to the predicted class labels. fit3 &lt;- svm(y ~ x1 + x2, data = train, kernel = &quot;linear&quot;) plot_model(fit3) Fit a SVM using a non-linear kernel to the data. Obtain a class prediction for each training observation. Plot the observations, colored according to the predicted class labels. fit4 &lt;- svm(y ~ x1 + x2, data = train, kernel = &quot;polynomial&quot;, degree = 2) plot_model(fit4) Comment on your results. When simulating data with a quadratic decision boundary, a logistic model with quadratic transformations of the variables and an svm model with a quadratic kernel both produce much better (and similar fits) than standard linear methods. 9.2.3 Question 6 At the end of Section 9.6.1, it is claimed that in the case of data that is just barely linearly separable, a support vector classifier with a small value of cost that misclassifies a couple of training observations may perform better on test data than one with a huge value of cost that does not misclassify any training observations. You will now investigate this claim. Generate two-class data with \\(p = 2\\) in such a way that the classes are just barely linearly separable. set.seed(2) # Simulate data that is separable by a line at y = 2.5 data &lt;- data.frame( x = rnorm(200), class = sample(c(&quot;red&quot;, &quot;blue&quot;), 200, replace = TRUE) ) data$y &lt;- (data$class == &quot;red&quot;) * 5 + rnorm(200) # Add barley separable points (these are simulated &quot;noise&quot; values) newdata &lt;- data.frame(x = rnorm(30)) newdata$y &lt;- 1.5*newdata$x + 3 + rnorm(30, 0, 1) newdata$class = ifelse((1.5*newdata$x + 3) - newdata$y &gt; 0, &quot;blue&quot;, &quot;red&quot;) data &lt;- rbind(data, newdata) # remove any that cause misclassification leaving data that is barley linearly # separable, but along an axis that is not y = 2.5 (which would be correct # for the &quot;true&quot; data. data &lt;- data[!(data$class == &quot;red&quot;) == ((1.5*data$x + 3 - data$y) &gt; 0), ] data &lt;- data[sample(seq_len(nrow(data)), 200), ] p &lt;- ggplot(data, aes(x = x, y = y, color = class)) + geom_point(size = 2) + scale_colour_identity() + geom_abline(intercept = 3, slope = 1.5, lty = 2) p Compute the cross-validation error rates for support vector classifiers with a range of cost values. How many training errors are misclassified for each value of cost considered, and how does this relate to the cross-validation errors obtained? How many training errors are misclassified for each value of cost? costs &lt;- 10^seq(-3, 5) sapply(costs, function(cost) { fit &lt;- svm(as.factor(class) ~ ., data = data, kernel = &quot;linear&quot;, cost = cost) pred &lt;- predict(fit, data) sum(pred != data$class) }) ## [1] 98 8 9 4 1 1 0 0 0 Cross-validation errors out &lt;- tune(svm, as.factor(class) ~ ., data = data, kernel = &quot;linear&quot;, ranges = list(cost = costs)) summary(out) ## ## Parameter tuning of &#39;svm&#39;: ## ## - sampling method: 10-fold cross validation ## ## - best parameters: ## cost ## 10 ## ## - best performance: 0.005 ## ## - Detailed performance results: ## cost error dispersion ## 1 1e-03 0.540 0.09067647 ## 2 1e-02 0.045 0.02838231 ## 3 1e-01 0.045 0.03689324 ## 4 1e+00 0.020 0.02581989 ## 5 1e+01 0.005 0.01581139 ## 6 1e+02 0.005 0.01581139 ## 7 1e+03 0.005 0.01581139 ## 8 1e+04 0.010 0.02108185 ## 9 1e+05 0.010 0.02108185 data.frame( cost = out$performances$cost, misclass = out$performances$error * nrow(data) ) ## cost misclass ## 1 1e-03 108 ## 2 1e-02 9 ## 3 1e-01 9 ## 4 1e+00 4 ## 5 1e+01 1 ## 6 1e+02 1 ## 7 1e+03 1 ## 8 1e+04 2 ## 9 1e+05 2 Generate an appropriate test data set, and compute the test errors corresponding to each of the values of cost considered. Which value of cost leads to the fewest test errors, and how does this compare to the values of cost that yield the fewest training errors and the fewest cross-validation errors? set.seed(2) test &lt;- data.frame( x = rnorm(200), class = sample(c(&quot;red&quot;, &quot;blue&quot;), 200, replace = TRUE) ) test$y &lt;- (test$class == &quot;red&quot;) * 5 + rnorm(200) p + geom_point(data = test, pch = 21) (errs &lt;- sapply(costs, function(cost) { fit &lt;- svm(as.factor(class) ~ ., data = data, kernel = &quot;linear&quot;, cost = cost) pred &lt;- predict(fit, test) sum(pred != test$class) })) ## [1] 95 2 3 9 16 16 19 19 19 (cost &lt;- costs[which.min(errs)]) ## [1] 0.01 (fit &lt;- svm(as.factor(class) ~ ., data = data, kernel = &quot;linear&quot;, cost = cost)) ## ## Call: ## svm(formula = as.factor(class) ~ ., data = data, kernel = &quot;linear&quot;, ## cost = cost) ## ## ## Parameters: ## SVM-Type: C-classification ## SVM-Kernel: linear ## cost: 0.01 ## ## Number of Support Vectors: 135 test$prediction &lt;- predict(fit, test) p &lt;- ggplot(test, aes(x = x, y = y, color = class, shape = prediction == class)) + geom_point(size = 2) + scale_colour_identity() p Discuss your results. A large cost leads to overfitting as the model finds the perfect linear separation between red and blue in the training data. A lower cost then leads to improved prediction in the test data. 9.2.4 Question 7 In this problem, you will use support vector approaches in order to predict whether a given car gets high or low gas mileage based on the Auto data set. Create a binary variable that takes on a 1 for cars with gas mileage above the median, and a 0 for cars with gas mileage below the median. library(ISLR2) data &lt;- Auto data$high_mpg &lt;- as.factor(as.numeric(data$mpg &gt; median(data$mpg))) Fit a support vector classifier to the data with various values of cost, in order to predict whether a car gets high or low gas mileage. Report the cross-validation errors associated with different values of this parameter. Comment on your results. Note you will need to fit the classifier without the gas mileage variable to produce sensible results. set.seed(42) costs &lt;- 10^seq(-4, 3, by = 0.5) results &lt;- list() f &lt;- high_mpg ~ displacement + horsepower + weight results$linear &lt;- tune(svm, f, data = data, kernel = &quot;linear&quot;, ranges = list(cost = costs)) summary(results$linear) ## ## Parameter tuning of &#39;svm&#39;: ## ## - sampling method: 10-fold cross validation ## ## - best parameters: ## cost ## 0.03162278 ## ## - best performance: 0.1019231 ## ## - Detailed performance results: ## cost error dispersion ## 1 1.000000e-04 0.5967949 0.05312225 ## 2 3.162278e-04 0.5967949 0.05312225 ## 3 1.000000e-03 0.2199359 0.08718077 ## 4 3.162278e-03 0.1353846 0.06058195 ## 5 1.000000e-02 0.1121795 0.04011293 ## 6 3.162278e-02 0.1019231 0.05087176 ## 7 1.000000e-01 0.1096154 0.05246238 ## 8 3.162278e-01 0.1044872 0.05154934 ## 9 1.000000e+00 0.1044872 0.05154934 ## 10 3.162278e+00 0.1044872 0.05154934 ## 11 1.000000e+01 0.1019231 0.05501131 ## 12 3.162278e+01 0.1019231 0.05501131 ## 13 1.000000e+02 0.1019231 0.05501131 ## 14 3.162278e+02 0.1019231 0.05501131 ## 15 1.000000e+03 0.1019231 0.05501131 Now repeat (b), this time using SVMs with radial and polynomial basis kernels, with different values of gamma and degree and cost. Comment on your results. results$polynomial &lt;- tune(svm, f, data = data, kernel = &quot;polynomial&quot;, ranges = list(cost = costs, degree = 1:3)) summary(results$polynomial) ## ## Parameter tuning of &#39;svm&#39;: ## ## - sampling method: 10-fold cross validation ## ## - best parameters: ## cost degree ## 0.1 1 ## ## - best performance: 0.101859 ## ## - Detailed performance results: ## cost degree error dispersion ## 1 1.000000e-04 1 0.5842949 0.04703306 ## 2 3.162278e-04 1 0.5842949 0.04703306 ## 3 1.000000e-03 1 0.5842949 0.04703306 ## 4 3.162278e-03 1 0.2167949 0.07891173 ## 5 1.000000e-02 1 0.1275641 0.04806885 ## 6 3.162278e-02 1 0.1147436 0.05661708 ## 7 1.000000e-01 1 0.1018590 0.05732429 ## 8 3.162278e-01 1 0.1069231 0.05949679 ## 9 1.000000e+00 1 0.1069231 0.06307278 ## 10 3.162278e+00 1 0.1069231 0.06307278 ## 11 1.000000e+01 1 0.1043590 0.06603760 ## 12 3.162278e+01 1 0.1043590 0.06603760 ## 13 1.000000e+02 1 0.1043590 0.06603760 ## 14 3.162278e+02 1 0.1043590 0.06603760 ## 15 1.000000e+03 1 0.1043590 0.06603760 ## 16 1.000000e-04 2 0.5842949 0.04703306 ## 17 3.162278e-04 2 0.5842949 0.04703306 ## 18 1.000000e-03 2 0.5842949 0.04703306 ## 19 3.162278e-03 2 0.5255128 0.08090636 ## 20 1.000000e-02 2 0.3980769 0.08172400 ## 21 3.162278e-02 2 0.3674359 0.07974741 ## 22 1.000000e-01 2 0.3597436 0.08336609 ## 23 3.162278e-01 2 0.3597436 0.09010398 ## 24 1.000000e+00 2 0.3444872 0.08767258 ## 25 3.162278e+00 2 0.3545513 0.10865903 ## 26 1.000000e+01 2 0.3239103 0.09593710 ## 27 3.162278e+01 2 0.3035256 0.08184137 ## 28 1.000000e+02 2 0.3061538 0.08953945 ## 29 3.162278e+02 2 0.3060897 0.08919821 ## 30 1.000000e+03 2 0.3035897 0.09305216 ## 31 1.000000e-04 3 0.5842949 0.04703306 ## 32 3.162278e-04 3 0.4955128 0.10081350 ## 33 1.000000e-03 3 0.3750641 0.08043982 ## 34 3.162278e-03 3 0.3036538 0.09096445 ## 35 1.000000e-02 3 0.2601282 0.07774595 ## 36 3.162278e-02 3 0.2499359 0.08407106 ## 37 1.000000e-01 3 0.2017949 0.07547413 ## 38 3.162278e-01 3 0.1937179 0.08427411 ## 39 1.000000e+00 3 0.1478205 0.04579654 ## 40 3.162278e+00 3 0.1451923 0.05169638 ## 41 1.000000e+01 3 0.1451282 0.04698931 ## 42 3.162278e+01 3 0.1500000 0.07549058 ## 43 1.000000e+02 3 0.1373718 0.05772558 ## 44 3.162278e+02 3 0.1271795 0.06484766 ## 45 1.000000e+03 3 0.1322436 0.06764841 results$radial &lt;- tune(svm, f, data = data, kernel = &quot;radial&quot;, ranges = list(cost = costs, gamma = 10^(-2:1))) summary(results$radial) ## ## Parameter tuning of &#39;svm&#39;: ## ## - sampling method: 10-fold cross validation ## ## - best parameters: ## cost gamma ## 1000 0.1 ## ## - best performance: 0.08179487 ## ## - Detailed performance results: ## cost gamma error dispersion ## 1 1.000000e-04 0.01 0.58410256 0.05435320 ## 2 3.162278e-04 0.01 0.58410256 0.05435320 ## 3 1.000000e-03 0.01 0.58410256 0.05435320 ## 4 3.162278e-03 0.01 0.58410256 0.05435320 ## 5 1.000000e-02 0.01 0.58410256 0.05435320 ## 6 3.162278e-02 0.01 0.26557692 0.10963269 ## 7 1.000000e-01 0.01 0.15038462 0.05783237 ## 8 3.162278e-01 0.01 0.11224359 0.04337812 ## 9 1.000000e+00 0.01 0.10730769 0.04512161 ## 10 3.162278e+00 0.01 0.10730769 0.04512161 ## 11 1.000000e+01 0.01 0.10737179 0.05526490 ## 12 3.162278e+01 0.01 0.10480769 0.05610124 ## 13 1.000000e+02 0.01 0.10480769 0.05610124 ## 14 3.162278e+02 0.01 0.10737179 0.05526490 ## 15 1.000000e+03 0.01 0.10993590 0.05690926 ## 16 1.000000e-04 0.10 0.58410256 0.05435320 ## 17 3.162278e-04 0.10 0.58410256 0.05435320 ## 18 1.000000e-03 0.10 0.58410256 0.05435320 ## 19 3.162278e-03 0.10 0.58410256 0.05435320 ## 20 1.000000e-02 0.10 0.15301282 0.06026554 ## 21 3.162278e-02 0.10 0.11480769 0.04514816 ## 22 1.000000e-01 0.10 0.10730769 0.04512161 ## 23 3.162278e-01 0.10 0.10730769 0.04512161 ## 24 1.000000e+00 0.10 0.10737179 0.05526490 ## 25 3.162278e+00 0.10 0.10737179 0.05526490 ## 26 1.000000e+01 0.10 0.10737179 0.05526490 ## 27 3.162278e+01 0.10 0.10737179 0.05526490 ## 28 1.000000e+02 0.10 0.09967949 0.04761387 ## 29 3.162278e+02 0.10 0.08429487 0.03207585 ## 30 1.000000e+03 0.10 0.08179487 0.03600437 ## 31 1.000000e-04 1.00 0.58410256 0.05435320 ## 32 3.162278e-04 1.00 0.58410256 0.05435320 ## 33 1.000000e-03 1.00 0.58410256 0.05435320 ## 34 3.162278e-03 1.00 0.58410256 0.05435320 ## 35 1.000000e-02 1.00 0.12506410 0.05342773 ## 36 3.162278e-02 1.00 0.10730769 0.06255920 ## 37 1.000000e-01 1.00 0.10993590 0.05561080 ## 38 3.162278e-01 1.00 0.10737179 0.05526490 ## 39 1.000000e+00 1.00 0.09711538 0.05107441 ## 40 3.162278e+00 1.00 0.08429487 0.03634646 ## 41 1.000000e+01 1.00 0.08692308 0.03877861 ## 42 3.162278e+01 1.00 0.08948718 0.03503648 ## 43 1.000000e+02 1.00 0.09198718 0.03272127 ## 44 3.162278e+02 1.00 0.10217949 0.04214031 ## 45 1.000000e+03 1.00 0.09692308 0.04645046 ## 46 1.000000e-04 10.00 0.58410256 0.05435320 ## 47 3.162278e-04 10.00 0.58410256 0.05435320 ## 48 1.000000e-03 10.00 0.58410256 0.05435320 ## 49 3.162278e-03 10.00 0.58410256 0.05435320 ## 50 1.000000e-02 10.00 0.58410256 0.05435320 ## 51 3.162278e-02 10.00 0.22205128 0.12710181 ## 52 1.000000e-01 10.00 0.11237179 0.03888895 ## 53 3.162278e-01 10.00 0.10217949 0.04375722 ## 54 1.000000e+00 10.00 0.09717949 0.03809440 ## 55 3.162278e+00 10.00 0.09717949 0.03809440 ## 56 1.000000e+01 10.00 0.09711538 0.04161705 ## 57 3.162278e+01 10.00 0.11487179 0.04240664 ## 58 1.000000e+02 10.00 0.13019231 0.03541140 ## 59 3.162278e+02 10.00 0.13532051 0.03865626 ## 60 1.000000e+03 10.00 0.14044872 0.04251917 sapply(results, function(x) x$best.performance) ## linear polynomial radial ## 0.10192308 0.10185897 0.08179487 sapply(results, function(x) x$best.parameters) ## $linear ## cost ## 6 0.03162278 ## ## $polynomial ## cost degree ## 7 0.1 1 ## ## $radial ## cost gamma ## 30 1000 0.1 Make some plots to back up your assertions in (b) and (c). Hint: In the lab, we used the plot() function for svm objects only in cases with \\(p = 2\\). When \\(p &gt; 2\\), you can use the plot() function to create plots displaying pairs of variables at a time. Essentially, instead of typing &gt; plot(svmfit, dat) where svmfit contains your fitted model and dat is a data frame containing your data, you can type &gt; plot(svmfit, dat, x1 ∼ x4) in order to plot just the first and fourth variables. However, you must replace x1 and x4 with the correct variable names. To find out more, type ?plot.svm. table(predict(results$radial$best.model, data), data$high_mpg) ## ## 0 1 ## 0 176 5 ## 1 20 191 plot(results$radial$best.model, data, horsepower~displacement) plot(results$radial$best.model, data, horsepower~weight) plot(results$radial$best.model, data, displacement~weight) 9.2.5 Question 8 This problem involves the OJ data set which is part of the ISLR2 package. Create a training set containing a random sample of 800 observations, and a test set containing the remaining observations. set.seed(42) train &lt;- sample(seq_len(nrow(OJ)), 800) test &lt;- setdiff(seq_len(nrow(OJ)), train) Fit a support vector classifier to the training data using cost = 0.01, with Purchase as the response and the other variables as predictors. Use the summary() function to produce summary statistics, and describe the results obtained. fit &lt;- svm(Purchase ~ ., data = OJ[train, ], kernel = &quot;linear&quot;, cost = 0.01) summary(fit) ## ## Call: ## svm(formula = Purchase ~ ., data = OJ[train, ], kernel = &quot;linear&quot;, ## cost = 0.01) ## ## ## Parameters: ## SVM-Type: C-classification ## SVM-Kernel: linear ## cost: 0.01 ## ## Number of Support Vectors: 432 ## ## ( 215 217 ) ## ## ## Number of Classes: 2 ## ## Levels: ## CH MM What are the training and test error rates? err &lt;- function(model, data) { t &lt;- table(predict(model, data), data[[&quot;Purchase&quot;]]) 1 - sum(diag(t)) / sum(t) } errs &lt;- function(model) { c(train = err(model, OJ[train, ]), test = err(model, OJ[test, ])) } errs(fit) ## train test ## 0.171250 0.162963 Use the tune() function to select an optimal cost. Consider values in the range 0.01 to 10. tuned &lt;- tune(svm, Purchase ~ ., data = OJ[train, ], kernel = &quot;linear&quot;, ranges = list(cost = 10^seq(-2, 1, length.out = 10))) tuned$best.parameters ## cost ## 7 1 summary(tuned) ## ## Parameter tuning of &#39;svm&#39;: ## ## - sampling method: 10-fold cross validation ## ## - best parameters: ## cost ## 1 ## ## - best performance: 0.1775 ## ## - Detailed performance results: ## cost error dispersion ## 1 0.01000000 0.18250 0.04133199 ## 2 0.02154435 0.18000 0.04005205 ## 3 0.04641589 0.18000 0.05041494 ## 4 0.10000000 0.18000 0.04901814 ## 5 0.21544347 0.18250 0.04377975 ## 6 0.46415888 0.18250 0.04090979 ## 7 1.00000000 0.17750 0.04031129 ## 8 2.15443469 0.18000 0.03961621 ## 9 4.64158883 0.17875 0.03821086 ## 10 10.00000000 0.18375 0.03438447 Compute the training and test error rates using this new value for cost. errs(tuned$best.model) ## train test ## 0.167500 0.162963 Repeat parts (b) through (e) using a support vector machine with a radial kernel. Use the default value for gamma. tuned2 &lt;- tune(svm, Purchase ~ ., data = OJ[train, ], kernel = &quot;radial&quot;, ranges = list(cost = 10^seq(-2, 1, length.out = 10))) tuned2$best.parameters ## cost ## 6 0.4641589 errs(tuned2$best.model) ## train test ## 0.1525000 0.1666667 Repeat parts (b) through (e) using a support vector machine with a polynomial kernel. Set degree = 2. tuned3 &lt;- tune(svm, Purchase ~ ., data = OJ[train, ], kernel = &quot;polynomial&quot;, ranges = list(cost = 10^seq(-2, 1, length.out = 10)), degree = 2) tuned3$best.parameters ## cost ## 9 4.641589 errs(tuned3$best.model) ## train test ## 0.1487500 0.1703704 Overall, which approach seems to give the best results on this data? Overall the “radial” kernel appears to perform best in this case. "],["deep-learning.html", "10 Deep Learning 10.1 Conceptual 10.2 Applied", " 10 Deep Learning 10.1 Conceptual 10.1.1 Question 1 Consider a neural network with two hidden layers: \\(p = 4\\) input units, 2 units in the first hidden layer, 3 units in the second hidden layer, and a single output. Draw a picture of the network, similar to Figures 10.1 or 10.4. Figure 10.1: A nice image. Write out an expression for \\(f(X)\\), assuming ReLU activation functions. Be as explicit as you can! The three layers (from our final output layer back to the start of our network) can be described as: \\[\\begin{align*} f(X) &amp;= g(w_{0}^{(3)} + \\sum^{K_2}_{l=1} w_{l}^{(3)} A_l^{(2)}) \\\\ A_l^{(2)} &amp;= h_l^{(2)}(X) = g(w_{l0}^{(2)} + \\sum_{k=1}^{K_1} w_{lk}^{(2)} A_k^{(1)})\\\\ A_k^{(1)} &amp;= h_k^{(1)}(X) = g(w_{k0}^{(1)} + \\sum_{j=1}^p w_{kj}^{(1)} X_j) \\\\ \\end{align*}\\] for \\(l = 1, ..., K_2 = 3\\) and \\(k = 1, ..., K_1 = 2\\) and \\(p = 4\\), where, \\[ g(z) = (z)_+ = \\begin{cases} 0, &amp; \\text{if } z &lt; 0 \\\\ z, &amp; \\text{otherwise} \\end{cases} \\] Now plug in some values for the coefficients and write out the value of \\(f(X)\\). We can perhaps achieve this most easily by fitting a real model. Note, in the plot shown here, we also include the “bias” or intercept terms. library(ISLR2) library(neuralnet) library(sigmoid) set.seed(5) train &lt;- sample(seq_len(nrow(ISLR2::Boston)), nrow(ISLR2::Boston) * 2/3) net &lt;- neuralnet(crim ~ lstat + medv + ptratio + rm, data = ISLR2::Boston[train, ], act.fct = relu, hidden = c(2, 3) ) plot(net) We can make a prediction for a given observation using this object. Firstly, let’s find an “ambiguous” test sample p &lt;- predict(net, ISLR2::Boston[-train, ]) x &lt;- ISLR2::Boston[-train, ][which.min(abs(p - mean(c(max(p), min(p))))), ] x &lt;- x[, c(&quot;lstat&quot;, &quot;medv&quot;, &quot;ptratio&quot;, &quot;rm&quot;)] predict(net, x) ## [,1] ## 441 19.14392 Or, repeating by “hand”: g &lt;- function(x) ifelse(x &gt; 0, x, 0) # relu activation function w &lt;- net$weights[[1]] # the estimated weights for each layer v &lt;- as.numeric(x) # our input predictors # to calculate our prediction we can take the dot product of our predictors # (with 1 at the start for the bias term) and our layer weights, lw) for (lw in w) v &lt;- g(c(1, v) %*% lw) v ## [,1] ## [1,] 19.14392 How many parameters are there? length(unlist(net$weights)) ## [1] 23 There are \\(4*2+2 + 2*3+3 + 3*1+1 = 23\\) parameters. 10.1.2 Question 2 Consider the softmax function in (10.13) (see also (4.13) on page 141) for modeling multinomial probabilities. In (10.13), show that if we add a constant \\(c\\) to each of the \\(z_l\\), then the probability is unchanged. If we add a constant \\(c\\) to each \\(Z_l\\) in equation 10.13 we get: \\[\\begin{align*} Pr(Y=m|X) &amp;= \\frac{e^{Z_m+c}}{\\sum_{l=0}^9e^{Z_l+c}} \\\\ &amp;= \\frac{e^{Z_m}e^c}{\\sum_{l=0}^9e^{Z_l}e^c} \\\\ &amp;= \\frac{e^{Z_m}e^c}{e^c\\sum_{l=0}^9e^{Z_l}} \\\\ &amp;= \\frac{e^{Z_m}}{\\sum_{l=0}^9e^{Z_l}} \\\\ \\end{align*}\\] which is just equation 10.13. In (4.13), show that if we add constants \\(c_j\\), \\(j = 0,1,...,p\\), to each of the corresponding coefficients for each of the classes, then the predictions at any new point \\(x\\) are unchanged. 4.13 is \\[ Pr(Y=k|X=x) = \\frac {e^{\\beta_{K0} + \\beta_{K1}x_1 + ... + \\beta_{Kp}x_p}} {\\sum_{l=1}^K e^{\\beta_{l0} + \\beta_{l1}x1 + ... + \\beta_{lp}x_p}} \\] adding constants \\(c_j\\) to each class gives: \\[\\begin{align*} Pr(Y=k|X=x) &amp;= \\frac {e^{\\beta_{K0} + \\beta_{K1}x_1 + c_1 + ... + \\beta_{Kp}x_p + c_p}} {\\sum_{l=1}^K e^{\\beta_{l0} + \\beta_{l1}x1 + c_1 + ... + \\beta_{lp}x_p + c_p}} \\\\ &amp;= \\frac {e^{c1 + ... + c_p}e^{\\beta_{K0} + \\beta_{K1}x_1 + ... + \\beta_{Kp}x_p}} {\\sum_{l=1}^K e^{c1 + ... + c_p}e^{\\beta_{l0} + \\beta_{l1}x1 + ... + \\beta_{lp}x_p}} \\\\ &amp;= \\frac {e^{c1 + ... + c_p}e^{\\beta_{K0} + \\beta_{K1}x_1 + ... + \\beta_{Kp}x_p}} {e^{c1 + ... + c_p}\\sum_{l=1}^K e^{\\beta_{l0} + \\beta_{l1}x1 + ... + \\beta_{lp}x_p}} \\\\ &amp;= \\frac {e^{\\beta_{K0} + \\beta_{K1}x_1 + ... + \\beta_{Kp}x_p}} {\\sum_{l=1}^K e^{\\beta_{l0} + \\beta_{l1}x1 + ... + \\beta_{lp}x_p}} \\\\ \\end{align*}\\] which collapses to 4.13 (with the same argument as above). This shows that the softmax function is over-parametrized. However, regularization and SGD typically constrain the solutions so that this is not a problem. 10.1.3 Question 3 Show that the negative multinomial log-likelihood (10.14) is equivalent to the negative log of the likelihood expression (4.5) when there are \\(M = 2\\) classes. Equation 10.14 is \\[ -\\sum_{i=1}^n \\sum_{m=0}^9 y_{im}\\log(f_m(x_i)) \\] Equation 4.5 is: \\[ \\ell(\\beta_0, \\beta_1) = \\prod_{i:y_i=1}p(x_i) \\prod_{i&#39;:y_i&#39;=0}(1-p(x_i&#39;)) \\] So, \\(\\log(\\ell)\\) is: \\[\\begin{align*} \\log(\\ell) &amp;= \\log \\left( \\prod_{i:y_i=1}p(x_i) \\prod_{i&#39;:y_i&#39;=0}(1-p(x_i&#39;)) \\right ) \\\\ &amp;= \\sum_{i:y_1=1}\\log(p(x_i)) + \\sum_{i&#39;:y_i&#39;=0}\\log(1-p(x_i&#39;)) \\\\ \\end{align*}\\] If we set \\(y_i\\) to be an indicator variable such that \\(y_{i1}\\) and \\(y_{i0}\\) are 1 and 0 (or 0 and 1) when our \\(i\\)th observation is 1 (or 0) respectively, then we can write: \\[ \\log(\\ell) = \\sum_{i}y_{i1}\\log(p(x_i)) + \\sum_{i}y_{i0}\\log(1-p(x_i&#39;)) \\] If we also let \\(f_1(x) = p(x)\\) and \\(f_0(x) = 1 - p(x)\\) then: \\[\\begin{align*} \\log(\\ell) &amp;= \\sum_i y_{i1}\\log(f_1(x_i)) + \\sum_{i}y_{i0}\\log(f_0(x_i&#39;)) \\\\ &amp;= \\sum_i \\sum_{m=0}^1 y_{im}\\log(f_m(x_i)) \\\\ \\end{align*}\\] When we take the negative of this, it is equivalent to 10.14 for two classes (\\(m = 0,1\\)). 10.1.4 Question 4 Consider a CNN that takes in \\(32 \\times 32\\) grayscale images and has a single convolution layer with three \\(5 \\times 5\\) convolution filters (without boundary padding). Draw a sketch of the input and first hidden layer similar to Figure 10.8. Figure 10.2: A nice image. How many parameters are in this model? There are 5 convolution matrices each with 5x5 weights (plus 5 bias terms) to estimate, therefore 130 parameters Explain how this model can be thought of as an ordinary feed-forward neural network with the individual pixels as inputs, and with constraints on the weights in the hidden units. What are the constraints? We can think of a convolution layer as a regularized fully connected layer. The regularization in this case is due to not all inputs being connected to all outputs, and weights being shared between connections. Each output node in the convolved image can be thought of as taking inputs from a limited number of input pixels (the neighboring pixels), with a set of weights specified by the convolution layer which are then shared by the connections to all other output nodes. If there were no constraints, then how many weights would there be in the ordinary feed-forward neural network in (c)? With no constraints, we would connect each output pixel in our 5x32x32 convolution layer to each node in the 32x32 original image (plus 5 bias terms), giving a total of 5,242,885 weights to estimate. 10.1.5 Question 5 In Table 10.2 on page 433, we see that the ordering of the three methods with respect to mean absolute error is different from the ordering with respect to test set \\(R^2\\). How can this be? Mean absolute error considers absolute differences between predictions and observed values, whereas \\(R^2\\) considers the (normalized) sum of squared differences, thus larger errors contribute relatively ore to \\(R^2\\) than mean absolute error. 10.2 Applied 10.2.1 Question 6 Consider the simple function \\(R(\\beta) = sin(\\beta) + \\beta/10\\). Draw a graph of this function over the range \\(\\beta \\in [−6, 6]\\). r &lt;- function(x) sin(x) + x/10 x &lt;- seq(-6, 6, 0.1) plot(x, r(x), type = &quot;l&quot;) What is the derivative of this function? \\[ cos(x) + 1/10 \\] Given \\(\\beta^0 = 2.3\\), run gradient descent to find a local minimum of \\(R(\\beta)\\) using a learning rate of \\(\\rho = 0.1\\). Show each of \\(\\beta^0, \\beta^1, ...\\) in your plot, as well as the final answer. The derivative of our function, i.e. \\(cos(x) + 1/10\\) gives us the gradient for a given \\(x\\). For gradient descent, we move \\(x\\) a little in the opposite direction, for some learning rate \\(\\rho = 0.1\\): \\[ x^{m+1} = x^m - \\rho (cos(x^m) + 1/10) \\] iter &lt;- function(x, rho) x - rho*(cos(x) + 1/10) gd &lt;- function(start, rho = 0.1) { b &lt;- start v &lt;- b while(abs(b - iter(b, 0.1)) &gt; 1e-8) { b &lt;- iter(b, 0.1) v &lt;- c(v, b) } v } res &lt;- gd(2.3) res[length(res)] ## [1] 4.612221 plot(x, r(x), type = &quot;l&quot;) points(res, r(res), col = &quot;red&quot;, pch = 19) Repeat with \\(\\beta^0 = 1.4\\). res &lt;- gd(1.4) res[length(res)] ## [1] -1.670964 plot(x, r(x), type = &quot;l&quot;) points(res, r(res), col = &quot;red&quot;, pch = 19) 10.2.2 Question 7 Fit a neural network to the Default data. Use a single hidden layer with 10 units, and dropout regularization. Have a look at Labs 10.9.1–-10.9.2 for guidance. Compare the classification performance of your model with that of linear logistic regression. library(keras) dat &lt;- ISLR2::Boston x &lt;- scale(model.matrix(crim ~ . - 1, data = dat)) n &lt;- nrow(dat) ntest &lt;- trunc(n / 3) testid &lt;- sample(1:n, ntest) y &lt;- dat$crim # logistic regression lfit &lt;- lm(crim ~ ., data = dat[-testid, ]) lpred &lt;- predict(lfit, dat[testid, ]) with(dat[testid, ], mean(abs(lpred - crim))) ## [1] 2.99129 # keras nn &lt;- keras_model_sequential() |&gt; layer_dense(units = 10, activation = &quot;relu&quot;, input_shape = ncol(x)) |&gt; layer_dropout(rate = 0.4) |&gt; layer_dense(units = 1) ## Loaded Tensorflow version 2.9.2 compile(nn, loss = &quot;mse&quot;, optimizer = optimizer_rmsprop(), metrics = list(&quot;mean_absolute_error&quot;) ) history &lt;- fit(nn, x[-testid, ], y[-testid], epochs = 100, batch_size = 26, validation_data = list(x[testid, ], y[testid]), verbose = 0 ) plot(history, smooth = FALSE) npred &lt;- predict(nn, x[testid, ]) mean(abs(y[testid] - npred)) ## [1] 2.200542 In this case, the neural network outperforms logistic regression having a lower absolute error rate on the test data. 10.2.3 Question 8 From your collection of personal photographs, pick 10 images of animals (such as dogs, cats, birds, farm animals, etc.). If the subject does not occupy a reasonable part of the image, then crop the image. Now use a pretrained image classification CNN as in Lab 10.9.4 to predict the class of each of your images, and report the probabilities for the top five predicted classes for each image. library(keras) images &lt;- list.files(&quot;images/animals&quot;) x &lt;- array(dim = c(length(images), 224, 224, 3)) for (i in seq_len(length(images))) { img &lt;- image_load(paste0(&quot;images/animals/&quot;, images[i]), target_size = c(224, 224)) x[i,,,] &lt;- image_to_array(img) } model &lt;- application_resnet50(weights = &quot;imagenet&quot;) pred &lt;- model |&gt; predict(x) |&gt; imagenet_decode_predictions(top = 5) names(pred) &lt;- images print(pred) ## $bird.jpg ## class_name class_description score ## 1 n01819313 sulphur-crested_cockatoo 0.33546212 ## 2 n01580077 jay 0.18020961 ## 3 n02441942 weasel 0.08320860 ## 4 n02058221 albatross 0.07002071 ## 5 n01855672 goose 0.05195731 ## ## $bird2.jpg ## class_name class_description score ## 1 n02006656 spoonbill 0.840428352 ## 2 n02012849 crane 0.016258694 ## 3 n01819313 sulphur-crested_cockatoo 0.009740738 ## 4 n02007558 flamingo 0.007816136 ## 5 n01667778 terrapin 0.007497438 ## ## $bird3.jpg ## class_name class_description score ## 1 n01833805 hummingbird 0.9767878056 ## 2 n02033041 dowitcher 0.0111253904 ## 3 n02028035 redshank 0.0042764195 ## 4 n02009229 little_blue_heron 0.0012727552 ## 5 n02002724 black_stork 0.0008971337 ## ## $bug.jpg ## class_name class_description score ## 1 n02190166 fly 0.67558521 ## 2 n02167151 ground_beetle 0.10097029 ## 3 n02172182 dung_beetle 0.05490869 ## 4 n02169497 leaf_beetle 0.03541917 ## 5 n02168699 long-horned_beetle 0.03515299 ## ## $butterfly.jpg ## class_name class_description score ## 1 n02951585 can_opener 0.20600407 ## 2 n03476684 hair_slide 0.09360629 ## 3 n04074963 remote_control 0.06316835 ## 4 n02110185 Siberian_husky 0.05179008 ## 5 n02123597 Siamese_cat 0.03785334 ## ## $butterfly2.jpg ## class_name class_description score ## 1 n02276258 admiral 9.999689e-01 ## 2 n01580077 jay 1.388068e-05 ## 3 n02277742 ringlet 1.235038e-05 ## 4 n02279972 monarch 3.037850e-06 ## 5 n02281787 lycaenid 1.261886e-06 ## ## $elba.jpg ## class_name class_description score ## 1 n02085620 Chihuahua 0.29891992 ## 2 n02091032 Italian_greyhound 0.20332786 ## 3 n02109961 Eskimo_dog 0.08477236 ## 4 n02086910 papillon 0.05140281 ## 5 n02110185 Siberian_husky 0.05064548 ## ## $hamish.jpg ## class_name class_description score ## 1 n02097209 standard_schnauzer 0.636145115 ## 2 n02097047 miniature_schnauzer 0.345084578 ## 3 n02097130 giant_schnauzer 0.016421778 ## 4 n02097298 Scotch_terrier 0.001911605 ## 5 n02096177 cairn 0.000205432 ## ## $poodle.jpg ## class_name class_description score ## 1 n02113799 standard_poodle 0.829671085 ## 2 n02088094 Afghan_hound 0.074567921 ## 3 n02113712 miniature_poodle 0.032005541 ## 4 n02102973 Irish_water_spaniel 0.018583104 ## 5 n02102318 cocker_spaniel 0.008629764 ## ## $tortoise.jpg ## class_name class_description score ## 1 n04033995 quilt 0.28395900 ## 2 n02110958 pug 0.15959540 ## 3 n03188531 diaper 0.14018074 ## 4 n02108915 French_bulldog 0.09364171 ## 5 n04235860 sleeping_bag 0.02608397 10.2.4 Question 9 Fit a lag-5 autoregressive model to the NYSE data, as described in the text and Lab 10.9.6. Refit the model with a 12-level factor representing the month. Does this factor improve the performance of the model? Fitting the model as described in the text. library(tidyverse) ## ── Attaching packages ─────────────────────────────────────── tidyverse 1.3.2 ── ## ✔ ggplot2 3.3.6 ✔ purrr 0.3.5 ## ✔ tibble 3.1.8 ✔ dplyr 1.0.10 ## ✔ tidyr 1.2.1 ✔ stringr 1.4.1 ## ✔ readr 2.1.3 ✔ forcats 0.5.2 ## ── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ── ## ✖ dplyr::compute() masks neuralnet::compute() ## ✖ dplyr::filter() masks stats::filter() ## ✖ dplyr::lag() masks stats::lag() library(ISLR2) xdata &lt;- data.matrix(NYSE[, c(&quot;DJ_return&quot;, &quot;log_volume&quot;,&quot;log_volatility&quot;)]) istrain &lt;- NYSE[, &quot;train&quot;] xdata &lt;- scale(xdata) lagm &lt;- function(x, k = 1) { n &lt;- nrow(x) pad &lt;- matrix(NA, k, ncol(x)) rbind(pad, x[1:(n - k), ]) } arframe &lt;- data.frame( log_volume = xdata[, &quot;log_volume&quot;], L1 = lagm(xdata, 1), L2 = lagm(xdata, 2), L3 = lagm(xdata, 3), L4 = lagm(xdata, 4), L5 = lagm(xdata, 5) ) arframe &lt;- arframe[-(1:5), ] istrain &lt;- istrain[-(1:5)] arfit &lt;- lm(log_volume ~ ., data = arframe[istrain, ]) arpred &lt;- predict(arfit, arframe[!istrain, ]) V0 &lt;- var(arframe[!istrain, &quot;log_volume&quot;]) 1 - mean((arpred - arframe[!istrain, &quot;log_volume&quot;])^2) / V0 ## [1] 0.413223 Now we add month (and work with tidyverse). arframe$month = as.factor(str_match(NYSE$date, &quot;-(\\\\d+)-&quot;)[,2])[-(1:5)] arfit2 &lt;- lm(log_volume ~ ., data = arframe[istrain, ]) arpred2 &lt;- predict(arfit2, arframe[!istrain, ]) V0 &lt;- var(arframe[!istrain, &quot;log_volume&quot;]) 1 - mean((arpred2 - arframe[!istrain, &quot;log_volume&quot;])^2) / V0 ## [1] 0.4170418 Adding month as a factor marginally improves the \\(R^2\\) of our model (from 0.413223 to 0.4170418). This is a significant improvement in fit and model 2 has a lower AIC. anova(arfit, arfit2) ## Analysis of Variance Table ## ## Model 1: log_volume ~ L1.DJ_return + L1.log_volume + L1.log_volatility + ## L2.DJ_return + L2.log_volume + L2.log_volatility + L3.DJ_return + ## L3.log_volume + L3.log_volatility + L4.DJ_return + L4.log_volume + ## L4.log_volatility + L5.DJ_return + L5.log_volume + L5.log_volatility ## Model 2: log_volume ~ L1.DJ_return + L1.log_volume + L1.log_volatility + ## L2.DJ_return + L2.log_volume + L2.log_volatility + L3.DJ_return + ## L3.log_volume + L3.log_volatility + L4.DJ_return + L4.log_volume + ## L4.log_volatility + L5.DJ_return + L5.log_volume + L5.log_volatility + ## month ## Res.Df RSS Df Sum of Sq F Pr(&gt;F) ## 1 4260 1791.0 ## 2 4249 1775.8 11 15.278 3.3234 0.000143 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 AIC(arfit, arfit2) ## df AIC ## arfit 17 8447.663 ## arfit2 28 8433.031 10.2.5 Question 10 In Section 10.9.6, we showed how to fit a linear AR model to the NYSE data using the lm() function. However, we also mentioned that we can “flatten” the short sequences produced for the RNN model in order to fit a linear AR model. Use this latter approach to fit a linear AR model to the NYSE data. Compare the test \\(R^2\\) of this linear AR model to that of the linear AR model that we fit in the lab. What are the advantages/disadvantages of each approach? The lm model is the same as that fit above: arfit &lt;- lm(log_volume ~ ., data = arframe[istrain, ]) arpred &lt;- predict(arfit, arframe[!istrain, ]) V0 &lt;- var(arframe[!istrain, &quot;log_volume&quot;]) 1 - mean((arpred - arframe[!istrain, &quot;log_volume&quot;])^2) / V0 ## [1] 0.4170418 Now we reshape the data for the RNN n &lt;- nrow(arframe) xrnn &lt;- data.matrix(arframe[, -1]) xrnn &lt;- array(xrnn, c(n, 3, 5)) xrnn &lt;- xrnn[, , 5:1] xrnn &lt;- aperm(xrnn, c(1, 3, 2)) We can add a “flatten” layer to turn the reshaped data into a long vector of predictors resulting in a linear AR model. model &lt;- keras_model_sequential() |&gt; layer_flatten(input_shape = c(5, 3)) |&gt; layer_dense(units = 1) Now let’s fit this model. model |&gt; compile(optimizer = optimizer_rmsprop(), loss = &quot;mse&quot;) history &lt;- model |&gt; fit( xrnn[istrain,, ], arframe[istrain, &quot;log_volume&quot;], batch_size = 64, epochs = 200, validation_data = list(xrnn[!istrain,, ], arframe[!istrain, &quot;log_volume&quot;]), verbose = 0 ) plot(history, smooth = FALSE) kpred &lt;- predict(model, xrnn[!istrain,, ]) 1 - mean((kpred - arframe[!istrain, &quot;log_volume&quot;])^2) / V0 ## [1] 0.4118827 Both models estimate the same number of coefficients/weights (16): coef(arfit) ## (Intercept) L1.DJ_return L1.log_volume L1.log_volatility ## 0.067916689 0.094410214 0.498673056 0.586274266 ## L2.DJ_return L2.log_volume L2.log_volatility L3.DJ_return ## -0.027299158 0.036903027 -0.931509135 0.037995916 ## L3.log_volume L3.log_volatility L4.DJ_return L4.log_volume ## 0.070312741 0.216160520 -0.004954842 0.117079461 ## L4.log_volatility L5.DJ_return L5.log_volume L5.log_volatility ## -0.039752786 -0.029620296 0.096034795 0.144510264 ## month02 month03 month04 month05 ## -0.100003367 -0.143781381 -0.028242819 -0.131120579 ## month06 month07 month08 month09 ## -0.125993911 -0.141608808 -0.163030102 -0.018889698 ## month10 month11 month12 ## -0.017206826 -0.037298183 0.008361380 model$get_weights() ## [[1]] ## [,1] ## [1,] -0.034423251 ## [2,] 0.097656645 ## [3,] 0.189218909 ## [4,] -0.006346388 ## [5,] 0.121429674 ## [6,] -0.071711779 ## [7,] 0.038498659 ## [8,] 0.084356934 ## [9,] 0.120809458 ## [10,] -0.028978353 ## [11,] 0.026936542 ## [12,] -0.722223818 ## [13,] 0.097266480 ## [14,] 0.515326083 ## [15,] 0.459617168 ## ## [[2]] ## [1] -0.008937487 The flattened RNN has a lower \\(R^2\\) on the test data than our lm model above. The lm model is quicker to fit and conceptually simpler also giving us the ability to inspect the coefficients for different variables. The flattened RNN is regularized to some extent as data are processed in batches. 10.2.6 Question 11 Repeat the previous exercise, but now fit a nonlinear AR model by “flattening” the short sequences produced for the RNN model. From the book: To fit a nonlinear AR model, we could add in a hidden layer. xfun::cache_rds({ model &lt;- keras_model_sequential() |&gt; layer_flatten(input_shape = c(5, 3)) |&gt; layer_dense(units = 32, activation = &quot;relu&quot;) |&gt; layer_dropout(rate = 0.4) |&gt; layer_dense(units = 1) model |&gt; compile( loss = &quot;mse&quot;, optimizer = optimizer_rmsprop(), metrics = &quot;mse&quot; ) history &lt;- model |&gt; fit( xrnn[istrain,, ], arframe[istrain, &quot;log_volume&quot;], batch_size = 64, epochs = 200, validation_data = list(xrnn[!istrain,, ], arframe[!istrain, &quot;log_volume&quot;]), verbose = 0 ) plot(history, smooth = FALSE, metrics = &quot;mse&quot;) kpred &lt;- predict(model, xrnn[!istrain,, ]) 1 - mean((kpred - arframe[!istrain, &quot;log_volume&quot;])^2) / V0 }) ## [1] 0.428735 This approach improves our \\(R^2\\) over the linear model above. 10.2.7 Question 12 Consider the RNN fit to the NYSE data in Section 10.9.6. Modify the code to allow inclusion of the variable day_of_week, and fit the RNN. Compute the test \\(R^2\\). To accomplish this, I’ll include day of the week as one of the lagged variables in the RNN. Thus, our input for each observation will be 4 x 5 (rather than 3 x 5). xfun::cache_rds({ xdata &lt;- data.matrix( NYSE[, c(&quot;day_of_week&quot;, &quot;DJ_return&quot;, &quot;log_volume&quot;,&quot;log_volatility&quot;)] ) istrain &lt;- NYSE[, &quot;train&quot;] xdata &lt;- scale(xdata) arframe &lt;- data.frame( log_volume = xdata[, &quot;log_volume&quot;], L1 = lagm(xdata, 1), L2 = lagm(xdata, 2), L3 = lagm(xdata, 3), L4 = lagm(xdata, 4), L5 = lagm(xdata, 5) ) arframe &lt;- arframe[-(1:5), ] istrain &lt;- istrain[-(1:5)] n &lt;- nrow(arframe) xrnn &lt;- data.matrix(arframe[, -1]) xrnn &lt;- array(xrnn, c(n, 4, 5)) xrnn &lt;- xrnn[,, 5:1] xrnn &lt;- aperm(xrnn, c(1, 3, 2)) dim(xrnn) model &lt;- keras_model_sequential() |&gt; layer_simple_rnn(units = 12, input_shape = list(5, 4), dropout = 0.1, recurrent_dropout = 0.1 ) |&gt; layer_dense(units = 1) model |&gt; compile(optimizer = optimizer_rmsprop(), loss = &quot;mse&quot;) history &lt;- model |&gt; fit( xrnn[istrain,, ], arframe[istrain, &quot;log_volume&quot;], batch_size = 64, epochs = 200, validation_data = list(xrnn[!istrain,, ], arframe[!istrain, &quot;log_volume&quot;]), verbose = 0 ) kpred &lt;- predict(model, xrnn[!istrain,, ]) 1 - mean((kpred - arframe[!istrain, &quot;log_volume&quot;])^2) / V0 }) ## [1] 0.4506411 10.2.8 Question 13 Repeat the analysis of Lab 10.9.5 on the IMDb data using a similarly structured neural network. There we used a dictionary of size 10,000. Consider the effects of varying the dictionary size. Try the values 1000, 3000, 5000, and 10,000, and compare the results. xfun::cache_rds({ library(knitr) accuracy &lt;- c() for(max_features in c(1000, 3000, 5000, 10000)) { imdb &lt;- dataset_imdb(num_words = max_features) c(c(x_train, y_train), c(x_test, y_test)) %&lt;-% imdb maxlen &lt;- 500 x_train &lt;- pad_sequences(x_train, maxlen = maxlen) x_test &lt;- pad_sequences(x_test, maxlen = maxlen) model &lt;- keras_model_sequential() |&gt; layer_embedding(input_dim = max_features, output_dim = 32) |&gt; layer_lstm(units = 32) |&gt; layer_dense(units = 1, activation = &quot;sigmoid&quot;) model |&gt; compile( optimizer = &quot;rmsprop&quot;, loss = &quot;binary_crossentropy&quot;, metrics = &quot;acc&quot; ) history &lt;- fit(model, x_train, y_train, epochs = 10, batch_size = 128, validation_data = list(x_test, y_test), verbose = 1 ) predy &lt;- predict(model, x_test) &gt; 0.5 accuracy &lt;- c(accuracy, mean(abs(y_test == as.numeric(predy)))) } tibble( &quot;Max Features&quot; = c(1000, 3000, 5000, 10000), &quot;Accuracy&quot; = accuracy ) |&gt; kable() }) Max Features Accuracy 1000 0.85412 3000 0.87044 5000 0.84700 10000 0.86012 Varying the dictionary size does not make a substantial impact on our estimates of accuracy. However, the models do take a substantial amount of time to fit and it is not clear we are finding the best fitting models in each case. For example, the model using a dictionary size of 10,000 obtained an accuracy of 0.8721 in the text which is as different from the estimate obtained here as are the differences between the models with different dictionary sizes. "],["survival-analysis-and-censored-data.html", "11 Survival Analysis and Censored Data 11.1 Conceptual 11.2 Applied", " 11 Survival Analysis and Censored Data 11.1 Conceptual 11.1.1 Question 1 For each example, state whether or not the censoring mechanism is independent. Justify your answer. In a study of disease relapse, due to a careless research scientist, all patients whose phone numbers begin with the number “2” are lost to follow up. Independent. There’s no reason to think disease relapse should be related to the first digit of a phone number. In a study of longevity, a formatting error causes all patient ages that exceed 99 years to be lost (i.e. we know that those patients are more than 99 years old, but we do not know their exact ages). Not independent. Older patients are more likely to see an event that younger. Hospital A conducts a study of longevity. However, very sick patients tend to be transferred to Hospital B, and are lost to follow up. Not independent. Sick patients are more likely to see an event that healthy. In a study of unemployment duration, the people who find work earlier are less motivated to stay in touch with study investigators, and therefore are more likely to be lost to follow up. Not independent. More employable individuals are more likely to see an event. In a study of pregnancy duration, women who deliver their babies pre-term are more likely to do so away from their usual hospital, and thus are more likely to be censored, relative to women who deliver full-term babies. Not independent. Delivery away from hospital will be associated with pregnancy duration. A researcher wishes to model the number of years of education of the residents of a small town. Residents who enroll in college out of town are more likely to be lost to follow up, and are also more likely to attend graduate school, relative to those who attend college in town. Not independent. Years of education will be associated with enrolling in out of town colleges. Researchers conduct a study of disease-free survival (i.e. time until disease relapse following treatment). Patients who have not relapsed within five years are considered to be cured, and thus their survival time is censored at five years. In other words we assume all events happen within five years, so censoring after this time is equivalent to not censoring at all so the censoring is independent. We wish to model the failure time for some electrical component. This component can be manufactured in Iowa or in Pittsburgh, with no difference in quality. The Iowa factory opened five years ago, and so components manufactured in Iowa are censored at five years. The Pittsburgh factory opened two years ago, so those components are censored at two years. If there is no difference in quality then location and therefore censoring is independent of failure time. We wish to model the failure time of an electrical component made in two different factories, one of which opened before the other. We have reason to believe that the components manufactured in the factory that opened earlier are of higher quality. In this case, the difference in opening times of the two locations will mean that any difference in quality between locations will be associated with censoring, so censoring is not independent. 11.1.2 Question 2 We conduct a study with \\(n = 4\\) participants who have just purchased cell phones, in order to model the time until phone replacement. The first participant replaces her phone after 1.2 years. The second participant still has not replaced her phone at the end of the two-year study period. The third participant changes her phone number and is lost to follow up (but has not yet replaced her phone) 1.5 years into the study. The fourth participant replaces her phone after 0.2 years. For each of the four participants (\\(i = 1,..., 4\\)), answer the following questions using the notation introduced in Section 11.1: Is the participant’s cell phone replacement time censored? No, Yes, Yes and No. Censoring occurs when we do not know if or when the phone was replaced. Is the value of \\(c_i\\) known, and if so, then what is it? \\(c_i\\) is censoring time. For the four participants these are: NA. 2. 1.5 and NA. Is the value of \\(t_i\\) known, and if so, then what is it? \\(t_i\\) is time to event. For the four participants these are: 1.2, NA, NA and 0.2. Is the value of \\(y_i\\) known, and if so, then what is it? \\(y_i\\) is the observed time. For the four participants these are: 1.2, 2, 1.5 and 0.2. Is the value of \\(\\delta_i\\) known, and if so, then what is it? \\(\\delta_i\\) is an indicator for censoring. The nomenclature introduced here defines this to be 1 if we observe the true “survival” time and 0 if we observe the censored time. Therefore, for these participants, the values are: 1, 0, 0 and 1. 11.1.3 Question 3 For the example in Exercise 2, report the values of \\(K\\), \\(d_1,...,d_K\\), \\(r_1,...,r_K\\), and \\(q_1,...,q_K\\), where this notation was defined in Section 11.3. \\(K\\) is the number of unique deaths, which is 2. \\(d_k\\) represents the unique death times, which are: 0.2, 1.2. \\(r_k\\) denotes the number of patients alive and in the study just before \\(d_k\\). Note the first event is for patient 4, then patient 1, then patient 3 is censored and finally the study ends with patient 2 still involved. Therefore \\(r_k\\) takes values are: 4, 3. \\(q_k\\) denotes the number of patients who died at time \\(d_k\\), therefore this takes values: 1, 1. We can check by using the survival package. library(survival) x &lt;- Surv(c(1.2, 2, 1.5, 0.2), event = c(1, 0, 0, 1)) summary(survfit(x ~ 1)) ## Call: survfit(formula = x ~ 1) ## ## time n.risk n.event survival std.err lower 95% CI upper 95% CI ## 0.2 4 1 0.75 0.217 0.426 1 ## 1.2 3 1 0.50 0.250 0.188 1 11.1.4 Question 4 This problem makes use of the Kaplan-Meier survival curve displayed in Figure 11.9. The raw data that went into plotting this survival curve is given in Table 11.4. The covariate column of that table is not needed for this problem. What is the estimated probability of survival past 50 days? There are 2 events that happen before 50 days. The number at risk \\(r_k\\) are 5 and 4 (one was censored early on), thus survival probability is \\(4/5 * 3/4 = 0.6\\). Equivalently, we can use the survival package. library(tidyverse) ## ── Attaching packages ─────────────────────────────────────── tidyverse 1.3.2 ── ## ✔ ggplot2 3.3.6 ✔ purrr 0.3.5 ## ✔ tibble 3.1.8 ✔ dplyr 1.0.10 ## ✔ tidyr 1.2.1 ✔ stringr 1.4.1 ## ✔ readr 2.1.3 ✔ forcats 0.5.2 ## ── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ── ## ✖ dplyr::filter() masks stats::filter() ## ✖ dplyr::lag() masks stats::lag() table_data &lt;- tribble( ~Y, ~D, ~X, 26.5, 1, 0.1, 37.2, 1, 11, 57.3, 1, -0.3, 90.8, 0, 2.8, 20.2, 0, 1.8, 89.8, 0, 0.4 ) x &lt;- Surv(table_data$Y, table_data$D) summary(survfit(x ~ 1)) ## Call: survfit(formula = x ~ 1) ## ## time n.risk n.event survival std.err lower 95% CI upper 95% CI ## 26.5 5 1 0.8 0.179 0.516 1 ## 37.2 4 1 0.6 0.219 0.293 1 ## 57.3 3 1 0.4 0.219 0.137 1 Write out an analytical expression for the estimated survival function. For instance, your answer might be something along the lines of \\[ \\hat{S}(t) = \\begin{cases} 0.8 &amp; \\text{if } t &lt; 31\\\\ 0.5 &amp; \\text{if } 31 \\le t &lt; 77\\\\ 0.22 &amp; \\text{if } 77 \\le t \\end{cases} \\] (The previous equation is for illustration only: it is not the correct answer!) \\[ \\hat{S}(t) = \\begin{cases} 1 &amp; \\text{if } t &lt; 26.5 \\\\ 0.8 &amp; \\text{if } 26.5 \\le t &lt; 37.2 \\\\ 0.6 &amp; \\text{if } 37.2 \\le t &lt; 57.3 \\\\ 0.4 &amp; \\text{if } 57.3 \\le t \\end{cases} \\] 11.1.5 Question 5 Sketch the survival function given by the equation \\[ \\hat{S}(t) = \\begin{cases} 0.8, &amp; \\text{if } t &lt; 31\\\\ 0.5, &amp; \\text{if } 31 \\le t &lt; 77\\\\ 0.22 &amp; \\text{if } 77 \\le t \\end{cases} \\] Your answer should look something like Figure 11.9. We can draw this plot, or even engineer data that will generate the required plot… plot(NULL, xlim = c(0, 100), ylim = c(0, 1), ylab = &quot;Estimated Probability of Survival&quot;, xlab = &quot;Time in Days&quot; ) lines( c(0, 31, 31, 77, 77, 100), c(0.8, 0.8, 0.5, 0.5, 0.22, 0.22) ) 11.1.6 Question 6 This problem makes use of the data displayed in Figure 11.1. In completing this problem, you can refer to the observation times as \\(y_1,...,y_4\\). The ordering of these observation times can be seen from Figure 11.1; their exact values are not required. Report the values of \\(\\delta_1,...,\\delta_4\\), \\(K\\), \\(d_1,...,d_K\\), \\(r_1,...,r_K\\), and \\(q_1,...,q_K\\). The relevant notation is defined in Sections 11.1 and 11.3. \\(\\delta\\) values are: 1, 0, 1, 0. \\(K\\) is 2 \\(d\\) values are \\(y_3\\) and \\(y_1\\). \\(r\\) values are 4 and 2. \\(q\\) values are 1 and 1. Sketch the Kaplan-Meier survival curve corresponding to this data set. (You do not need to use any software to do this—you can sketch it by hand using the results obtained in (a).) plot(NULL, xlim = c(0, 350), ylim = c(0, 1), ylab = &quot;Estimated Probability of Survival&quot;, xlab = &quot;Time in Days&quot; ) lines( c(0, 150, 150, 300, 300, 350), c(1, 1, 0.75, 0.75, 0.375, 0.375) ) x &lt;- Surv(c(300, 350, 150, 250), c(1, 0, 1, 0)) Based on the survival curve estimated in (b), what is the probability that the event occurs within 200 days? What is the probability that the event does not occur within 310 days? 0.75 and 0.375. Write out an expression for the estimated survival curve from (b). \\[ \\hat{S}(t) = \\begin{cases} 1 &amp; \\text{if } t &lt; y_3 \\\\ 0.75 &amp; \\text{if } y_3 \\le t &lt; y_1 \\\\ 0.375 &amp; \\text{if } y_1 \\le t \\end{cases} \\] 11.1.7 Question 7 In this problem, we will derive (11.5) and (11.6), which are needed for the construction of the log-rank test statistic (11.8). Recall the notation in Table 11.1. Assume that there is no difference between the survival functions of the two groups. Then we can think of \\(q_{1k}\\) as the number of failures if we draw $r_{1k} observations, without replacement, from a risk set of \\(r_k\\) observations that contains a total of \\(q_k\\) failures. Argue that \\(q_{1k}\\) follows a hypergeometric distribution. Write the parameters of this distribution in terms of \\(r_{1k}\\), \\(r_k\\), and \\(q_k\\). A hypergeometric distributions models sampling without replacement from a finite pool where each sample is a success or failure. This fits the situation here, where with have a finite number of samples in the risk set. The hypergeometric distribution is parameterized as \\(k\\) successes in \\(n\\) draws, without replacement, from a population of size \\(N\\) with \\(K\\) objects with that feature. Mapping to our situation, \\(q_{1k}\\) is \\(k\\), \\(r_{1k}\\) is \\(n\\), \\(r_k\\) is \\(N\\) and \\(q_k\\) is \\(K\\). Given your previous answer, and the properties of the hypergeometric distribution, what are the mean and variance of \\(q_{1k}\\)? Compare your answer to (11.5) and (11.6). With the above parameterization, the mean (\\(n K/N\\)) is \\(r_{1k} q_k/r_K\\). The variance \\(n K/N (N-K)/N (N-n)/(N-1)\\) is \\[ r_{1k} \\frac{q_k}{r_k} \\frac{r_k-q_k}{r_k} \\frac{r_k - r_{1k}}{r_k - 1} \\] These are equivalent to 11.5 and 11.6. 11.1.8 Question 8 Recall that the survival function \\(S(t)\\), the hazard function \\(h(t)\\), and the density function \\(f(t)\\) are defined in (11.2), (11.9), and (11.11), respectively. Furthermore, define \\(F(t) = 1 − S(t)\\). Show that the following relationships hold: \\[ f(t) = dF(t)/dt \\\\ S(t) = \\exp\\left(-\\int_0^t h(u)du\\right) \\] If \\(F(t) = 1 - S(t)\\), then \\(F(t)\\) is the cumulative density function (cdf) for \\(t\\). For a continuous distribution, a cdf, e.g. \\(F(t)\\) can be expressed as an integral (up to some value \\(x\\)) of the probability density function (pdf), i.e. \\(F(t) = \\int_{-\\infty}^x f(x) dt\\). Equivalently, the derivative of the cdf is its pdf: \\(f(t) = \\frac{d F(t)}{dt}\\). Then, \\(h(t) = \\frac{f(t)}{S(t)} = \\frac{dF(t)/dt}{S(t)} = \\frac{-dS(t)/dt}{S(t)}\\). From basic calculus, this can be rewritten as \\(h(t) = -\\frac{d}{dt}\\log{S(t)}\\). Integrating and then exponentiating we get the second identity. 11.1.9 Question 9 In this exercise, we will explore the consequences of assuming that the survival times follow an exponential distribution. Suppose that a survival time follows an \\(Exp(\\lambda)\\) distribution, so that its density function is \\(f(t) = \\lambda\\exp(−\\lambda t)\\). Using the relationships provided in Exercise 8, show that \\(S(t) = \\exp(−\\lambda t)\\). The cdf of an exponential distribution is \\(1 - \\exp{-\\lambda x}\\) and \\(S(t)\\) is \\(1 - F(t)\\) where \\(F(t)\\) is the cdf. Hence, \\(S(t) = \\exp(−\\lambda t)\\). Now suppose that each of \\(n\\) independent survival times follows an \\(Exp(\\lambda)\\) distribution. Write out an expression for the likelihood function (11.13). \\[ \\ell = \\prod_i \\lambda^{d_i}y_i^{-\\lambda} \\] Show that the maximum likelihood estimator for \\(\\lambda\\) is \\[ \\hat\\lambda = \\sum_{i=1}^n \\delta_i / \\sum_{i=1}^n y_i. \\] \\[\\begin{align*} \\log \\ell &amp;= \\sum_i{\\delta_i\\log\\lambda - \\lambda y_i} \\\\ &amp;= \\log\\lambda\\sum_i{\\delta_i} - \\lambda\\sum_i{y_i} \\end{align*}\\] Differentiating this expression with respect to λ we get: \\[ \\frac{d \\log \\ell}{d \\lambda} = \\frac{\\sum_i{\\delta_i}}{\\lambda} - \\sum_i{y_i} \\] A maximum is when this is 0, or when \\(\\hat\\lambda = \\sum_{i=1}^n \\delta_i / \\sum_{i=1}^n y_i\\). Use your answer to (c) to derive an estimator of the mean survival time. Hint: For (d), recall that the mean of an \\(Exp(\\lambda)\\) random variable is \\(1/\\lambda\\). Estimated mean survival would be \\(1/\\lambda\\) which given the above would be \\(\\sum_{i=1}^n y_i / \\sum_{i=1}^n \\delta_i\\), which can be thought of as the total observation time over the total number of deaths. 11.2 Applied 11.2.1 Question 10 This exercise focuses on the brain tumor data, which is included in the ISLR2 R library. Plot the Kaplan-Meier survival curve with ±1 standard error bands, using the survfit() function in the survival package. library(ISLR2) x &lt;- Surv(BrainCancer$time, BrainCancer$status) plot(survfit(x ~ 1), xlab = &quot;Months&quot;, ylab = &quot;Estimated Probability of Survival&quot;, col = &quot;steelblue&quot;, conf.int = 0.67 ) Draw a bootstrap sample of size \\(n = 88\\) from the pairs (\\(y_i\\), \\(\\delta_i\\)), and compute the resulting Kaplan-Meier survival curve. Repeat this process \\(B = 200\\) times. Use the results to obtain an estimate of the standard error of the Kaplan-Meier survival curve at each timepoint. Compare this to the standard errors obtained in (a). library(tidyverse) plot(survfit(x ~ 1), xlab = &quot;Months&quot;, ylab = &quot;Estimated Probability of Survival&quot;, col = &quot;steelblue&quot;, conf.int = 0.67 ) fit &lt;- survfit(x ~ 1) dat &lt;- tibble(time = c(0, fit$time)) for (i in 1:200) { y &lt;- survfit(sample(x, 88, replace = TRUE) ~ 1) y &lt;- tibble(time = c(0, y$time), &quot;s{i}&quot; := c(1, y$surv)) dat &lt;- left_join(dat, y, by = &quot;time&quot;) } res &lt;- fill(dat, starts_with(&quot;s&quot;)) |&gt; rowwise() |&gt; transmute(sd = sd(c_across(starts_with(&quot;s&quot;)))) se &lt;- res$sd[2:nrow(res)] lines(fit$time, fit$surv - se, lty = 2, col = &quot;red&quot;) lines(fit$time, fit$surv + se, lty = 2, col = &quot;red&quot;) Fit a Cox proportional hazards model that uses all of the predictors to predict survival. Summarize the main findings. fit &lt;- coxph(Surv(time, status) ~ sex + diagnosis + loc + ki + gtv + stereo, data = BrainCancer) fit ## Call: ## coxph(formula = Surv(time, status) ~ sex + diagnosis + loc + ## ki + gtv + stereo, data = BrainCancer) ## ## coef exp(coef) se(coef) z p ## sexMale 0.18375 1.20171 0.36036 0.510 0.61012 ## diagnosisLG glioma 0.91502 2.49683 0.63816 1.434 0.15161 ## diagnosisHG glioma 2.15457 8.62414 0.45052 4.782 1.73e-06 ## diagnosisOther 0.88570 2.42467 0.65787 1.346 0.17821 ## locSupratentorial 0.44119 1.55456 0.70367 0.627 0.53066 ## ki -0.05496 0.94653 0.01831 -3.001 0.00269 ## gtv 0.03429 1.03489 0.02233 1.536 0.12466 ## stereoSRT 0.17778 1.19456 0.60158 0.296 0.76760 ## ## Likelihood ratio test=41.37 on 8 df, p=1.776e-06 ## n= 87, number of events= 35 ## (1 observation deleted due to missingness) diagnosisHG and ki are highly significant. Stratify the data by the value of ki. (Since only one observation has ki=40, you can group that observation together with the observations that have ki=60.) Plot Kaplan-Meier survival curves for each of the five strata, adjusted for the other predictors. To adjust for other predictors, we fit a model that includes those predictors and use this model to predict new, artificial, data where we allow ki to take each possible value, but set the other predictors to be the mode or mean of the other predictors. library(ggfortify) modaldata &lt;- data.frame( sex = rep(&quot;Female&quot;, 5), diagnosis = rep(&quot;Meningioma&quot;, 5), loc = rep(&quot;Supratentorial&quot;, 5), ki = c(60, 70, 80, 90, 100), gtv = rep(mean(BrainCancer$gtv), 5), stereo = rep(&quot;SRT&quot;, 5) ) survplots &lt;- survfit(fit, newdata = modaldata) plot(survplots, xlab = &quot;Months&quot;, ylab = &quot;Survival Probability&quot;, col = 2:6) legend(&quot;bottomleft&quot;, c(&quot;60&quot;, &quot;70&quot;, &quot;80&quot;, &quot;90&quot;, &quot;100&quot;), col = 2:6, lty = 1) 11.2.2 Question 11 This example makes use of the data in Table 11.4. Create two groups of observations. In Group 1, \\(X &lt; 2\\), whereas in Group 2, \\(X \\ge 2\\). Plot the Kaplan-Meier survival curves corresponding to the two groups. Be sure to label the curves so that it is clear which curve corresponds to which group. By eye, does there appear to be a difference between the two groups’ survival curves? x &lt;- split(Surv(table_data$Y, table_data$D), table_data$X &lt; 2) plot(NULL, xlim = c(0, 100), ylim = c(0, 1), ylab = &quot;Survival Probability&quot;) lines(survfit(x[[1]] ~ 1), conf.int=FALSE, col = 2) lines(survfit(x[[2]] ~ 1), conf.int=FALSE, col = 3) legend(&quot;bottomleft&quot;, c(&quot;&gt;= 2&quot;, &quot;&lt;2&quot;), col = 2:3, lty = 1) There does not appear to be any difference between the curves. Fit Cox’s proportional hazards model, using the group indicator as a covariate. What is the estimated coefficient? Write a sentence providing the interpretation of this coefficient, in terms of the hazard or the instantaneous probability of the event. Is there evidence that the true coefficient value is non-zero? fit &lt;- coxph(Surv(Y, D) ~ X &lt; 2, data = table_data) fit ## Call: ## coxph(formula = Surv(Y, D) ~ X &lt; 2, data = table_data) ## ## coef exp(coef) se(coef) z p ## X &lt; 2TRUE 0.3401 1.4051 1.2359 0.275 0.783 ## ## Likelihood ratio test=0.08 on 1 df, p=0.7797 ## n= 6, number of events= 3 The coefficient is \\(0.3401\\). This implies a slightly increased hazard when \\(X &lt; 2\\) but it is not significantly different to zero (P = 0.8). Recall from Section 11.5.2 that in the case of a single binary covariate, the log-rank test statistic should be identical to the score statistic for the Cox model. Conduct a log-rank test to determine whether there is a difference between the survival curves for the two groups. How does the p-value for the log-rank test statistic compare to the \\(p\\)-value for the score statistic for the Cox model from (b)? summary(fit)$sctest ## test df pvalue ## 0.07644306 1.00000000 0.78217683 survdiff(Surv(Y, D) ~ X &lt; 2, data = table_data)$chisq ## [1] 0.07644306 They are identical. "],["unsupervised-learning.html", "12 Unsupervised Learning 12.1 Conceptual 12.2 Applied", " 12 Unsupervised Learning 12.1 Conceptual 12.1.1 Question 1 This problem involves the \\(K\\)-means clustering algorithm. Prove (12.18). 12.18 is: \\[ \\frac{1}{|C_k|}\\sum_{i,i&#39; \\in C_k} \\sum_{j=1}^p (x_{ij} - x_{i&#39;j})^2 = 2 \\sum_{i \\in C_k} \\sum_{j=1}^p (x_{ij} - \\bar{x}_{kj})^2 \\] where \\[\\bar{x}_{kj} = \\frac{1}{|C_k|}\\sum_{i \\in C_k} x_{ij}\\] On the left hand side we compute the difference between each observation (indexed by \\(i\\) and \\(i&#39;\\)). In the second we compute the difference between each observation and the mean. Intuitively this identity is clear (the factor of 2 is present because we calculate the difference between each pair twice). However, to prove. Note first that, \\[\\begin{align} (x_{ij} - x_{i&#39;j})^2 = &amp; ((x_{ij} - \\bar{x}_{kj}) - (x_{i&#39;j} - \\bar{x}_{kj}))^2 \\\\ = &amp; (x_{ij} - \\bar{x}_{kj})^2 - 2(x_{ij} - \\bar{x}_{kj})(x_{i&#39;j} - \\bar{x}_{kj}) + (x_{i&#39;j} - \\bar{x}_{kj})^2 \\end{align}\\] Note that the first term is independent of \\(i&#39;\\) and the last is independent of \\(i\\). Therefore, 10.12 can be written as: \\[\\begin{align} \\frac{1}{|C_k|}\\sum_{i,i&#39; \\in C_k} \\sum_{j=1}^p (x_{ij} - x_{i&#39;j})^2 = &amp; \\frac{1}{|C_k|}\\sum_{i,i&#39; \\in C_k}\\sum_{j=1}^p (x_{ij} - \\bar{x}_{kj})^2 - \\frac{1}{|C_k|}\\sum_{i,i&#39; \\in C_k}\\sum_{j=1}^p 2(x_{ij} - \\bar{x}_{kj})(x_{i&#39;j} - \\bar{x}_{kj}) + \\frac{1}{|C_k|}\\sum_{i,i&#39; \\in C_k}\\sum_{j=1}^p (x_{i&#39;j} - \\bar{x}_{kj})^2 \\\\ = &amp; \\frac{|C_k|}{|C_k|}\\sum_{i \\in C_k}\\sum_{j=1}^p (x_{ij} - \\bar{x}_{kj})^2 - \\frac{2}{|C_k|}\\sum_{i,i&#39; \\in C_k}\\sum_{j=1}^p (x_{ij} - \\bar{x}_{kj})(x_{i&#39;j} - \\bar{x}_{kj}) + \\frac{|C_k|}{|C_k|}\\sum_{i \\in C_k}\\sum_{j=1}^p (x_{ij} - \\bar{x}_{kj})^2 \\\\ = &amp; 2 \\sum_{i \\in C_k}\\sum_{j=1}^p (x_{ij} - \\bar{x}_{kj})^2 \\end{align}\\] Note that we can drop the term containing \\((x_{ij} - \\bar{x}_{kj})(x_{i&#39;j} - \\bar{x}_{kj})\\) since this is 0 when summed over combinations of \\(i\\) and \\(i&#39;\\) for a given \\(j\\). On the basis of this identity, argue that the \\(K\\)-means clustering algorithm (Algorithm 12.2) decreases the objective (12.17) at each iteration. Equation 10.12 demonstrates that the euclidean distance between each possible pair of samples can be related to the difference from each sample to the mean of the cluster. The K-means algorithm works by minimizing the euclidean distance to each centroid, thus also minimizes the within-cluster variance. 12.1.2 Question 2 Suppose that we have four observations, for which we compute a dissimilarity matrix, given by \\[\\begin{bmatrix} &amp; 0.3 &amp; 0.4 &amp; 0.7 \\\\ 0.3 &amp; &amp; 0.5 &amp; 0.8 \\\\ 0.4 &amp; 0.5 &amp; &amp; 0.45 \\\\ 0.7 &amp; 0.8 &amp; 0.45 &amp; \\\\ \\end{bmatrix}\\] For instance, the dissimilarity between the first and second observations is 0.3, and the dissimilarity between the second and fourth observations is 0.8. On the basis of this dissimilarity matrix, sketch the dendrogram that results from hierarchically clustering these four observations using complete linkage. Be sure to indicate on the plot the height at which each fusion occurs, as well as the observations corresponding to each leaf in the dendrogram. m &lt;- matrix(c(0, 0.3, 0.4, 0.7, 0.3, 0, 0.5, 0.8, 0.4, 0.5, 0., 0.45, 0.7, 0.8, 0.45, 0), ncol = 4) c1 &lt;- hclust(as.dist(m), method = &quot;complete&quot;) plot(c1) Repeat (a), this time using single linkage clustering. c2 &lt;- hclust(as.dist(m), method = &quot;single&quot;) plot(c2) Suppose that we cut the dendrogram obtained in (a) such that two clusters result. Which observations are in each cluster? table(1:4, cutree(c1, 2)) ## ## 1 2 ## 1 1 0 ## 2 1 0 ## 3 0 1 ## 4 0 1 Suppose that we cut the dendrogram obtained in (b) such that two clusters result. Which observations are in each cluster? table(1:4, cutree(c2, 2)) ## ## 1 2 ## 1 1 0 ## 2 1 0 ## 3 1 0 ## 4 0 1 It is mentioned in the chapter that at each fusion in the dendrogram, the position of the two clusters being fused can be swapped without changing the meaning of the dendrogram. Draw a dendrogram that is equivalent to the dendrogram in (a), for which two or more of the leaves are repositioned, but for which the meaning of the dendrogram is the same. plot(c1, labels = c(2, 1, 3, 4)) 12.1.3 Question 3 In this problem, you will perform \\(K\\)-means clustering manually, with \\(K = 2\\), on a small example with \\(n = 6\\) observations and \\(p = 2\\) features. The observations are as follows. Obs. \\(X_1\\) \\(X_2\\) 1 1 4 2 1 3 3 0 4 4 5 1 5 6 2 6 4 0 Plot the observations. library(ggplot2) d &lt;- data.frame( x1 = c(1, 1, 0, 5, 6, 4), x2 = c(4, 3, 4, 1, 2, 0) ) ggplot(d, aes(x = x1, y = x2)) + geom_point() Randomly assign a cluster label to each observation. You can use the sample() command in R to do this. Report the cluster labels for each observation. set.seed(42) d$cluster &lt;- sample(c(1, 2), size = nrow(d), replace = TRUE) Compute the centroid for each cluster. centroids &lt;- sapply(c(1,2), function(i) colMeans(d[d$cluster == i, 1:2])) Assign each observation to the centroid to which it is closest, in terms of Euclidean distance. Report the cluster labels for each observation. dist &lt;- sapply(1:2, function(i) { sqrt((d$x1 - centroids[1, i])^2 + (d$x2 - centroids[2, i])^2) }) d$cluster &lt;- apply(dist, 1, which.min) Repeat (c) and (d) until the answers obtained stop changing. centroids &lt;- sapply(c(1,2), function(i) colMeans(d[d$cluster == i, 1:2])) dist &lt;- sapply(1:2, function(i) { sqrt((d$x1 - centroids[1, i])^2 + (d$x2 - centroids[2, i])^2) }) d$cluster &lt;- apply(dist, 1, which.min) In this case, we get stable labels after the first iteration. In your plot from (a), color the observations according to the cluster labels obtained. ggplot(d, aes(x = x1, y = x2, color = factor(cluster))) + geom_point() 12.1.4 Question 4 Suppose that for a particular data set, we perform hierarchical clustering using single linkage and using complete linkage. We obtain two dendrograms. At a certain point on the single linkage dendrogram, the clusters {1, 2, 3} and {4, 5} fuse. On the complete linkage dendrogram, the clusters {1, 2, 3} and {4, 5} also fuse at a certain point. Which fusion will occur higher on the tree, or will they fuse at the same height, or is there not enough information to tell? The complete linkage fusion will likely be higher in the tree since single linkage is defined as being the minimum distance between two clusters. However, there is a chance that they could be at the same height (so technically there is not enough information to tell). At a certain point on the single linkage dendrogram, the clusters {5} and {6} fuse. On the complete linkage dendrogram, the clusters {5} and {6} also fuse at a certain point. Which fusion will occur higher on the tree, or will they fuse at the same height, or is there not enough information to tell? They will fuse at the same height (the algorithm for calculating distance is the same when the clusters are of size 1). 12.1.5 Question 5 In words, describe the results that you would expect if you performed \\(K\\)-means clustering of the eight shoppers in Figure 12.16, on the basis of their sock and computer purchases, with \\(K = 2\\). Give three answers, one for each of the variable scalings displayed. Explain. In cases where variables are scaled we would expect clusters to correspond to whether or not the retainer sold a computer. In the first case (raw numbers of items sold), we would expect clusters to represent low vs high numbers of sock purchases. To test, we can run the analysis in R: set.seed(42) dat &lt;- data.frame( socks = c(8, 11, 7, 6, 5, 6, 7, 8), computers = c(0, 0, 0, 0, 1, 1, 1, 1) ) kmeans(dat, 2)$cluster ## [1] 1 1 2 2 2 2 2 1 kmeans(scale(dat), 2)$cluster ## [1] 1 1 1 1 2 2 2 2 dat$computers &lt;- dat$computers * 2000 kmeans(dat, 2)$cluster ## [1] 1 1 1 1 2 2 2 2 12.1.6 Question 6 We saw in Section 12.2.2 that the principal component loading and score vectors provide an approximation to a matrix, in the sense of (12.5). Specifically, the principal component score and loading vectors solve the optimization problem given in (12.6). Now, suppose that the M principal component score vectors zim, \\(m = 1,...,M\\), are known. Using (12.6), explain that the first \\(M\\) principal component loading vectors \\(\\phi_{jm}\\), \\(m = 1,...,M\\), can be obtaining by performing \\(M\\) separate least squares linear regressions. In each regression, the principal component score vectors are the predictors, and one of the features of the data matrix is the response. 12.2 Applied 12.2.1 Question 7 In the chapter, we mentioned the use of correlation-based distance and Euclidean distance as dissimilarity measures for hierarchical clustering. It turns out that these two measures are almost equivalent: if each observation has been centered to have mean zero and standard deviation one, and if we let \\(r_{ij}\\) denote the correlation between the \\(i\\)th and \\(j\\)th observations, then the quantity \\(1 − r_{ij}\\) is proportional to the squared Euclidean distance between the ith and jth observations. On the USArrests data, show that this proportionality holds. Hint: The Euclidean distance can be calculated using the dist() function, and correlations can be calculated using the cor() function. dat &lt;- t(scale(t(USArrests))) d1 &lt;- dist(dat)^2 d2 &lt;- as.dist(1 - cor(t(dat))) plot(d1, d2) 12.2.2 Question 8 In Section 12.2.3, a formula for calculating PVE was given in Equation 12.10. We also saw that the PVE can be obtained using the sdev output of the prcomp() function. On the USArrests data, calculate PVE in two ways: Using the sdev output of the prcomp() function, as was done in Section 12.2.3. pr &lt;- prcomp(USArrests, scale = TRUE) pr$sdev^2 / sum(pr$sdev^2) ## [1] 0.62006039 0.24744129 0.08914080 0.04335752 By applying Equation 12.10 directly. That is, use the prcomp() function to compute the principal component loadings. Then, use those loadings in Equation 12.10 to obtain the PVE. These two approaches should give the same results. colSums(pr$x^2) / sum(colSums(scale(USArrests)^2)) ## PC1 PC2 PC3 PC4 ## 0.62006039 0.24744129 0.08914080 0.04335752 Hint: You will only obtain the same results in (a) and (b) if the same data is used in both cases. For instance, if in (a) you performed prcomp() using centered and scaled variables, then you must center and scale the variables before applying Equation 12.10 in (b). 12.2.3 Question 9 Consider the USArrests data. We will now perform hierarchical clustering on the states. Using hierarchical clustering with complete linkage and Euclidean distance, cluster the states. set.seed(42) hc &lt;- hclust(dist(USArrests), method = &quot;complete&quot;) Cut the dendrogram at a height that results in three distinct clusters. Which states belong to which clusters? ct &lt;- cutree(hc, 3) sapply(1:3, function(i) names(ct)[ct == i]) ## [[1]] ## [1] &quot;Alabama&quot; &quot;Alaska&quot; &quot;Arizona&quot; &quot;California&quot; ## [5] &quot;Delaware&quot; &quot;Florida&quot; &quot;Illinois&quot; &quot;Louisiana&quot; ## [9] &quot;Maryland&quot; &quot;Michigan&quot; &quot;Mississippi&quot; &quot;Nevada&quot; ## [13] &quot;New Mexico&quot; &quot;New York&quot; &quot;North Carolina&quot; &quot;South Carolina&quot; ## ## [[2]] ## [1] &quot;Arkansas&quot; &quot;Colorado&quot; &quot;Georgia&quot; &quot;Massachusetts&quot; ## [5] &quot;Missouri&quot; &quot;New Jersey&quot; &quot;Oklahoma&quot; &quot;Oregon&quot; ## [9] &quot;Rhode Island&quot; &quot;Tennessee&quot; &quot;Texas&quot; &quot;Virginia&quot; ## [13] &quot;Washington&quot; &quot;Wyoming&quot; ## ## [[3]] ## [1] &quot;Connecticut&quot; &quot;Hawaii&quot; &quot;Idaho&quot; &quot;Indiana&quot; ## [5] &quot;Iowa&quot; &quot;Kansas&quot; &quot;Kentucky&quot; &quot;Maine&quot; ## [9] &quot;Minnesota&quot; &quot;Montana&quot; &quot;Nebraska&quot; &quot;New Hampshire&quot; ## [13] &quot;North Dakota&quot; &quot;Ohio&quot; &quot;Pennsylvania&quot; &quot;South Dakota&quot; ## [17] &quot;Utah&quot; &quot;Vermont&quot; &quot;West Virginia&quot; &quot;Wisconsin&quot; Hierarchically cluster the states using complete linkage and Euclidean distance, after scaling the variables to have standard deviation one. hc2 &lt;- hclust(dist(scale(USArrests)), method = &quot;complete&quot;) What effect does scaling the variables have on the hierarchical clustering obtained? In your opinion, should the variables be scaled before the inter-observation dissimilarities are computed? Provide a justification for your answer. ct &lt;- cutree(hc, 3) sapply(1:3, function(i) names(ct)[ct == i]) ## [[1]] ## [1] &quot;Alabama&quot; &quot;Alaska&quot; &quot;Arizona&quot; &quot;California&quot; ## [5] &quot;Delaware&quot; &quot;Florida&quot; &quot;Illinois&quot; &quot;Louisiana&quot; ## [9] &quot;Maryland&quot; &quot;Michigan&quot; &quot;Mississippi&quot; &quot;Nevada&quot; ## [13] &quot;New Mexico&quot; &quot;New York&quot; &quot;North Carolina&quot; &quot;South Carolina&quot; ## ## [[2]] ## [1] &quot;Arkansas&quot; &quot;Colorado&quot; &quot;Georgia&quot; &quot;Massachusetts&quot; ## [5] &quot;Missouri&quot; &quot;New Jersey&quot; &quot;Oklahoma&quot; &quot;Oregon&quot; ## [9] &quot;Rhode Island&quot; &quot;Tennessee&quot; &quot;Texas&quot; &quot;Virginia&quot; ## [13] &quot;Washington&quot; &quot;Wyoming&quot; ## ## [[3]] ## [1] &quot;Connecticut&quot; &quot;Hawaii&quot; &quot;Idaho&quot; &quot;Indiana&quot; ## [5] &quot;Iowa&quot; &quot;Kansas&quot; &quot;Kentucky&quot; &quot;Maine&quot; ## [9] &quot;Minnesota&quot; &quot;Montana&quot; &quot;Nebraska&quot; &quot;New Hampshire&quot; ## [13] &quot;North Dakota&quot; &quot;Ohio&quot; &quot;Pennsylvania&quot; &quot;South Dakota&quot; ## [17] &quot;Utah&quot; &quot;Vermont&quot; &quot;West Virginia&quot; &quot;Wisconsin&quot; Scaling results in different clusters and the choice of whether to scale or not depends on the data in question. In this case, the variables are: Murder numeric Murder arrests (per 100,000) Assault numeric Assault arrests (per 100,000) UrbanPop numeric Percent urban population Rape numeric Rape arrests (per 100,000) These variables are not naturally on the same unit and the units involved are somewhat arbitrary (so for example, Murder could be measured per 1 million rather than per 100,000) so in this case I would argue the data should be scaled. 12.2.4 Question 10 In this problem, you will generate simulated data, and then perform PCA and \\(K\\)-means clustering on the data. Generate a simulated data set with 20 observations in each of three classes (i.e. 60 observations total), and 50 variables. Hint: There are a number of functions in R that you can use to generate data. One example is the rnorm() function; runif() is another option. Be sure to add a mean shift to the observations in each class so that there are three distinct classes. set.seed(42) data &lt;- matrix(rnorm(60 * 50), ncol = 50) classes &lt;- rep(c(&quot;A&quot;, &quot;B&quot;, &quot;C&quot;), each = 20) dimnames(data) &lt;- list(classes, paste0(&quot;v&quot;, 1:50)) data[classes == &quot;B&quot;, 1:10] &lt;- data[classes == &quot;B&quot;, 1:10] + 1.2 data[classes == &quot;C&quot;, 5:30] &lt;- data[classes == &quot;C&quot;, 5:30] + 1 Perform PCA on the 60 observations and plot the first two principal component score vectors. Use a different color to indicate the observations in each of the three classes. If the three classes appear separated in this plot, then continue on to part (c). If not, then return to part (a) and modify the simulation so that there is greater separation between the three classes. Do not continue to part (c) until the three classes show at least some separation in the first two principal component score vectors. pca &lt;- prcomp(data) ggplot(data.frame(Class = classes, PC1 = pca$x[, 1], PC2 = pca$x[, 2]), aes(x = PC1, y = PC2, col = Class)) + geom_point() Perform \\(K\\)-means clustering of the observations with \\(K = 3\\). How well do the clusters that you obtained in \\(K\\)-means clustering compare to the true class labels? Hint: You can use the table() function in R to compare the true class labels to the class labels obtained by clustering. Be careful how you interpret the results: \\(K\\)-means clustering will arbitrarily number the clusters, so you cannot simply check whether the true class labels and clustering labels are the same. km &lt;- kmeans(data, 3)$cluster table(km, names(km)) ## ## km A B C ## 1 1 20 1 ## 2 0 0 19 ## 3 19 0 0 \\(K\\)-means separates out the clusters nearly perfectly. Perform \\(K\\)-means clustering with \\(K = 2\\). Describe your results. km &lt;- kmeans(data, 2)$cluster table(km, names(km)) ## ## km A B C ## 1 18 20 1 ## 2 2 0 19 \\(K\\)-means effectively defines cluster 2 to be class B, but cluster 1 is a mix of classes A and B. Now perform \\(K\\)-means clustering with \\(K = 4\\), and describe your results. km &lt;- kmeans(data, 4)$cluster table(km, names(km)) ## ## km A B C ## 1 0 7 2 ## 2 18 1 0 ## 3 0 0 18 ## 4 2 12 0 \\(K\\)-means effectively defines cluster 1 to be class B, cluster 2 to be class A but clusters 3 and 4 are split over class C. Now perform \\(K\\)-means clustering with \\(K = 3\\) on the first two principal component score vectors, rather than on the raw data. That is, perform \\(K\\)-means clustering on the \\(60 \\times 2\\) matrix of which the first column is the first principal component score vector, and the second column is the second principal component score vector. Comment on the results. km &lt;- kmeans(pca$x[, 1:2], 3)$cluster table(km, names(km)) ## ## km A B C ## 1 0 20 2 ## 2 20 0 0 ## 3 0 0 18 \\(K\\)-means again separates out the clusters nearly perfectly. Using the scale() function, perform \\(K\\)-means clustering with \\(K = 3\\) on the data after scaling each variable to have standard deviation one. How do these results compare to those obtained in (b)? Explain. km &lt;- kmeans(scale(data), 3)$cluster table(km, names(km)) ## ## km A B C ## 1 1 20 1 ## 2 19 0 0 ## 3 0 0 19 \\(K\\)-means appears to perform less well on the scaled data in this case. 12.2.5 Question 11 Write an R function to perform matrix completion as in Algorithm 12.1, and as outlined in Section 12.5.2. In each iteration, the function should keep track of the relative error, as well as the iteration count. Iterations should continue until the relative error is small enough or until some maximum number of iterations is reached (set a default value for this maximum number). Furthermore, there should be an option to print out the progress in each iteration. Test your function on the Boston data. First, standardize the features to have mean zero and standard deviation one using the scale() function. Run an experiment where you randomly leave out an increasing (and nested) number of observations from 5% to 30%, in steps of 5%. Apply Algorithm 12.1 with \\(M = 1,2,...,8\\). Display the approximation error as a function of the fraction of observations that are missing, and the value of \\(M\\), averaged over 10 repetitions of the experiment. 12.2.6 Question 12 In Section 12.5.2, Algorithm 12.1 was implemented using the svd() function. However, given the connection between the svd() function and the prcomp() function highlighted in the lab, we could have instead implemented the algorithm using prcomp(). Write a function to implement Algorithm 12.1 that makes use of prcomp() rather than svd(). 12.2.7 Question 13 On the book website, www.StatLearning.com, there is a gene expression data set (Ch12Ex13.csv) that consists of 40 tissue samples with measurements on 1,000 genes. The first 20 samples are from healthy patients, while the second 20 are from a diseased group. Load in the data using read.csv(). You will need to select header = F. data &lt;- read.csv(&quot;data/Ch12Ex13.csv&quot;, header = FALSE) colnames(data) &lt;- c(paste0(&quot;H&quot;, 1:20), paste0(&quot;D&quot;, 1:20)) Apply hierarchical clustering to the samples using correlation-based distance, and plot the dendrogram. Do the genes separate the samples into the two groups? Do your results depend on the type of linkage used? hc.complete &lt;- hclust(as.dist(1 - cor(data)), method = &quot;complete&quot;) plot(hc.complete) hc.complete &lt;- hclust(as.dist(1 - cor(data)), method = &quot;average&quot;) plot(hc.complete) hc.complete &lt;- hclust(as.dist(1 - cor(data)), method = &quot;single&quot;) plot(hc.complete) Yes the samples clearly separate into the two groups, although the results depend somewhat on the linkage method used. In the case of average clustering, the disease samples all fall within a subset of the healthy samples. Your collaborator wants to know which genes differ the most across the two groups. Suggest a way to answer this question, and apply it here. This is probably best achieved with a supervised approach. A simple method would be to determine which genes show the most significant differences between the groups by applying a t-test to each group. We can then select those with a FDR adjusted p-value less than some given threshold (e.g. 0.05). class &lt;- factor(rep(c(&quot;Healthy&quot;, &quot;Diseased&quot;), each = 20)) pvals &lt;- p.adjust(apply(data, 1, function(v) t.test(v ~ class)$p.value)) which(pvals &lt; 0.05) ## [1] 11 12 13 14 15 16 17 18 19 20 501 502 503 504 505 506 507 508 ## [19] 509 511 512 513 514 515 516 517 519 520 521 522 523 524 525 526 527 528 ## [37] 529 530 531 532 533 534 535 536 537 538 539 540 541 542 543 544 545 546 ## [55] 547 548 549 550 551 552 553 554 555 556 557 558 559 560 561 562 563 564 ## [73] 565 566 567 568 569 570 571 572 574 575 576 577 578 579 580 581 582 583 ## [91] 584 586 587 588 589 590 591 592 593 595 596 597 598 599 600 "],["multiple-testing.html", "13 Multiple Testing 13.1 Conceptual 13.2 Applied", " 13 Multiple Testing 13.1 Conceptual 13.1.1 Question 1 Suppose we test \\(m\\) null hypotheses, all of which are true. We control the Type I error for each null hypothesis at level \\(\\alpha\\). For each sub-problem, justify your answer. In total, how many Type I errors do we expect to make? We expect \\(\\alpha\\). Suppose that the m tests that we perform are independent. What is the family-wise error rate associated with these m tests? Hint: If two events A and B are independent, then Pr(A ∩ B) = Pr(A) Pr(B). \\(m\\alpha\\) Suppose that \\(m = 2\\), and that the p-values for the two tests are positively correlated, so that if one is small then the other will tend to be small as well, and if one is large then the other will tend to be large. How does the family-wise error rate associated with these \\(m = 2\\) tests qualitatively compare to the answer in (b) with \\(m = 2\\)? Hint: First, suppose that the two p-values are perfectly correlated. If they were perfectly correlated, we would effectively be performing a single test (thus FWER would be \\(alpha\\)). In the case when they are positively correlated therefore, we can expect the FWER to be less than \\(2\\alpha\\). Suppose again that \\(m = 2\\), but that now the p-values for the two tests are negatively correlated, so that if one is large then the other will tend to be small. How does the family-wise error rate associated with these \\(m = 2\\) tests qualitatively compare to the answer in (b) with \\(m = 2\\)? Hint: First, suppose that whenever one p-value is less than \\(\\alpha\\), then the other will be greater than \\(\\alpha\\). In other words, we can never reject both null hypotheses. In the case outlined in the hint, we will always have 1 false positive (type 1 error), which, implies that when negatively correlated we expect the FWER to be greater than \\(2\\alpha\\). 13.1.2 Question 2 Suppose that we test \\(m\\) hypotheses, and control the Type I error for each hypothesis at level \\(\\alpha\\). Assume that all \\(m\\) p-values are independent, and that all null hypotheses are true. Let the random variable \\(A_j\\) equal 1 if the \\(j\\)th null hypothesis is rejected, and 0 otherwise. What is the distribution of \\(A_j\\)? \\(A_j\\) follows a Bernoulli distribution: \\(A_j \\sim \\text{Bernoulli}(p)\\) What is the distribution of \\(\\sum_{j=1}^m A_j\\)? Follows a binomial distribution \\(\\sum_{j=1}^m A_j \\sim Bi(m, \\alpha)\\). What is the standard deviation of the number of Type I errors that we will make? The variance of a Binomial is \\(npq\\), so for this situation the standard deviation would be \\(\\sqrt{m \\alpha (1-\\alpha)}\\). 13.1.3 Question 3 Suppose we test \\(m\\) null hypotheses, and control the Type I error for the \\(j\\)th null hypothesis at level \\(\\alpha_j\\), for \\(j=1,...,m\\). Argue that the family-wise error rate is no greater than \\(\\sum_{j=1}^m \\alpha_j\\). \\(p(A \\cup B) = p(A) + p(B)\\) if \\(A\\) and \\(B\\) are independent or \\(p(A) + p(B) - p(A \\cap B)\\) when they are not. Since \\(p(A \\cap B)\\) must be positive, \\(p(A \\cup B) &lt; p(A) + p(B)\\) (whether independent or not). Therefore, the probability of a type I error in any of \\(m\\) hypotheses can be no larger than the sum of the probabilities for each individual hypothesis (which is \\(\\alpha_j\\) for the \\(j\\)th). 13.1.4 Question 4 Suppose we test \\(m = 10\\) hypotheses, and obtain the p-values shown in Table 13.4. pvals &lt;- c(0.0011, 0.031, 0.017, 0.32, 0.11, 0.90, 0.07, 0.006, 0.004, 0.0009) names(pvals) &lt;- paste0(&quot;H&quot;, sprintf(&quot;%02d&quot;, 1:10)) Suppose that we wish to control the Type I error for each null hypothesis at level \\(\\alpha = 0.05\\). Which null hypotheses will we reject? names(which(pvals &lt; 0.05)) ## [1] &quot;H01&quot; &quot;H02&quot; &quot;H03&quot; &quot;H08&quot; &quot;H09&quot; &quot;H10&quot; We reject all NULL hypotheses where \\(p &lt; 0.05\\). Now suppose that we wish to control the FWER at level \\(\\alpha = 0.05\\). Which null hypotheses will we reject? Justify your answer. names(which(pvals &lt; 0.05 / 10)) ## [1] &quot;H01&quot; &quot;H09&quot; &quot;H10&quot; We reject all NULL hypotheses where \\(p &lt; 0.005\\). Now suppose that we wish to control the FDR at level \\(q = 0.05\\). Which null hypotheses will we reject? Justify your answer. names(which(p.adjust(pvals, &quot;fdr&quot;) &lt; 0.05)) ## [1] &quot;H01&quot; &quot;H03&quot; &quot;H08&quot; &quot;H09&quot; &quot;H10&quot; We reject all NULL hypotheses where \\(q &lt; 0.05\\). Now suppose that we wish to control the FDR at level \\(q = 0.2\\). Which null hypotheses will we reject? Justify your answer. names(which(p.adjust(pvals, &quot;fdr&quot;) &lt; 0.2)) ## [1] &quot;H01&quot; &quot;H02&quot; &quot;H03&quot; &quot;H05&quot; &quot;H07&quot; &quot;H08&quot; &quot;H09&quot; &quot;H10&quot; We reject all NULL hypotheses where \\(q &lt; 0.2\\). Of the null hypotheses rejected at FDR level \\(q = 0.2\\), approximately how many are false positives? Justify your answer. We expect 20% (in this case 2 out of the 8) rejections to be false (false positives). 13.1.5 Question 5 For this problem, you will make up p-values that lead to a certain number of rejections using the Bonferroni and Holm procedures. Give an example of five p-values (i.e. five numbers between 0 and 1 which, for the purpose of this problem, we will interpret as p-values) for which both Bonferroni’s method and Holm’s method reject exactly one null hypothesis when controlling the FWER at level 0.1. In this case, for Bonferroni, we need one p-value to be less than \\(0.1 / 5 = 0.02\\). and the others to be above. For Holm’s method, we need the most significant p-value to be below \\(0.1/(5 + 1 - 1) = 0.02\\) also. An example would be: 1, 1, 1, 1, 0.001. pvals &lt;- c(1, 1, 1, 1, 0.001) sum(p.adjust(pvals, method = &quot;bonferroni&quot;) &lt; 0.1) ## [1] 1 sum(p.adjust(pvals, method = &quot;holm&quot;) &lt; 0.1) ## [1] 1 Now give an example of five p-values for which Bonferroni rejects one null hypothesis and Holm rejects more than one null hypothesis at level 0.1. An example would be: 1, 1, 1, 0.02, 0.001. For Holm’s method we reject two because \\(0.02 &lt; 0.1/(5 + 1 - 2)\\). pvals &lt;- c(1, 1, 1, 0.02, 0.001) sum(p.adjust(pvals, method = &quot;bonferroni&quot;) &lt; 0.1) ## [1] 1 sum(p.adjust(pvals, method = &quot;holm&quot;) &lt; 0.1) ## [1] 2 13.1.6 Question 6 For each of the three panels in Figure 13.3, answer the following questions: There are always: 8 positives (red) and 2 negatives (black). False / true positives are black / red points below the line respectively. False / true negatives are red / black points above the line respectively. Type I / II errors are the same as false positives and false negatives respectively. How many false positives, false negatives, true positives, true negatives, Type I errors, and Type II errors result from applying the Bonferroni procedure to control the FWER at level \\(\\alpha = 0.05\\)? Panel FP FN TP TN Type I Type II 1 0 1 7 2 0 1 2 0 1 7 2 0 1 3 0 5 3 2 0 5 How many false positives, false negatives, true positives, true negatives, Type I errors, and Type II errors result from applying the Holm procedure to control the FWER at level \\(\\alpha = 0.05\\)? Panel FP FN TP TN Type I Type II 1 0 1 7 2 0 1 2 0 0 8 2 0 0 3 0 0 8 2 0 0 What is the false discovery rate associated with using the Bonferroni procedure to control the FWER at level \\(\\alpha = 0.05\\)? False discovery rate is the expected ratio of false positives to total positives. There are never any false positives (black points below the line). There are always the same number of total positives (8). For panels 1, 2, 3 this would be 0/8, 0/8 and 0/8 respectively. What is the false discovery rate associated with using the Holm procedure to control the FWER at level \\(\\alpha = 0.05\\)? For panels 1, 2, 3 this would be 0/8, 0/8 and 0/8 respectively. How would the answers to (a) and (c) change if we instead used the Bonferroni procedure to control the FWER at level \\(\\alpha = 0.001\\)? This would equate to a more stringent threshold. We would not call any more false positives, so the results would not change. 13.2 Applied 13.2.1 Question 7 This problem makes use of the Carseats dataset in the ISLR2 package. For each quantitative variable in the dataset besides Sales, fit a linear model to predict Sales using that quantitative variable. Report the p-values associated with the coefficients for the variables. That is, for each model of the form \\(Y = \\beta_0 + \\beta_1X + \\epsilon\\), report the p-value associated with the coefficient \\(\\beta_1\\). Here, \\(Y\\) represents Sales and \\(X\\) represents one of the other quantitative variables. library(ISLR2) nm &lt;- c(&quot;CompPrice&quot;, &quot;Income&quot;, &quot;Advertising&quot;, &quot;Population&quot;, &quot;Price&quot;, &quot;Age&quot;) pvals &lt;- sapply(nm, function(n) { summary(lm(Carseats[[&quot;Sales&quot;]] ~ Carseats[[n]]))$coef[2,4] }) Suppose we control the Type I error at level \\(\\alpha = 0.05\\) for the p-values obtained in (a). Which null hypotheses do we reject? names(which(pvals &lt; 0.05)) ## [1] &quot;Income&quot; &quot;Advertising&quot; &quot;Price&quot; &quot;Age&quot; Now suppose we control the FWER at level 0.05 for the p-values. Which null hypotheses do we reject? names(which(pvals &lt; 0.05/length(nm))) ## [1] &quot;Income&quot; &quot;Advertising&quot; &quot;Price&quot; &quot;Age&quot; Finally, suppose we control the FDR at level 0.2 for the p-values. Which null hypotheses do we reject? names(which(p.adjust(pvals, &quot;fdr&quot;) &lt; 0.2)) ## [1] &quot;Income&quot; &quot;Advertising&quot; &quot;Price&quot; &quot;Age&quot; 13.2.2 Question 8 In this problem, we will simulate data from \\(m = 100\\) fund managers. set.seed(1) n &lt;- 20 m &lt;- 100 X &lt;- matrix(rnorm(n * m), ncol = m) set.seed(1) n &lt;- 20 m &lt;- 100 X &lt;- matrix(rnorm(n * m), ncol = m) These data represent each fund manager’s percentage returns for each of \\(n = 20\\) months. We wish to test the null hypothesis that each fund manager’s percentage returns have population mean equal to zero. Notice that we simulated the data in such a way that each fund manager’s percentage returns do have population mean zero; in other words, all \\(m\\) null hypotheses are true. Conduct a one-sample \\(t\\)-test for each fund manager, and plot a histogram of the \\(p\\)-values obtained. pvals &lt;- apply(X, 2, function(p) t.test(p)$p.value) hist(pvals, main = NULL) If we control Type I error for each null hypothesis at level \\(\\alpha = 0.05\\), then how many null hypotheses do we reject? sum(pvals &lt; 0.05) ## [1] 4 If we control the FWER at level 0.05, then how many null hypotheses do we reject? sum(pvals &lt; 0.05 / length(pvals)) ## [1] 0 If we control the FDR at level 0.05, then how many null hypotheses do we reject? sum(p.adjust(pvals, &quot;fdr&quot;) &lt; 0.05) ## [1] 0 Now suppose we “cherry-pick” the 10 fund managers who perform the best in our data. If we control the FWER for just these 10 fund managers at level 0.05, then how many null hypotheses do we reject? If we control the FDR for just these 10 fund managers at level 0.05, then how many null hypotheses do we reject? best &lt;- order(apply(X, 2, sum), decreasing = TRUE)[1:10] sum(pvals[best] &lt; 0.05 / 10) ## [1] 1 sum(p.adjust(pvals[best], &quot;fdr&quot;) &lt; 0.05) ## [1] 1 Explain why the analysis in (e) is misleading. Hint The standard approaches for controlling the FWER and FDR assume that all tested null hypotheses are adjusted for multiplicity, and that no “cherry-picking” of the smallest p-values has occurred. What goes wrong if we cherry-pick? This is misleading because we are not correctly accounting for all tests performed. Cherry picking the similar to repeating a test until by chance we find a significant result. "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
